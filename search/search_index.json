{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vis\u00e3o Geral","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o do LuxorASAP, a su\u00edte de scripts e m\u00f3dulos Python utilizada para automa\u00e7\u00e3o de processos de back-office de investimentos na Luxor.</p> <p>Esta documenta\u00e7\u00e3o est\u00e1 organizada nas seguintes se\u00e7\u00f5es:</p> <ul> <li>Arquitetura: vis\u00e3o geral da estrutura de diret\u00f3rios e fluxos de dados.</li> <li>M\u00f3dulos: refer\u00eancias das docstrings dos principais scripts e pacotes.</li> <li>How To: guias r\u00e1pidos para tarefas recorrentes e resolu\u00e7\u00e3o de problemas.</li> </ul>","tags":["overview"]},{"location":"#como-usar","title":"Como usar","text":"<ol> <li>Utilize o menu lateral para navegar entre as se\u00e7\u00f5es.</li> <li>Em Arquitetura &gt; Estrutura de arquivos, explore o layout completo do projeto.</li> <li>Na se\u00e7\u00e3o Diagrams, visualize os fluxos de dados, extra\u00e7\u00e3o do Bloomberg e processos de atualiza\u00e7\u00e3o.</li> <li>Em How To, siga instru\u00e7\u00f5es passo a passo para tarefas espec\u00edficas, como adicionar novos ativos.</li> </ol>","tags":["overview"]},{"location":"#interacao-via-chatgpt","title":"Intera\u00e7\u00e3o via ChatGPT","text":"<p>Ap\u00f3s publicar em GitHub Pages (por exemplo, <code>https://luxorgroup.github.io/luxorASAP-docs/</code>), copie a URL e cole em uma sess\u00e3o do ChatGPT. O modelo carregar\u00e1 a documenta\u00e7\u00e3o em tempo real e responder\u00e1 perguntas sobre o sistema.</p>","tags":["overview"]},{"location":"architecture/bloomberg_data_pipeline/","title":"Bloomberg Data Pipeline","text":"<p>Este documento descreve a arquitetura e o funcionamento do pipeline de extra\u00e7\u00e3o de dados da Bloomberg utilizado no LuxorASAP. Ele cobre desde a extra\u00e7\u00e3o via <code>blpAPI</code>, passando pelo armazenamento em <code>ADLS</code>, at\u00e9 a transforma\u00e7\u00e3o e integra\u00e7\u00e3o das tabelas finais.</p>"},{"location":"architecture/bloomberg_data_pipeline/#visao-geral","title":"Vis\u00e3o Geral","text":"<pre><code>flowchart TD\n  subgraph Bloomberg Data Pipeline\n    MDE[market_data_extractor.py]\n    MDE --&gt;|--datapoint| Raw1[px_last_raw + last_all_flds_raw]\n    MDE --&gt;|--histdata| Raw2[hist_px_last_raw + hist_all_flds_raw + holidays_raw]\n    Raw1 --&gt; Raw[ADLS: luxorasap/raw/parquet]\n    Raw2 --&gt; Raw\n    Raw --&gt; MDL[market_data_loader.py]\n    MDL --&gt; Enriched[\"ADLS: luxorasap/enriched/parquet (px_last, hist_px_last, holidays, last_all_flds, hist_all_flds)\"]\n  end</code></pre>"},{"location":"architecture/bloomberg_data_pipeline/#etapa-1-extracao-via-market_data_extractorpy","title":"Etapa 1 \u2013 Extra\u00e7\u00e3o via <code>market_data_extractor.py</code>","text":"<p>O script <code>market_data_extractor.py</code> \u00e9 o ponto inicial do pipeline, respons\u00e1vel por consultar dados da Bloomberg via <code>blpAPI</code> e salvar arquivos <code>.parquet</code> no ADLS em <code>luxorasap/raw/parquet</code>.</p>"},{"location":"architecture/bloomberg_data_pipeline/#modos-de-execucao","title":"Modos de Execu\u00e7\u00e3o","text":""},{"location":"architecture/bloomberg_data_pipeline/#1-datapoint-extracao-continua-bdp","title":"1. <code>--datapoint</code>: Extra\u00e7\u00e3o cont\u00ednua (BDP)","text":"<pre><code>flowchart TD\n  subgraph Datapoint\n    MDEX[\"market_data_extractor.py\"]\n    MDEX --&gt; A1[\"Leitura: historico_trades_ativos.xlsx (aba ativos)\"]\n    MDEX --&gt; A2[\"Leitura: historico_trades_ativos.xlsx (aba asset_field_map)\"]\n    A1 --&gt; C1{\"Disable BDP = FALSE?\"}\n    C1 --&gt;|Sim| Q1[\"BDP \u2192 px_last_raw.parquet\"]\n    A2 --&gt; C2{\"Last_Data = TRUE?\"}\n    C2 --&gt;|Sim| Q2[\"BDP \u2192 last_all_flds_raw.parquet\"]\n    Q1 &amp; Q2 --&gt; W1[\"Grava em luxorasap/raw/parquet\"]\n  end</code></pre> <p>Este modo do <code>market_data_extractor.py</code> realiza chamadas cont\u00ednuas \u00e0 API BDP da Bloomberg para extrair os \u00faltimos pre\u00e7os (<code>px_last_raw</code>) e outros campos configurados (<code>last_all_flds_raw</code>). A frequ\u00eancia da extra\u00e7\u00e3o (day, hour, minute) e os ativos consultados s\u00e3o definidos nas abas <code>ativos</code> e <code>asset_field_map</code> do Excel <code>historico_trades_ativos.xlsx</code>. O script tamb\u00e9m contabiliza os hits consumidos (m\u00e1ximo de 500 mil por dia).</p>"},{"location":"architecture/bloomberg_data_pipeline/#2-histdata-extracao-historica-bdh-bds","title":"2. <code>--histdata</code>: Extra\u00e7\u00e3o hist\u00f3rica (BDH + BDS)","text":"<p><pre><code>flowchart TD\n  subgraph Histdata\n    MDEX[\"market_data_extractor.py\"]\n    MDEX --&gt; B1[\"Leitura: historico_trades_ativos.xlsx (aba ativos)\"]\n    MDEX --&gt; B2[\"Leitura: historico_trades_ativos.xlsx (aba asset_field_map)\"]\n    B1 --&gt; C3{\"Hist_Price = TRUE?\"}\n    C3 --&gt;|Sim| Q3[\"BDH \u2192 hist_px_last_raw.parquet\"]\n    B2 --&gt; C4{\"Historical_Data = TRUE?\"}\n    C4 --&gt;|Sim| Q4[\"BDH \u2192 hist_all_flds_raw.parquet\"]\n    MDEX --&gt; Q5[\"BDS \u2192 holidays_raw.parquet\"]\n    Q3 &amp; Q4 &amp; Q5 --&gt; W2[\"Grava em luxorasap/raw/parquet\"]\n  end</code></pre> Este modo executa consultas \u00fanicas para baixar pre\u00e7os hist\u00f3ricos (<code>hist_px_last_raw</code>), campos hist\u00f3ricos (<code>hist_all_flds_raw</code>) e feriados de mercado (<code>holidays_raw</code>). As configura\u00e7\u00f5es s\u00e3o feitas no mesmo Excel, nas colunas <code>Hist_Price</code>, <code>Historical_Data</code> e <code>Start_Date</code>. Esse modo n\u00e3o \u00e9 cont\u00ednuo e n\u00e3o possui limita\u00e7\u00e3o expl\u00edcita de uso por parte da Bloomberg.</p>"},{"location":"architecture/bloomberg_data_pipeline/#resumo","title":"Resumo","text":"Modo Fun\u00e7\u00e3o BBG Tabelas de Sa\u00edda Fonte de Frequ\u00eancia Observa\u00e7\u00f5es <code>--datapoint</code> BDP <code>px_last_raw</code>, <code>last_all_flds_raw</code> aba ativos e asset_field_map Consulta cont\u00ednua (day/hour/minute). Monitora limite de 500\u202fk hits/dia <code>--histdata</code> BDH &amp; BDS <code>hist_px_last_raw</code>, <code>hist_all_flds_raw</code>, <code>holidays_raw</code> abas ativos e asset_field_map Execu\u00e7\u00e3o pontual (batch). Sem limite expl\u00edcito"},{"location":"architecture/bloomberg_data_pipeline/#etapa-2-integracao-via-market_data_loaderpy","title":"Etapa 2 \u2013 Integra\u00e7\u00e3o via <code>market_data_loader.py</code>","text":"<p>Ap\u00f3s a extra\u00e7\u00e3o, o script <code>market_data_loader.py</code> observa altera\u00e7\u00f5es nas pastas <code>raw/parquet</code> e inicia a transforma\u00e7\u00e3o e integra\u00e7\u00e3o dos dados.</p> <pre><code>flowchart TD\n  subgraph Loader\n    ML[\"market_data_loader.py\"]\n    ML --&gt; M1[\"Monitora: luxorasap/raw/parquet\"]\n    M1 --&gt; px_raw[\"px_last_raw.parquet\"]\n    M1 --&gt; hist_px[\"hist_px_last_raw.parquet\"]\n    M1 --&gt; fld_raw[\"last_all_flds_raw.parquet\"]\n    M1 --&gt; hist_fld[\"hist_all_flds_raw.parquet\"]\n    M1 --&gt; hol_raw[\"holidays_raw.parquet\"]\n\n    subgraph Enriquecimento\n      px_raw --&gt; Merge[\"Merge com fontes externas\"]\n      Merge --&gt; px[\"px_last.parquet\"]\n      hist_px --&gt; Ajuste[\"Corrige FIAs via IBOV\"]\n      Ajuste --&gt; hpx[\"hist_px_last.parquet\"]\n      fld_raw --&gt; lfld[\"last_all_flds.parquet\"]\n      hist_fld --&gt; hfld[\"hist_all_flds.parquet\"]\n      hol_raw --&gt; hol[\"holidays.parquet\"]\n    end\n\n    subgraph Output\n      px &amp; hpx &amp; lfld &amp; hfld &amp; hol --&gt; E[\"luxorasap/enriched/parquet\"]\n    end\n  end</code></pre> <p>Ao identificar novos arquivos, ele integra os dados da Bloomberg com outras fontes de pre\u00e7o internas, corrige pre\u00e7os dos FIAs com base no IBOV e gera as tabelas finais enriquecidas (<code>px_last</code>, <code>hist_px_last</code>, <code>holidays</code>, <code>last_all_flds</code>, <code>hist_all_flds</code>) na pasta <code>luxorasap/enriched/parquet</code>.</p>"},{"location":"architecture/bloomberg_data_pipeline/#fontes-integradas-em-px_lastparquet","title":"Fontes integradas em <code>px_last.parquet</code>:","text":"<ul> <li><code>px_last_raw</code> (Bloomberg via BDP)</li> <li><code>hist_non_bbg_px_last</code></li> <li><code>custom_prices</code></li> <li><code>custom_funds_quotas</code></li> <li><code>all_funds_quotas</code></li> <li><code>hist_vc_px_last</code></li> </ul>"},{"location":"architecture/bloomberg_data_pipeline/#transformacoes-adicionais","title":"Transforma\u00e7\u00f5es adicionais:","text":"<ul> <li>FIAs: os pre\u00e7os em <code>hist_px_last_raw</code> s\u00e3o corrigidos at\u00e9 o dia atual com base no IBOV.</li> </ul>"},{"location":"architecture/bloomberg_data_pipeline/#tabelas-finais-geradas-em-luxorasapenrichedparquet","title":"Tabelas finais geradas em <code>luxorasap/enriched/parquet</code>:","text":"<ul> <li><code>px_last.parquet</code>: \u00faltimos pre\u00e7os de todos os ativos.</li> <li><code>hist_px_last.parquet</code>: hist\u00f3ricos ajustados (com corre\u00e7\u00f5es).</li> <li><code>holidays.parquet</code>: feriados integrados.</li> <li><code>last_all_flds.parquet</code>: campos atuais de ativos.</li> <li><code>hist_all_flds.parquet</code>: campos hist\u00f3ricos de ativos.</li> </ul>"},{"location":"architecture/bloomberg_data_pipeline/#conclusao","title":"Conclus\u00e3o","text":"<p>Este pipeline garante a extra\u00e7\u00e3o estruturada e robusta dos dados da Bloomberg, respeitando as limita\u00e7\u00f5es da API e unificando m\u00faltiplas fontes de pre\u00e7o em uma camada final confi\u00e1vel. O processo est\u00e1 preparado para rodar de forma cont\u00ednua e autom\u00e1tica, assegurando que os dados estejam sempre atualizados para alimentar modelos, dashboards e an\u00e1lises internas da Luxor.</p>"},{"location":"architecture/file_tree/","title":"Vis\u00e3o geral","text":"<p>Este documento apresenta a \u00e1rvore de diret\u00f3rios completa do LuxorASAP na vers\u00e3o que est\u00e1 hoje em produ\u00e7\u00e3o, acompanhada de uma breve explica\u00e7\u00e3o de cada item. Legenda: pastas terminam com <code>/</code>; arquivos exibem a descri\u00e7\u00e3o comentada na mesma linha.</p> <p>Observa\u00e7\u00e3o \u2013 Esta \u00e1rvore \u00e9 a \u201cfonte da verdade\u201d para localizar artefatos. Scripts de ETL e m\u00f3dulos Python t\u00eam documenta\u00e7\u00e3o t\u00e9cnica adicional em <code>docs/modules/\u2026</code>.</p>","tags":["architecture","file-tree"]},{"location":"architecture/file_tree/#arvore-de-diretorios-detalhada","title":"\u00c1rvore de diret\u00f3rios detalhada","text":"<pre><code>LuxorASAP\n\u251c\u2500 carteira_online/\n\u2502  \u251c\u2500 production/\n\u2502  \u2502  \u251c\u2500 bases_input/\n\u2502  \u2502  \u2502  \u251c\u2500 Operation_Desk_V4.xlsm      \u2013 planilha p/ ajuste de posi\u00e7\u00e3o; CRUD de ativos (1 aba/fundo)\n\u2502  \u2502  \u2502  \u2514\u2500 input_data.xlsx             \u2013 c\u00f3pia gerada ao clicar \u201csalvar input\u201d; lida por portfolio_builder.py\n\u2502  \u2502  \u251c\u2500 carteiras_btg/                 \u2013 planilhas BTG on-shore; lidas por leitor_carteira_excel.py\n\u2502  \u2502  \u251c\u2500 bases_historicas/\n\u2502  \u2502  \u2502  \u2514\u2500 historical_return.xlsx      \u2013 cotas gerenciais geradas por return_calculator.py\n\u2502  \u2502  \u251c\u2500 bases_output/\n\u2502  \u2502  \u2502  \u2514\u2500 base_portfolios.xlsx        \u2013 carteira gerencial; c\u00f3pia di\u00e1ria vai para carteiras_luxor_historico/\n\u2502  \u2502  \u251c\u2500 carteiras_luxor_historico/\n\u2502  \u2502  \u2502  \u251c\u2500 2025/                       \u2013 relat\u00f3rios de abertura/fechamento di\u00e1rios\n\u2502  \u2502  \u2502  \u251c\u2500 bases_completas/            \u2013 hist\u00f3rico completo das carteiras\n\u2502  \u2502  \u2502  \u2514\u2500 metricas_risco/\n\u2502  \u2502  \u2502      \u2514\u2500 hist_risk_metrics.xlsx  \u2013 m\u00e9tricas de risco hist\u00f3ricas\n\u2502  \u2502  \u251c\u2500 portfolio_builder.py           \u2013 atualiza base_carteira_online_v12.xlsx + m\u00e9tricas\n\u2502  \u2502  \u251c\u2500 return_calculator.py           \u2013 calcula cotas di\u00e1rias, retornos 1D/MTD/YTD\n\u2502  \u2502  \u251c\u2500 asset_data_extraction.py       \u2013 legacy \u2013 importado mas n\u00e3o mais usado\n\u2502  \u2502  \u251c\u2500 btg_data_extraction.py         \u2013 l\u00ea carteiras_btg.xlsx; usado em cash_calculator.py\n\u2502  \u2502  \u251c\u2500 cash_calculator.py             \u2013 quebra de caixa &amp; provis\u00e3o de trades a liquidar\n\u2502  \u2502  \u251c\u2500 historical_data_extractor.py   \u2013 legacy \u2013 swap NDF (parou em 2023)\n\u2502  \u2502  \u251c\u2500 leitor_carteira_excel.py       \u2013 compila carteiras BTG mais recentes\n\u2502  \u2502  \u2514\u2500 onedrive_file_redirect.py\n\u2502  \u251c\u2500 base_carteira_online_v12.xlsx     \u2013 output final p/ PBI carteira_online.pbix\n\u2502  \u2514\u2500 historical_return.xlsx            \u2013 output final p/ PBI relatorio_retornos.pbix\n\u251c\u2500 source_bases/\n\u2502  \u251c\u2500 scripts/\n\u2502  \u2502  \u251c\u2500 hist_concentration_updater.py  \u2013 atualiza hist_portfolios_concentration.parquet (descontinuado)\n\u2502  \u2502  \u251c\u2500 custom_funds_quotas_updater.py \u2013 gera cotas p/ portfolios customizados\n\u2502  \u2502  \u251c\u2500 non_bbg_data_updater.py        \u2013 cria hist_non_bbg_px_last.parquet\n\u2502  \u2502  \u251c\u2500 daily_pnl_updater.py           \u2013 migra\u00e7\u00e3o p/ luxor-data-pipelines (n\u00e3o rodar com --hist aqui)\n\u2502  \u2502  \u251c\u2500 historical_quotas_updater.py   \u2013 atualiza all_funds_quotas.parquet\n\u2502  \u2502  \u251c\u2500 risk_metrics_updater.py        \u2013 incremental historico_risk_metrics.xlsx \u2192 parquet\n\u2502  \u2502  \u251c\u2500 us_cash_updater.py             \u2013 gera hist_us_cash.parquet (caixa offshore)\n\u2502  \u2502  \u2514\u2500 swaps_updater.py               \u2013 legacy \u2013 swaps zerados em 2023\n\u2502  \u251c\u2500 ativos_sem_bbg.xlsx               \u2013 s\u00e9ries personalizadas (IPCA+7, Hawker etc.)\n\u2502  \u251c\u2500 historico_caixa.xlsx              \u2013 caixa US hist\u00f3rico (poss\u00edvel legado)\n\u2502  \u251c\u2500 historico_risk_metrics.xlsx       \u2013 m\u00e9tricas hist\u00f3ricas dos portfolios\n\u2502  \u251c\u2500 historico_swap.xlsx               \u2013 swap hist\u00f3rico (atualiza\u00e7\u00e3o encerrada em 2023)\n\u2502  \u251c\u2500 historico_trades_ativos.xlsx      \u2013 **planilha-mestre** de boletas, ativos, pre\u00e7os il\u00edquidos, map-fields\n\u2502  \u251c\u2500 manual_prices_inputs.xlsx         \u2013 input manual de cotas oficiais (IP Atlas, TCI\u2026)\n\u2502  \u2514\u2500 others.xlsx                       \u2013 regras de aporte/resgate, taxas, corretagens (PBI operations_report)\n\u251c\u2500 run_luxorASAP.py                     \u2013 daemon: monitora historico_trades_ativos.xlsx \u2192 parquet &amp; posi\u00e7\u00f5es\n\u2502  \u251c\u2500 fund.py                           \u2013 abstra\u00e7\u00e3o de fundo: ativos, bancos, trades, caixa\n\u2502  \u251c\u2500 asset.py                          \u2013 processa trades de um ativo \u2192 quantidades por data\n\u2502  \u2514\u2500 bank_account.py                   \u2013 posi\u00e7\u00f5es &amp; fluxos por conta banc\u00e1ria\n\u251c\u2500 source_bases_updater.py              \u2013 orquestra scripts em source_bases/scripts/\n\u251c\u2500 market_data_extractor.py             \u2013 extrai BDH/BDP do Bloomberg (LuxorDB/raw)\n\u251c\u2500 market_data_loader.py                \u2013 unifica pre\u00e7os, calcula estimativas e atualiza LuxorDB/tables\n\u251c\u2500 luxorDB_datareader.py                \u2013 API de consulta ao LuxorDB\n\u251c\u2500 luxorDB_dataloader.py                \u2013 camada de persist\u00eancia padronizada p/ LuxorDB\n\u2514\u2500 LuxorDB/\n   \u251c\u2500 raw/                              \u2013 dumps brutos (Bloomberg, etc.)\n   \u2514\u2500 tables/                           \u2013 tabelas processadas (n\u00edveis variados)\n</code></pre>","tags":["architecture","file-tree"]},{"location":"modules/luxorDB_dataloader/","title":"<code>luxorDB_dataloader.py</code>","text":"<p>Camada de escrita/persist\u00eancia padronizada no LuxorDB.</p> <p>Este m\u00f3dulo cont\u00e9m a classe <code>DataLoader</code>, respons\u00e1vel por gerenciar o carregamento e a persist\u00eancia de dados para o LuxorDB.</p> <p>A classe <code>DataLoader</code> oferece funcionalidades para: - Carregar tabelas a partir de arquivos Excel ou dados em mem\u00f3ria. - Monitorar arquivos e tabelas para detectar modifica\u00e7\u00f5es e acionar recargas. - Normalizar colunas de texto para min\u00fasculas. - Persistir dados em diferentes formatos (Excel, Parquet) no sistema de arquivos local. - Exportar dados para o Azure Blob Storage em formato Parquet.</p> <p>O objetivo principal \u00e9 garantir que os dados no LuxorDB estejam sempre atualizados e consistentes, facilitando a integra\u00e7\u00e3o com outras partes do sistema.</p> <p>De forma bem resumida: Este m\u00f3dulo \u00e9 respons\u00e1vel por carregar, monitorar e persistir dados no LuxorDB, garantindo que as tabelas estejam sempre atualizadas e consistentes. Ele gerencia a leitura de arquivos Excel e dados em mem\u00f3ria, normaliza colunas e exporta os dados para formatos como Excel, Parquet e Azure Blob Storage. Oferece funcoes para:</p>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader","title":"<code>DataLoader</code>","text":"Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>class DataLoader:\n\n    def __init__(self, luxorDB_directory = None):\n        \"\"\"Fornece uma forma padronizada de carregar tabelas para a luxorDB.\n            1. Possui metodos para carregar tabelas que j\u00e1 estao carregadas na mem\u00f3ria\n                - Sao os metodos que possuem 'table' no nome\n            2. Possui metodos para carregar arquivos de excel, com todas as suas abas\n                Inclui metodo para checagem de alteracao de versao do arquivo\n                - Sao os metodos que possuem 'file' no nome\n        Args:\n            luxorDB_directory (pathlib.Path, optional): Caminho completo ate o diretorio de destino dos dados.\n        \"\"\"\n        self.luxorDB_directory = luxorDB_directory\n\n        if self.luxorDB_directory is None:\n            self.luxorDB_directory = Path(__file__).absolute().parent/\"LuxorDB\"/\"tables\"\n\n        self.tracked_files = {}\n        self.tracked_tables = {}\n\n    def __persist_column_formatting(self, t):\n\n        columns_to_persist = {\"Name\", \"Class\", \"Vehicles\", \"Segment\"}\n\n        if len(set(t.columns).intersection(columns_to_persist)) &gt; 0:\n            # Vamos persistir a formatacao de algumas colunas\n            columns_order = list(t.columns)\n            columns_to_persist = list(set(t.columns).intersection(columns_to_persist))\n            persistent_data = t[columns_to_persist].copy()\n\n            columns_to_normalize = list(set(columns_order) - set(columns_to_persist))\n            t = self.text_to_lowercase(t[columns_to_normalize])\n            t.loc[:,columns_to_persist] = persistent_data\n            return t[columns_order]\n\n        # Nos outros casos, transformaremos tudo em lowercase\n        return self.text_to_lowercase(t)\n\n\n    def text_to_lowercase(self, t):\n        \"\"\"\n        Converte todas as colunas de texto para lowercase\n        Args:\n            t (pd.DataFrame): pandas DataFrame\n        Returns:\n            pd.DataFrame\n        \"\"\"\n\n        return t.map(lambda x: x.lower().strip() if isinstance(x, str) else x)\n\n\n    def add_file_tracker(self, tracked_file_path, filetype=\"excel\", sheet_names={}, \n            excel_size_limit = None,index=False, index_name=\"index\",normalize_columns=False):\n        \"\"\" Adiciona arquivo na lista para checar por alteracao\n        Args:\n            tracked_file_path (pathlib.Path): caminho completo ate o arquivo,\n                    incluindo nome do arquivo e extens\u00e3o.\n            sheet_names (dict, optional): Caso seja uma planilha com varias abas, mapear\n                    aqui o nome da aba para o nome do arquivo de saida desejado.\n        \"\"\"\n        if tracked_file_path not in self.tracked_files:\n            self.tracked_files[tracked_file_path] = {\n                    \"last_mtime\" : dt.datetime.timestamp(dt.datetime(2000,1,1)),\n                    \"filetype\" : filetype, \"sheet_names\": sheet_names,\n                    \"excel_size_limit\" : excel_size_limit,\n                    \"index\" : index,\n                    \"index_name\" : index_name,\n                    \"normalize_columns\" : normalize_columns,\n                }\n\n\n    def add_table_tracker(self, table_name:str):\n        \"\"\" Adiciona tabela na lista para controle de alteracao.\"\"\"\n\n        if table_name not in self.tracked_tables:\n            self.tracked_tables[table_name] = dt.datetime.timestamp(dt.datetime(2000,1,1))\n\n\n    def remove_file_tracker(self, tracked_file_path):\n\n        if tracked_file_path in self.tracked_files:\n            del self.tracked_files[tracked_file_path]\n\n\n    def remove_table_tracker(self, table_name:str):\n\n        if table_name in self.tracked_tables:\n            del self.tracked_tables[table_name]\n\n\n    def is_file_modified(self, tracked_file_path: Path) -&gt; {bool, float}:\n        \"\"\" Checa se o arquivo foi modificado desde a ultima leitura.\n        Returns:\n            tuple(bool, float): (foi modificado?, timestamp da ultima modificacao)\n        \"\"\"\n\n        file_data = self.tracked_files[tracked_file_path]\n\n        last_saved_time = file_data[\"last_mtime\"]\n        file_last_update = tracked_file_path.stat().st_mtime\n        return file_last_update &gt; last_saved_time, file_last_update\n\n\n    def set_file_modified_time(self, tracked_file_path, file_mtime):\n\n        self.tracked_files[tracked_file_path][\"last_mtime\"] = file_mtime\n\n\n    def load_file_if_modified(self, tracked_file_path, export_to_blob=False, blob_directory='enriched/parquet'):\n        \"\"\"Carrega arquivo no caminho indicado, carregando na base de dados caso modificado.\n        Args:\n            tracked_file_path (pathlib.Path): caminho ate o arquivo(cadastro previamente por add_file_tracker)\n            type_map (_type_, optional): _description_. Defaults to None.\n            filetype (str, optional): _description_. Defaults to \"excel\".\n        \"\"\"\n        file_data = self.tracked_files[tracked_file_path]\n\n        last_saved_time = file_data[\"last_mtime\"]\n        filetype = file_data[\"filetype\"]\n        file_sheets = file_data[\"sheet_names\"]\n\n        file_last_update = tracked_file_path.stat().st_mtime\n\n        if file_last_update &gt; last_saved_time: # Houve alteracao no arquivo\n            if filetype == \"excel\":\n                file_sheets = None if len(file_sheets) == 0 else list(file_sheets.keys())\n\n                # tables sera sempre um dicionario de tabelas\n                tables = None\n                trials = 25\n                t_counter = 1\n                while trials - t_counter &gt; 0:\n                    try:\n                        tables = pd.read_excel(tracked_file_path, sheet_name=file_sheets)\n                        t_counter = trials # leitura concluida\n                    except PermissionError:\n\n                        logger.error(f\"Erro ao tentar ler arquivo '{tracked_file_path}.\\nTentativa {t_counter} de {trials};'.\\nSe estiver aberto feche.\")\n                        time.sleep(10)\n                        t_counter += 1\n\n                for sheet_name, table_data in tables.items():\n\n                    table_name = sheet_name if file_sheets is None else file_data[\"sheet_names\"][sheet_name]\n\n                    if table_name == \"trades\":\n                        table_data[\"ID\"] = table_data.index\n\n                    self.__export_table(table_name, table_data, index=file_data[\"index\"], index_name=file_data[\"index_name\"],\n                                            normalize_columns=file_data[\"normalize_columns\"], export_to_blob=export_to_blob,\n                                            blob_directory=blob_directory)\n                self.tracked_files[tracked_file_path][\"last_mtime\"] = file_last_update\n\n\n    def load_table_if_modified(self, table_name, table_data, last_update, index=False, index_name=\"index\", normalize_columns=False,\n                               do_not_load_excel=False, export_to_blob=False, blob_directory='enriched/parquet',\n                               is_data_in_bytes=False, bytes_extension=\".xlsx\"):\n        \"\"\"\n        Args:\n            table_name (str): nome da tabela (sera o mesmo do arquivo a ser salvo)\n            table_data (pd.DataFrame): tabela de dados\n            last_update (timestamp): timestamp da ultima edicao feita na tabela\n        \"\"\"\n\n        if table_name not in self.tracked_tables:\n            self.add_table_tracker(table_name)\n\n\n        last_update_time = self.tracked_tables[table_name]\n        if last_update &gt; last_update_time:\n\n            self.tracked_tables[table_name] = last_update\n            self.__export_table(table_name, table_data, index=index, index_name=index_name, normalize_columns=normalize_columns,\n                                do_not_load_excel=do_not_load_excel, export_to_blob=export_to_blob, blob_directory=blob_directory,\n                                is_data_in_bytes=is_data_in_bytes, bytes_extension=bytes_extension)\n\n\n    def scan_files(self, export_to_blob=False, blob_directory='enriched/parquet'):\n        \"\"\"\n            Para todos os arquivos cadastrados, vai buscar e carregar quando houver\n            arquivo mais recente.\n        \"\"\"\n\n        for file in self.tracked_files:\n\n            self.load_file_if_modified(file, export_to_blob=export_to_blob, blob_directory=blob_directory)\n\n\n    #def __load_bytes(self, content: bytes, extension=\".xlsx\") -&gt; pd.DataFrame:\n    #    if extension == \".xlsx\" or extension == \"xlsx\" or extension == \"xls\":\n    #        df = pd.read_excel(io.BytesIO(content), engine=\"openpyxl\")\n    #    \n    #        return df\n#\n    #    raise ValueError(f'Extension {extension} not supported')\n\n    def __load_bytes(self, content: bytes, extension: str) -&gt; pd.DataFrame:\n        extension = extension.lower()\n\n        if extension in [\".xlsx\", \".xls\", \"xlsx\", \"xls\"]:\n            df = pd.read_excel(io.BytesIO(content), engine=\"openpyxl\")\n            return df\n\n        if extension == \".csv\":\n            try:\n                return pd.read_csv(io.BytesIO(content), encoding=\"utf-8\")\n            except UnicodeDecodeError:\n                return pd.read_csv(io.BytesIO(content), encoding=\"latin1\")\n\n        if extension == \".parquet\":\n            df = pd.read_parquet(io.BytesIO(content))\n            return df\n\n        raise ValueError(f'Extension {extension} not supported')\n\n\n    def __export_table(self, table_name, table_data, index=False, index_name=\"index\", normalize_columns=False,\n                       do_not_load_excel=False, export_to_blob=False, blob_directory='enriched/parquet',\n                       is_data_in_bytes=False, bytes_extension=\".xlsx\"):\n\n        dest_directory = self.luxorDB_directory\n        #TODO -&gt; formatar para index=False\n        # Salvando em formato excel\n        attempts = 10\n        count_attempt = 0\n\n        if is_data_in_bytes:\n            table_data = self.__load_bytes(table_data, extension=bytes_extension)\n\n        # Se o index tiver dados, vamos trata-los para virar uma coluna\n        if index:\n            # Tratando o nome do index, caso seja necessario transformar em coluna\n            prev_index = table_data.index.name\n            if prev_index is not None and index_name == \"index\":\n                index_name = prev_index\n            table_data.index.name = index_name\n\n            table_data = table_data.reset_index()\n\n\n        if normalize_columns:\n            table_data = self.__persist_column_formatting(table_data)\n\n        if not do_not_load_excel:\n            while count_attempt &lt; attempts:\n                count_attempt += 1\n                try:\n                    if len(table_data) &gt; 1_000_000:\n                        table_data = table_data.tail(1_000_000)\n                    table_data.to_excel(dest_directory/f\"{table_name}.xlsx\", index=False)\n                    count_attempt = attempts # sair do loop\n\n                except PermissionError:\n                    logger.error(f\"Erro ao tentar salvar arquivo {table_name}. Feche o arquivo. Tentativa {count_attempt} de {attempts}\")\n                    time.sleep(10 + count_attempt * 5)\n\n        # Salvando em csv \n        # -&gt; Salvar em csv foi descontinuado por falta de uso.\n        #table_data.to_csv(dest_directory/\"csv\"/f\"{table_name}.csv\", sep=\";\", index=False)\n\n        # Salvando em parquet (tudo como string... dtypes deverao ser atribuidos na leitura)\n        table_data = table_data.astype(str)\n        table_data.to_parquet(dest_directory/\"parquet\"/f\"{table_name}.parquet\", engine=\"fastparquet\", index=False)\n\n        if export_to_blob:\n            # Definindo o Container e o Blob Name\n            container_name = \"luxorasap\"\n            blob_name = f\"{blob_directory}/{table_name}.parquet\"  #\n\n            # Convers\u00e3o para parquet em mem\u00f3ria (sem precisar salvar local)\n            table = pa.Table.from_pandas(table_data)\n            parquet_buffer = io.BytesIO()\n            pq.write_table(table, parquet_buffer)\n            parquet_buffer.seek(0)  # Reseta o ponteiro para o in\u00edcio do buffer\n\n            # Conectando ao Blob Storage\n            connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n            blob_service_client = BlobServiceClient.from_connection_string(conn_str=connection_string)\n\n            # Criando um Blob Client\n            blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n            blob_client.upload_blob(parquet_buffer, overwrite=True)\n</code></pre>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader.__init__","title":"<code>__init__(luxorDB_directory=None)</code>","text":"<p>Fornece uma forma padronizada de carregar tabelas para a luxorDB.     1. Possui metodos para carregar tabelas que j\u00e1 estao carregadas na mem\u00f3ria         - Sao os metodos que possuem 'table' no nome     2. Possui metodos para carregar arquivos de excel, com todas as suas abas         Inclui metodo para checagem de alteracao de versao do arquivo         - Sao os metodos que possuem 'file' no nome Args:     luxorDB_directory (pathlib.Path, optional): Caminho completo ate o diretorio de destino dos dados.</p> Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>def __init__(self, luxorDB_directory = None):\n    \"\"\"Fornece uma forma padronizada de carregar tabelas para a luxorDB.\n        1. Possui metodos para carregar tabelas que j\u00e1 estao carregadas na mem\u00f3ria\n            - Sao os metodos que possuem 'table' no nome\n        2. Possui metodos para carregar arquivos de excel, com todas as suas abas\n            Inclui metodo para checagem de alteracao de versao do arquivo\n            - Sao os metodos que possuem 'file' no nome\n    Args:\n        luxorDB_directory (pathlib.Path, optional): Caminho completo ate o diretorio de destino dos dados.\n    \"\"\"\n    self.luxorDB_directory = luxorDB_directory\n\n    if self.luxorDB_directory is None:\n        self.luxorDB_directory = Path(__file__).absolute().parent/\"LuxorDB\"/\"tables\"\n\n    self.tracked_files = {}\n    self.tracked_tables = {}\n</code></pre>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader.add_file_tracker","title":"<code>add_file_tracker(tracked_file_path, filetype='excel', sheet_names={}, excel_size_limit=None, index=False, index_name='index', normalize_columns=False)</code>","text":"<p>Adiciona arquivo na lista para checar por alteracao Args:     tracked_file_path (pathlib.Path): caminho completo ate o arquivo,             incluindo nome do arquivo e extens\u00e3o.     sheet_names (dict, optional): Caso seja uma planilha com varias abas, mapear             aqui o nome da aba para o nome do arquivo de saida desejado.</p> Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>def add_file_tracker(self, tracked_file_path, filetype=\"excel\", sheet_names={}, \n        excel_size_limit = None,index=False, index_name=\"index\",normalize_columns=False):\n    \"\"\" Adiciona arquivo na lista para checar por alteracao\n    Args:\n        tracked_file_path (pathlib.Path): caminho completo ate o arquivo,\n                incluindo nome do arquivo e extens\u00e3o.\n        sheet_names (dict, optional): Caso seja uma planilha com varias abas, mapear\n                aqui o nome da aba para o nome do arquivo de saida desejado.\n    \"\"\"\n    if tracked_file_path not in self.tracked_files:\n        self.tracked_files[tracked_file_path] = {\n                \"last_mtime\" : dt.datetime.timestamp(dt.datetime(2000,1,1)),\n                \"filetype\" : filetype, \"sheet_names\": sheet_names,\n                \"excel_size_limit\" : excel_size_limit,\n                \"index\" : index,\n                \"index_name\" : index_name,\n                \"normalize_columns\" : normalize_columns,\n            }\n</code></pre>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader.add_table_tracker","title":"<code>add_table_tracker(table_name)</code>","text":"<p>Adiciona tabela na lista para controle de alteracao.</p> Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>def add_table_tracker(self, table_name:str):\n    \"\"\" Adiciona tabela na lista para controle de alteracao.\"\"\"\n\n    if table_name not in self.tracked_tables:\n        self.tracked_tables[table_name] = dt.datetime.timestamp(dt.datetime(2000,1,1))\n</code></pre>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader.is_file_modified","title":"<code>is_file_modified(tracked_file_path)</code>","text":"<p>Checa se o arquivo foi modificado desde a ultima leitura. Returns:     tuple(bool, float): (foi modificado?, timestamp da ultima modificacao)</p> Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>def is_file_modified(self, tracked_file_path: Path) -&gt; {bool, float}:\n    \"\"\" Checa se o arquivo foi modificado desde a ultima leitura.\n    Returns:\n        tuple(bool, float): (foi modificado?, timestamp da ultima modificacao)\n    \"\"\"\n\n    file_data = self.tracked_files[tracked_file_path]\n\n    last_saved_time = file_data[\"last_mtime\"]\n    file_last_update = tracked_file_path.stat().st_mtime\n    return file_last_update &gt; last_saved_time, file_last_update\n</code></pre>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader.load_file_if_modified","title":"<code>load_file_if_modified(tracked_file_path, export_to_blob=False, blob_directory='enriched/parquet')</code>","text":"<p>Carrega arquivo no caminho indicado, carregando na base de dados caso modificado. Args:     tracked_file_path (pathlib.Path): caminho ate o arquivo(cadastro previamente por add_file_tracker)     type_map (type, optional): description. Defaults to None.     filetype (str, optional): description. Defaults to \"excel\".</p> Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>def load_file_if_modified(self, tracked_file_path, export_to_blob=False, blob_directory='enriched/parquet'):\n    \"\"\"Carrega arquivo no caminho indicado, carregando na base de dados caso modificado.\n    Args:\n        tracked_file_path (pathlib.Path): caminho ate o arquivo(cadastro previamente por add_file_tracker)\n        type_map (_type_, optional): _description_. Defaults to None.\n        filetype (str, optional): _description_. Defaults to \"excel\".\n    \"\"\"\n    file_data = self.tracked_files[tracked_file_path]\n\n    last_saved_time = file_data[\"last_mtime\"]\n    filetype = file_data[\"filetype\"]\n    file_sheets = file_data[\"sheet_names\"]\n\n    file_last_update = tracked_file_path.stat().st_mtime\n\n    if file_last_update &gt; last_saved_time: # Houve alteracao no arquivo\n        if filetype == \"excel\":\n            file_sheets = None if len(file_sheets) == 0 else list(file_sheets.keys())\n\n            # tables sera sempre um dicionario de tabelas\n            tables = None\n            trials = 25\n            t_counter = 1\n            while trials - t_counter &gt; 0:\n                try:\n                    tables = pd.read_excel(tracked_file_path, sheet_name=file_sheets)\n                    t_counter = trials # leitura concluida\n                except PermissionError:\n\n                    logger.error(f\"Erro ao tentar ler arquivo '{tracked_file_path}.\\nTentativa {t_counter} de {trials};'.\\nSe estiver aberto feche.\")\n                    time.sleep(10)\n                    t_counter += 1\n\n            for sheet_name, table_data in tables.items():\n\n                table_name = sheet_name if file_sheets is None else file_data[\"sheet_names\"][sheet_name]\n\n                if table_name == \"trades\":\n                    table_data[\"ID\"] = table_data.index\n\n                self.__export_table(table_name, table_data, index=file_data[\"index\"], index_name=file_data[\"index_name\"],\n                                        normalize_columns=file_data[\"normalize_columns\"], export_to_blob=export_to_blob,\n                                        blob_directory=blob_directory)\n            self.tracked_files[tracked_file_path][\"last_mtime\"] = file_last_update\n</code></pre>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader.load_table_if_modified","title":"<code>load_table_if_modified(table_name, table_data, last_update, index=False, index_name='index', normalize_columns=False, do_not_load_excel=False, export_to_blob=False, blob_directory='enriched/parquet', is_data_in_bytes=False, bytes_extension='.xlsx')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>nome da tabela (sera o mesmo do arquivo a ser salvo)</p> required <code>table_data</code> <code>DataFrame</code> <p>tabela de dados</p> required <code>last_update</code> <code>timestamp</code> <p>timestamp da ultima edicao feita na tabela</p> required Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>def load_table_if_modified(self, table_name, table_data, last_update, index=False, index_name=\"index\", normalize_columns=False,\n                           do_not_load_excel=False, export_to_blob=False, blob_directory='enriched/parquet',\n                           is_data_in_bytes=False, bytes_extension=\".xlsx\"):\n    \"\"\"\n    Args:\n        table_name (str): nome da tabela (sera o mesmo do arquivo a ser salvo)\n        table_data (pd.DataFrame): tabela de dados\n        last_update (timestamp): timestamp da ultima edicao feita na tabela\n    \"\"\"\n\n    if table_name not in self.tracked_tables:\n        self.add_table_tracker(table_name)\n\n\n    last_update_time = self.tracked_tables[table_name]\n    if last_update &gt; last_update_time:\n\n        self.tracked_tables[table_name] = last_update\n        self.__export_table(table_name, table_data, index=index, index_name=index_name, normalize_columns=normalize_columns,\n                            do_not_load_excel=do_not_load_excel, export_to_blob=export_to_blob, blob_directory=blob_directory,\n                            is_data_in_bytes=is_data_in_bytes, bytes_extension=bytes_extension)\n</code></pre>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader.scan_files","title":"<code>scan_files(export_to_blob=False, blob_directory='enriched/parquet')</code>","text":"<p>Para todos os arquivos cadastrados, vai buscar e carregar quando houver arquivo mais recente.</p> Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>def scan_files(self, export_to_blob=False, blob_directory='enriched/parquet'):\n    \"\"\"\n        Para todos os arquivos cadastrados, vai buscar e carregar quando houver\n        arquivo mais recente.\n    \"\"\"\n\n    for file in self.tracked_files:\n\n        self.load_file_if_modified(file, export_to_blob=export_to_blob, blob_directory=blob_directory)\n</code></pre>"},{"location":"modules/luxorDB_dataloader/#luxorDB_dataloader.DataLoader.text_to_lowercase","title":"<code>text_to_lowercase(t)</code>","text":"<p>Converte todas as colunas de texto para lowercase Args:     t (pd.DataFrame): pandas DataFrame Returns:     pd.DataFrame</p> Source code in <code>LuxorASAP\\luxorDB_dataloader.py</code> <pre><code>def text_to_lowercase(self, t):\n    \"\"\"\n    Converte todas as colunas de texto para lowercase\n    Args:\n        t (pd.DataFrame): pandas DataFrame\n    Returns:\n        pd.DataFrame\n    \"\"\"\n\n    return t.map(lambda x: x.lower().strip() if isinstance(x, str) else x)\n</code></pre>"},{"location":"modules/luxorDB_datareader/","title":"<code>luxorDB_datareader.py</code>","text":"<p>Classe <code>LuxorQuery</code> \u2013 API de acesso/leitura ao LuxorDB.</p> <p>Modulo para leitura e manipulacao dos dados do LuxorDB. Desenvolvido por: Sergio C. Filho Data: 2023-01-01 Versao: 1.0</p> Resumo <p>A classe 'LuxorQuery' fornece uma interface para acessar e manipular dados do LuxorDB. Ela gerencia o carregamento de tabelas (CSV, Parquet, Excel), otimiza o acesso a pre\u00e7os e dados hist\u00f3ricos, e oferece funcionalidades para c\u00e1lculos financeiros como retornos, posi\u00e7\u00f5es e convers\u00f5es de moeda.</p> Explica\u00e7\u00e3o mais detalhada <p>A classe 'LuxorQuery' \u00e9 uma ferramenta abrangente para interagir com o LuxorDB. Ela oferece as seguintes funcionalidades:</p> <ul> <li>Carregamento de Dados Flex\u00edvel: Suporta o carregamento de tabelas de diversas fontes (ADLS, Parquet, Excel)</li> <li>Otimiza\u00e7\u00e3o de Pre\u00e7os e Dados Hist\u00f3ricos: Implementa um cache de pre\u00e7os para consultas r\u00e1pidas e gerencia o acesso a s\u00e9ries hist\u00f3ricas de pre\u00e7os de ativos, VCs, e dados n\u00e3o-Bloomberg.</li> <li>Manipula\u00e7\u00e3o de Dados Financeiros:<ul> <li>Pre\u00e7os: Recupera pre\u00e7os de ativos em datas espec\u00edficas, com convers\u00e3o de moeda e tratamento para dados intraday.</li> <li>Retornos: Calcula retornos percentuais para ativos individuais e grupos, com op\u00e7\u00f5es de ajuste por amortiza\u00e7\u00e3o e per\u00edodos personalizados (YTD, MTD, etc.).</li> <li>Posi\u00e7\u00f5es: Fornece o hist\u00f3rico de posi\u00e7\u00f5es de fundos, incluindo posi\u00e7\u00f5es internas de fundos aninhados.</li> <li>Movimenta\u00e7\u00f5es de Caixa: Acessa e sumariza movimenta\u00e7\u00f5es de caixa por fundo.</li> <li>D\u00edvida a Termo: Calcula a d\u00edvida a termo para um fundo espec\u00edfico.</li> <li>Pre\u00e7o M\u00e9dio: Obt\u00e9m o pre\u00e7o m\u00e9dio de um ativo em um fundo em uma data espec\u00edfica.</li> </ul> </li> <li>Gerenciamento de Datas e Feriados: Inclui fun\u00e7\u00f5es para verificar feriados, calcular dias \u00fateis e determinar datas de in\u00edcio de per\u00edodos financeiros.</li> <li>Benchmarks: Calcula benchmarks compostos, como o benchmark Luxor (S&amp;P + Caixa), com ajuste para diferentes moedas e per\u00edodos hist\u00f3ricos.</li> <li>Normaliza\u00e7\u00e3o de Trades: Converte trades para a moeda desejada, ajustando para BDRs e c\u00e2mbio.</li> <li>Dados de Ativos: Acessa informa\u00e7\u00f5es detalhadas sobre ativos, como tamanho de BDRs e outros campos de dados.</li> </ul> <p>A classe \u00e9 projetada para ser robusta, com tratamento de erros e otimiza\u00e7\u00f5es para garantir o desempenho na recupera\u00e7\u00e3o e manipula\u00e7\u00e3o de grandes volumes de dados financeiros.</p>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery","title":"<code>LuxorQuery</code>","text":"Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>class LuxorQuery:\n\n    def __init__(self, update_mode=\"optimized\", is_develop_mode=False, tables_path=None, \n                 blob_directory='enriched/parquet'):\n\n        \"\"\"\n            update_mode: \n                'standard' - Carrega todas as tabelas disponiveis\n                'optimized' - Carrega apenas as tabelas utilizadas sob demanda\n        \"\"\"\n\n        self.modified_tables = []\n        self.is_develop_mode = is_develop_mode\n        self.blob_directory = blob_directory\n\n        self.tables_path = tables_path\n        if tables_path is None:\n            self.tables_path = self.__set_tables_path()\n\n        self.tables_in_use = {}\n        self.asset_last_prices = {}\n        self.price_cache = {}  # otimizacao da consulta de preco\n        self.price_tables_loaded = {}\n\n\n        self.lipi_manga_incorp_date = dt.date(2022,12,9)\n\n\n        self.update_modes_name = {\"standard\" : 0, \"optimized\" : 1}\n        self.update_mode = self.update_modes_name[update_mode]\n        self.update() # Nessa 1\u00b0 exec. vai inicializar os dicionarios acima\n\n\n\n    def __set_tables_path(self):\n\n        cur_dir = Path().absolute()\n        cur_dir_items = cur_dir.parts\n        home = cur_dir.home()\n        onedrive_name = cur_dir_items[3]\n        if \"OneDrive\".lower() not in onedrive_name.lower():\n            logger.critical(\"No formato atual, para utilizar esse modulo a main deve ser executada dentro do diretorio do OneDrive.\")\n        #formato considerado eh: \"C:/Users/{user}/{onedrive_name}\"\n\n        tables_path = Path(home)/onedrive_name/\"projetos\"/\"LuxorASAP\"/\"luxorDB\"/\"tables\"\n        if self.is_develop_mode:\n            tables_path = Path(home)/onedrive_name/\"projetos\"/\"LuxorASAP_Develop\"/\"luxorDB\"/\"tables\"\n\n        return tables_path\n\n\n    def __is_table_modified(self, table_name):\n        \"\"\" Retorna 'True' ou 'False' informando se a tabela informada em 'table_name' foi criada ou modificada.\n\n        Args:\n            table_name (str): nome tabela\n\n        Returns:\n            bool: True se foi criada ou modificada\n        \"\"\"\n\n        if table_name not in self.tables_in_use:\n            return True\n\n        try:\n            file_path = self.tables_in_use[table_name][\"table_path\"]\n            file_last_update = os.path.getmtime(file_path)\n\n            return file_last_update &gt; self.tables_in_use[table_name][\"update_time\"]\n\n        except:\n            logger.critical(f\"Arquivo &lt;{file_path}&gt; n\u00e3o encontrado.\")\n\n        return False\n\n\n    def __persist_column_formatting(self, t):\n\n        columns_to_persist = {\"Name\", \"Class\", \"Vehicles\", \"Segment\"}\n\n        if len(set(t.columns).intersection(columns_to_persist)) &gt; 0:\n            # Vamos persistir a formatacao de algumas colunas\n            columns_order = list(t.columns)\n            columns_to_persist = list(set(t.columns).intersection(columns_to_persist))\n            persistent_data = t[columns_to_persist].copy()\n\n            columns_to_normalize = list(set(columns_order) - set(columns_to_persist))\n            t = self.text_to_lowercase(t[columns_to_normalize])\n            t.loc[:,columns_to_persist] = persistent_data\n            return t[columns_order]\n\n        # Nos outros casos, transformaremos tudo em lowercase\n        return self.text_to_lowercase(t)\n\n\n    def __get_tickers_bbg(self):\n        # Deprecated. \n        # Criado apenas para manter compatibilidade\n        logger.warning(\"Acesso direto a tabela 'bbg_ticker' sera descontinuado.\\nAo inves disso, pegue os valores unicos de asset[Ticker_BBG]\")\n        return pd.DataFrame(self.get_table(\"assets\")[\"Ticker_BBG\"].unique())\n\n\n\n    def get_table(self, table_name, index=False, index_name=\"index\", dtypes_override={}, force_reload=False):\n        \"\"\"\n            Retorna uma copia do DataFrame do 'table_name' correspondente. Se n\u00e3o estiver disponivel,\n            retorna None.\n\n            table_name: \n                'px_last'              - \u00daltimo preco dos ativos\\n\n                'trades'               - Historico de boletas dos trades da Luxor\\n\n                'assets'               - Tabela de ativos validos\\n\n                'hist_px_last'         - Pre\u00e7o hist\u00f3rico dos ativos desde 1990\\n\n                'hist_vc_px_last'      - Pre\u00e7o hist\u00f3rico dos VCs\\n\n                'hist_non_bbg_px_last' - Pre\u00e7o hist\u00f3rico de ativos sem preco no bbg (fidc trybe, spx hawker)\\n\n                '[NOME_FUNDO]_quotas   - Cotas historicas do fundo (fund_a, fund_b, hmx, ...\\n\n                'hist_us_cash'         - Historico de caixas dos fundos (us apenas)\\n\n                'bbg_tickers'          - Ticker bbg de todos os tickers cadastrados\\n\n                'bdr_sizes'            - Tabela com o ultimo peso de cada bdr\\n\n                'hist_swap'            - Tabela historica dos swaps\\n\n                'hist_positions'       - Historico de posicoes dos fundos\\n\n                'hist_positions_by_bank- Posicoes por banco\\n\n                'cash_movements'       - Historico de movimentacoes com data de liquidacao\\n\n                'holidays'             - Tabela de feriados das bolsas\\n\n                'daily_pnl'            - Tabela de PnL diario\\n\n                'custom_funds_quotas'  - Cotas dos fundos customizados(luxor equities, non-equities, etc)\\n\n\n            dtypes_override: dict : set - Dicionario com os tipos de dados das colunas devem ser sobrescritos.\n                Deve possuir as chaves 'float', 'date', 'bool' e 'str_nan_format'(troca 'nan' por pd.NA)\n                    Para cada chave, colocar um Set com os nomes das colunas que receberao o cast.\n        \"\"\"\n        table_name = table_name.lower().replace(\" \", \"_\")\n        if table_name == 'bbg_tickers': return self.__get_tickers_bbg() # DEPRECATED TODO: remover apos testes\n\n        if (table_name in self.tables_in_use) and not force_reload:\n            return self.tables_in_use[table_name][\"table_data\"]\n\n        return self.__load_table(table_name, index=index, index_name=index_name, dtypes_override=dtypes_override)\n\n\n\n    def __load_table(self, table_name, index=False, index_name=\"index\", dtypes_override={}):\n\n\n        def __read_blob_parquet(table_name):\n\n            connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n            blob_service_client = BlobServiceClient.from_connection_string(conn_str=connection_string)\n\n            container_name = \"luxorasap\"\n            blob_name = f\"{self.blob_directory}/{table_name}.parquet\"\n\n            blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n\n            # Download do blob em mem\u00f3ria\n            download_stream = None\n            try:\n                download_stream = blob_client.download_blob()\n            except Exception:\n                print(f\"Tabela '{table_name}' n\u00e3o encontrada no blob.\")\n                return None, False\n            parquet_data = download_stream.readall()\n\n            # Ler o parquet do stream em mem\u00f3ria\n            parquet_buffer = io.BytesIO(parquet_data)\n            table = pq.read_table(parquet_buffer)\n            df = table.to_pandas()\n\n            return df, True\n\n\n        def __load_parquet(table_name):\n            table_path = self.tables_path/\"parquet\"/f\"{table_name}.parquet\"\n\n            try:\n                update_time = os.path.getmtime(table_path)\n                table_data = None\n                # Primeiro, vamos tentar ler do blob\n                table_data, blob_read_success = __read_blob_parquet(table_name)\n\n                if not blob_read_success:\n                    print(\"--&gt; Onedrive fallback.\")\n                    table_data = pd.read_parquet(table_path,engine=\"fastparquet\")\n\n                table_columns = set(table_data.columns)\n\n                float_dtypes = {\"Last_Price\", \"Price\", \"px_last\", \"Quota\", \"#\", \"Avg_price\", \"Variation\", \"Variation_tot\",\n                                \"Value\", \"%_MoM\", \"PL\", \"AUM\", \"%_pl\", \"Market_Value\", \"P&amp;L_MoM\", \"Debt\", \"Kd\", \"P&amp;L_YTD\", \"Return_Multiplier\",\n                                \"Weight\", \"%_YTD\", \"Net_Debt/Ebitda\", \"adr_adr_per_sh\", \"Volume\", \"Total\",\"Liquidity_Rule\",\n                                \"Daily_Return\", \"Amount\", \"Adjust\", \"Year\", \"Daily_Adm_Fee\", \"Timestamp\"}\n                if 'float' in dtypes_override:\n                    float_dtypes = float_dtypes.union(dtypes_override['float'])\n\n                date_dtypes = {\"Date\", \"last_update_dt\", \"Query_Time\", \"update_time\", \"Update_Time\",\n                               \"Settlement Date\", \"Today\"}\n                if 'date' in dtypes_override:\n                    date_dtypes = date_dtypes.union(dtypes_override['date'])\n\n                bool_dtypes = {\"Ignore Flag\", \"Disable BDP\", \"Hist_Price\", \"Historical_Data\"}\n                if 'bool' in dtypes_override:\n                    bool_dtypes = bool_dtypes.union(dtypes_override['bool'])\n\n                str_nan_format = {\"Ticker_BBG\"}\n                if 'str_nan_format' in dtypes_override:\n                    str_nan_format = str_nan_format.union(dtypes_override['str_nan_format'])\n\n                if table_name != \"last_all_flds\":\n                    for col in table_columns.intersection(float_dtypes):\n                        table_data[col] = table_data[col].astype(float)\n\n                if table_name == \"hist_risk_metrics\":\n                    try:\n                        cols_to_format = list(set(table_data.columns) - {\"Date\", \"Fund\"})\n                        table_data[cols_to_format] = table_data[cols_to_format].astype(float)\n                    except ValueError:\n                        logger.warning(\"Ao carregar tabela 'hist_px_last', nao foi possivel converter dados para float.\")\n\n                for col in table_columns.intersection(date_dtypes):\n                    try:\n                        table_data[col] = pd.to_datetime(table_data[col], format=\"mixed\")\n\n                    except ValueError:\n                        table_data[col] = table_data[col].apply(lambda x: pd.to_datetime(x))\n\n                for col in table_columns.intersection(bool_dtypes):\n                    table_data[col] = (table_data[col].str.lower()\n                                                    .replace(\"false\", \"\").replace(\"falso\", \"\")\n                                                    .replace(\"0\", \"\").replace(\"nan\", \"\").astype(bool))\n\n                for col in table_columns.intersection(str_nan_format):\n                    table_data[col] = table_data[col].replace(\"nan\", pd.NA).replace(\"\", pd.NA)\n\n\n\n                return table_data.copy(), table_path, update_time\n\n            except Exception:\n                logger.warning(f\"Nao foi possivel carregar a tabela &lt;{table_name}&gt; no formato .parquet\")\n                return None, None, None\n\n        def __load_csv(table_name):\n            table_path = self.tables_path/\"csv\"/f\"{table_name}.csv\"\n\n            try:\n                update_time = os.path.getmtime(table_path)\n                table_data = pd.read_csv(table_path, sep=\";\")\n                return table_data.copy(), table_path, update_time\n            except Exception:\n                logger.warning(f\"Nao foi possivel carregar a tabela &lt;{table_name}&gt; no formato .csv\")\n                return None, None, None\n\n        def __load_excel(table_name):\n            try:\n                table_path = self.tables_path/f\"{table_name}.xlsx\"\n                update_time = os.path.getmtime(table_path)\n                # Nao deixar crashar caso nao consiga ler do excel !\n                table_data = pd.read_excel(table_path)\n                logger.warning(f\"Tabela {table_name} carregada do arquivo em excel. Limite 1M de linhas.\")\n                return table_data.copy(), table_path, update_time\n            except FileNotFoundError:\n                return None, table_path, None       \n        # Tentando carregar, do mais eficiente pro menos eficiente.\n        table_data, table_path, update_time = __load_parquet(table_name)\n        #if table_data is None: CSV DESCONTINUADO POR FALTA DE USO\n        #    table_data, table_path, update_time = __load_csv(table_name)\n        if table_data is None:\n            table_data, table_path, update_time = __load_excel(table_name)\n\n        #assert(table_data is not None)\n        if table_data is None:\n            logger.error(f\"Nao foi possivel carregar a tabela &lt;{table_name}&gt;.\")\n            return table_data\n\n        if index:\n            try: \n                table_data = table_data.set_index(index_name, drop=True)\n\n            except Exception:\n                logger.error(f\"Nao foi poss\u00edvel setar a coluna {index_name} como index para a tabela {table_name}.\")\n\n        table_data = self.__persist_column_formatting(table_data)\n\n        self.tables_in_use[table_name] = {\"table_data\" : table_data,\n                                          \"table_path\" : table_path,\n                                          \"update_time\" : update_time\n                                          }\n        return table_data\n\n\n    def __load_table_group(self, table_keys):\n\n        price_tables_loaded_flag = False\n\n        for table_key in table_keys:\n\n            index = table_key in [\"px_last\", \"last_all_flds\"]\n            index_name = \"Key\" if table_key in [\"px_last\", \"last_all_flds\"] else None\n            self.__load_table(table_key, index=index, index_name=index_name)\n\n            if table_key == \"px_last\":\n                self.asset_last_prices = self.get_table(table_key).to_dict(\"index\")\n\n            elif table_key == \"last_all_flds\":\n                self.last_all_flds = self.get_table(table_key).to_dict(\"index\")\n\n            elif table_key in [\"hist_px_last\", \"hist_non_bbg_px_last\", \"hist_vc_px_last\", \"all_funds_quotas\", \"custom_funds_quotas\", \"custom_prices\"]:\n                price_tables_loaded_flag = True\n                table = (self.get_table(table_key)\n                            .rename(\n                                columns={\"Fund\" : \"Asset\", \"Quota\" : \"Last_Price\", \"Ticker\" : \"Asset\",\n                                            \"Price\" : \"Last_Price\"\n                                            })[[\"Date\",\"Asset\",\"Last_Price\"]]\n                        )\n                if table_key == 'hist_vc_px_last':\n\n                    last_prices = table.groupby(\"Asset\")[\"Last_Price\"].last()\n                    last_date_df = pd.DataFrame({\"Date\": [dt.datetime.now()] * len(last_prices)})\n                    last_date_df[\"Asset\"] = last_prices.index\n                    last_date_df[\"Last_Price\"] = last_prices.values\n\n                    table = (pd.concat([table, last_date_df]).sort_values(by=\"Date\")\n                                                            .set_index(\"Date\").groupby(\"Asset\")\n                                                            .apply(lambda g: g[~g.index.duplicated(keep=\"first\")].resample(\"D\").ffill()).reset_index(level=0, drop=True)\n                                                            .reset_index())\n\n                self.price_tables_loaded[table_key] = table\n\n        if ('px_last' in table_keys) and ('hist_px_last' in self.tables_in_use):\n            # Vamos pegar o px_last e colocar na tabela de precos historicos, atualizando ultima ocorrencia\n            hist_prices = self.get_table(\"hist_px_last\")\n            hist_prices = self.__update_hist_px_last_intraday(hist_prices)\n            self.price_tables_loaded[\"hist_px_last\"] = hist_prices\n            price_tables_loaded_flag = True\n\n\n        if price_tables_loaded_flag: # Somente se a execucao alterou o estado de \u00b4price_tables_loaded  \n            self.hist_prices_table = pd.concat(self.price_tables_loaded.values()).dropna()\n\n\n    def update(self, update_attempts_limit=8):\n        \"\"\"\n            Atualiza todas as tabelas em uso.\n        \"\"\"\n\n        update_attempts = 0\n        update_success = False\n        self.price_cache = {}  # -&gt; reset da otimizacao da consulta de preco \n\n        while not update_success and (update_attempts &lt; update_attempts_limit):\n            try:\n                update_attempts += 1\n                for table_key in self.tables_in_use:\n                    # Verificando se tabela foi criada ou modificada\n                    if self.__is_table_modified(table_key):                        \n                        self.__load_table_group([table_key])\n\n\n                update_success = True\n\n            except PermissionError:\n                hist_prices_tables = [] # desconsidera appends feitos no loop nao concluido\n                logger.error(\"N\u00e3o foi poss\u00edvel carregar as tabelas pois tem algum arquivo aberto.\")\n                logger.info(f\"Tentativas de atualiza\u00e7\u00e3o: {update_attempts} de {update_attempts_limit}\")\n                time.sleep(30)\n\n            except:\n                logger.error(\"N\u00e3o foi possivel carregar as tabelas.\")\n                logger.info(f\"Tentativas de atualiza\u00e7\u00e3o: {update_attempts} de {update_attempts_limit}\")\n                time.sleep(5*update_attempts)\n\n        if not update_success:\n            logger.critical(\"Nao foi possivel atualizar os dados. Execu\u00e7\u00e3o finalizada.\")\n\n\n    def text_to_lowercase(self, t):\n        \"\"\"\n        Converte todas as colunas de texto para lowercase\n        Args:\n            t (dt.DataFrame): pandas DataFrame\n        Returns:\n            dt.DataFrame\n        \"\"\"\n        try:\n            return t.map(lambda x: x.lower().strip() if isinstance(x, str) else x)\n        except AttributeError:\n            logger.warning(\"Pendente de atualizacao para o python 3.12.2\")\n            return t.applymap(lambda x: x.lower().strip() if isinstance(x, str) else x)\n\n    def get_px_update_time(self):\n        \"\"\" Informa Hor\u00e1rio da ultima atualizacao da base de precos.\n        Returns:\n            dt.datetime\n        \"\"\"\n        time = self.get_table(\"px_last\")[\"Query_Time\"].max()\n        return dt.time(time.hour, time.minute, time.second)\n\n\n    def get_price_key(self, asset_key):\n        \"\"\" Retorna a chave correta a ser usada para consultar o preco/rentabilidade do ativo.\"\"\"\n\n        asset = self.get_table(\"assets\").query(\"Key == @asset_key\").squeeze()\n\n        if asset[\"Group\"] in [\"vc\", \"luxor\"]:\n            return asset[\"Ticker\"]\n\n        # Verificando se ha valor valido de ticker bbg\n        if type(asset[\"Ticker_BBG\"]) == type(\"\"):\n\n            return asset[\"Ticker_BBG\"]\n        # caso nao haja, sera usado o ticker\n        return asset[\"Ticker\"]\n\n\n    def get_price(self, ticker, px_date=None, logger_level=\"trace\", dr_adjusted=False,\n        currency=\"local\", usdbrl_ticker=\"bmfxclco curncy\", asset_location=\"us\"):\n        \"\"\" Informa o pre\u00e7o do ativo. Quando px_date nao eh informado, retorna o\n        pre\u00e7o mais recente.\n\n        Args:\n            ticker (str): identificador do ativo.\n            px_date (dt.date, optional): Data de referencia.\n            logger_level (str, optional): Defaults to \"trace\".\n\n        Returns:\n            float: pre\u00e7o do ativo.\n        \"\"\"\n\n        if pd.isna(ticker): return None\n\n\n        currency_factor = 1\n        if currency != \"local\":\n            try:\n                if asset_location == \"bz\" and currency == \"usd\":\n                    usdbrl = self.get_price(usdbrl_ticker, px_date=px_date)\n                    currency_factor = 1/usdbrl\n                elif asset_location == \"us\" and currency == \"brl\":\n                    usdbrl = self.get_price(usdbrl_ticker, px_date=px_date)\n                    currency_factor = usdbrl\n            except ValueError:\n                logger.error(f\"Erro ao converter moeda para {currency} para o ticker '{ticker}'.\")\n                currency_factor = 1\n\n        ticker = ticker.lower()\n\n        # dados do usd cupom limpo comecam em abril de 2011. Antes disso vamos pergar usdbrl normalmente\n        if (ticker == \"bmfxclco curncy\") and (px_date is not None) and (px_date &lt; dt.date(2011,4,14) or (px_date is not None and px_date == dt.date.today())) :\n            ticker = \"usdbrl curncy\"\n\n        px_last_at_date = None\n\n        cache_key = ticker + str(px_date) # chave unica do preco para essa data\n        # se o preco foi consultado recentemente, podemos retorna-lo rapidamente.\n        if cache_key in self.price_cache:\n            return self.price_cache[cache_key] * currency_factor\n\n        if (px_date is None) or (px_date&gt;= dt.date.today()):\n            # Nesse caso retornamos o mais recente utilizando o dicionario\n\n            if \"px_last\" not in self.tables_in_use:\n                self.__load_table_group([\"px_last\"])\n\n            #if ticker in self.asset_last_prices.keys():\n            try:\n                px_last_at_date = self.asset_last_prices[ticker][\"px_last\"]\n                self.price_cache[cache_key] = px_last_at_date\n            except:\n                price_1_tickers = ['fip mission 1.1', 'caixa', 'caixa us']\n                if ticker in price_1_tickers :\n                    px_last_at_date = 1\n                    self.price_cache[cache_key] = px_last_at_date\n                else:\n                #if px_last_at_date is None:\n                    if logger_level == \"trace\":\n                        logger.trace(f\"Pre\u00e7o nao disponivel para o ticker '{ticker}'. Pre\u00e7o setado para 0.\")\n                    elif logger_level == \"info\":\n                        logger.info(f\"Pre\u00e7o nao disponivel para o ticker '{ticker}'. Pre\u00e7o setado para 0.\")\n                    else: # logger_level == \"erro\":\n                        logger.error(f\"Pre\u00e7o nao disponivel para o ticker '{ticker}'. Pre\u00e7o setado para 0.\")\n                    px_last_at_date = 0\n\n\n            return px_last_at_date * currency_factor\n\n        # Vamos olhar em cada tabela de precos procurando pelo ticker informado\n        if not self.price_tables_loaded:\n            self.__load_table_group([\"hist_px_last\", \"hist_non_bbg_px_last\", \"hist_vc_px_last\",\n                                     \"all_funds_quotas\", \"custom_funds_quotas\", \"custom_prices\"])\n\n\n        try:\n            # Busca otimizada do preco pelo ativo e pela data informada\n            #px_last_at_date = (self.hist_prices_table[\"Last_Price\"]\n            #                    .to_numpy()[(\n            #                            (self.hist_prices_table[\"Date\"].dt.date.to_numpy() &lt;= px_date) \n            #                            &amp; \n            #                            (self.hist_prices_table[\"Asset\"].to_numpy() == ticker) \n            #                            )].item(-1)\n            #                    )\n            #return  px_last_at_date\n\n            px_last_at_date = self.hist_prices_table.query(\"Date &lt;= @px_date and Asset == @ticker\")\n\n            return px_last_at_date.tail(1)[\"Last_Price\"].squeeze() * currency_factor if len(px_last_at_date) &gt; 0 else 0\n\n        except (IndexError , KeyError):\n            # Nao achou o ativo em nenhuma das tabelas, retorna 0\n            if logger_level == \"trace\":\n                logger.trace(f\"Pre\u00e7o nao disponivel para o tikcker '{ticker}'. Pre\u00e7o setado para 0.\")\n            elif logger_level == \"info\":\n                logger.info(f\"Pre\u00e7o nao disponivel para o tikcker '{ticker}'. Pre\u00e7o setado para 0.\")\n            else: # logger_level == \"erro\":\n                logger.error(f\"Pre\u00e7o nao disponivel para o tikcker '{ticker}'. Pre\u00e7o setado para 0.\")\n\n            logger.trace(f\"Pre\u00e7o nao disponivel para o tikcker '{ticker}'. Pre\u00e7o setado para 0.\")\n            self.price_cache[cache_key] = px_last_at_date\n            return 0\n\n\n    def usdbrl_clean_coupon_fix(self, usdbrl_df):\n        \"\"\" Corrige o problema do ticker bmfxclco curncy nao ter dados intraday.\n            Usa a variacao intraday do usdbrl curncy\n        Args:\n            usdbrl_df (pd.DataFrame): dataframe com precos historicos do bmfxclco curncy\n        \"\"\"\n        max_date = usdbrl_df[\"Date\"].max().date()\n        today = dt.date.today()\n        if max_date &lt; today:\n            # vamos pegar a variacao do usdbrl curncy\n            var_usdbrl = self.get_pct_change('usdbrl curncy', \n                                               recent_date=today,\n                                               previous_date=max_date)\n            # vamos pegar o last_price em max_date\n            last_price = usdbrl_df.query(\"Date == @max_date\")[\"Last_Price\"].squeeze()\n            # vamos ajustar com a variacao do usdbrl\n            last_price = last_price * (1 + var_usdbrl)\n            #vamos colocar na base na data de hoje\n            usdbrl_df = pd.concat([usdbrl_df, \n                        pd.DataFrame({\"Date\":[today],\n                                      \"Asset\":['bmfxclco curncy'], \"Last_Price\":[last_price]})])\n\n            usdbrl_df[\"Date\"] = pd.to_datetime(usdbrl_df[\"Date\"])\n\n        return usdbrl_df\n\n\n    def __update_hist_px_last_intraday(self, hist_prices):\n\n        tickers = list(hist_prices[\"Asset\"].unique())\n        px_last = self.get_table(\"px_last\", index=True, index_name=\"Key\").copy()\n        # Selecionando todos os tickers que estao na tabela px_last\n        px_last = px_last.query(\"Key in @tickers\").reset_index()\n        # Vamos alterar a ultima ocorrencia desses tickers na hist_prices pelos valores na px_last\n        # vamos fazer de forma vetorizada]\n        hist_prices_columns = list(hist_prices.columns)\n        hist_prices = hist_prices.merge(px_last, left_on=[\"Asset\", \"Date\"],\n                                        right_on=[\"Key\", \"last_update_dt\"], how=\"left\")\n        hist_prices[\"Last_Price\"] = np.where(~hist_prices['px_last'].isna(),\n                                             hist_prices['px_last'], \n                                             hist_prices['Last_Price'])\n        hist_prices = hist_prices[hist_prices_columns]\n\n        return hist_prices   \n\n\n    def __hist_prices_intraday_extensor(self, hist_prices):\n        \"\"\" Completa a tabela de precos historicos com o preco intraday mais recente.\n\n        Args:\n            hist_prices (pd.DataFrame): dataframe de precos historicos\n        \"\"\"\n\n        # Verificar cada Asset unico existente na tabela\n        assets = hist_prices[\"Asset\"].unique()\n        today = dt.date.today()\n        updated_dfs = []\n        # vamos fazer cada ativo por vez\n        for asset in assets:\n            # vamos pegar a tabela de precos do ativo\n            asset_prices = hist_prices.query(\"Asset == @asset\")\n            # Vamos pegar a data mais recente\n            last_date = asset_prices[\"Date\"].max().date()\n            if last_date == today:\n                last_price = self.get_price(asset)\n                asset_prices.loc[asset_prices[\"Date\"] == last_date, \"Last_Price\"] = last_price\n                updated_dfs.append(asset_prices.copy())\n                continue\n            # vamos pegar o ultimo preco presente no df\n            last_price = asset_prices.query(\"Date == @last_date\")[\"Last_Price\"].squeeze()\n            # vamos pegar a variacao ate o momento mais recente\n            query_asset = asset\n            if asset == 'bmfxclco curncy':\n                query_asset = 'usdbrl curncy'\n            variation = 0\n            if today &gt; last_date:\n                variation = self.get_pct_change(query_asset, recent_date=today, previous_date=last_date)\n            # vamos ajustar o preco com a variacao\n            last_price = last_price * (1 + variation)\n            # vamos colocar na base na data de hoje\n            updated_dfs.append(pd.concat([asset_prices, \n                        pd.DataFrame({\"Date\":[today],\n                                      \"Asset\":[asset], \"Last_Price\":[last_price]})]))\n\n        updated_df = pd.concat(updated_dfs)\n        updated_df[\"Date\"] = pd.to_datetime(updated_df[\"Date\"])\n\n        return updated_df\n\n\n\n    def get_prices(self, tickers=None, recent_date=dt.date.today(), previous_date=dt.date.today()-dt.timedelta(days=30),\n                         period=None, currency=\"local\", usdbrl_ticker=\"bmfxclco curncy\", force_continuous_date_range=True,\n                         holiday_location=\"all\", get_intraday_prices=False, force_month_end=False):\n        \"\"\"\n            Filtra o historico de pre\u00e7os pelos tickers e pelo periodo informado.\n\n        Args:\n            recent_date (dt.date, optional): data de fim. Por padrao, data de hoje.\n            previous_date (dt.date, optional): data de inicio. Por padrao, 30 dias antes de hoje.\n            tickers (list|set, optional): Lista de tickers que devem ser incluidos, quando existirem.\n            period(str): ytd, mtd ou 'xm' onde 'x' \u00e9 o numero de meses.\n            currency(str): codigo de 3 caracteres da moeda ou 'all' para considerar a moeda local de cada ativo.\n            period(str): ytd|mtd|'xm' onde 'x' \u00e9 o numero de meses. Usara como base o parametro 'recent_date'\n            holiday_location(str): all|any|us|bz ... ver metodo 'is_holiday'\n\n        Returns:\n            pd.DataFrame: Tabela de precos filtrada e convertida para moeda desejada\n        \"\"\"\n        if period is not None:\n            previous_date = self.get_start_period_dt(recent_date, period, holiday_location=holiday_location, force_bday=True,\n                                                     force_month_end=force_month_end)\n\n        if recent_date &lt; previous_date:\n            logger.warning(\"Possivel inversao dos parametros de inicio e fim do periodo.\")\n            temp = recent_date\n            recent_date = previous_date\n            previous_date = temp\n\n        if not self.price_tables_loaded:\n            try:\n                self.__load_table_group([\"hist_px_last\", \"hist_non_bbg_px_last\", \"hist_vc_px_last\", \"all_funds_quotas\",\n                                     \"custom_funds_quotas\", \"custom_prices\", \"px_last\"])\n            except FileNotFoundError:\n                # Vamos tentar sem o custom_funds_quotas, pois pode nao existir ainda.\n                self.__load_table_group([\"hist_px_last\", \"hist_non_bbg_px_last\", \"hist_vc_px_last\",\n                                         \"all_funds_quotas\", \"custom_prices\", \"px_last\"])\n\n        if tickers is None:\n            #prices = self.hist_prices_table.query(\"Date &lt;= @recent_date and Date &gt;= @previous_date\")\n            # Nesse caso, tickers sera uma lista com todos os ativos da tabela de precos\n            tickers = self.hist_prices_table[\"Asset\"].unique()\n\n        if isinstance(tickers, str):\n                tickers = [tickers]\n\n        tickers = [t.lower() for t in tickers] # padronizando tickers para lowercase\n\n        prices = self.hist_prices_table.copy()\n\n\n        surrogate_previous_date = previous_date - dt.timedelta(days=30)\n        surrogate_recent_date = recent_date + dt.timedelta(days=30)\n        prices = prices.query(\"Date &lt;= @surrogate_recent_date and Date &gt;= @surrogate_previous_date\\\n                                   and Asset.isin(@tickers)\")\n\n        if force_continuous_date_range:\n            # Vamos ajustar as datas logo aqui, caso flag esteja ativa\n            prices = prices.set_index(\"Date\").groupby(\"Asset\")\\\n                            .resample(\"D\").last().ffill()\\\n                            .reset_index(level=0, drop=True).reset_index()\n\n        if get_intraday_prices:\n            prices = self.__hist_prices_intraday_extensor(prices)\n\n        if currency != \"local\":\n\n            # TODO:  Resolver problema de consistencia com ticker e ticker_bbg \n\n            assets= self.get_table(\"assets\").copy().query(\"Type != 'a\u00e7\u00f5es_bdr' and Asset != 'spx eagle'\")\n\n            ticker_map = assets.query(\"~Ticker_BBG.isna() and Ticker_BBG != Ticker\")[[\"Ticker\", \"Ticker_BBG\"]].set_index(\"Ticker\").to_dict(\"index\")\n            assets[\"Ticker\"] = assets[\"Ticker\"].apply(lambda x: ticker_map[x][\"Ticker_BBG\"] if x in ticker_map.keys() else x)\n\n            prices = pd.merge(assets[[\"Ticker\", \"Location\"]], prices, left_on=\"Ticker\", right_on=\"Asset\")[[\"Date\", \"Asset\", \"Last_Price\", \"Location\"]]\n            currency_map = {\"bz\":\"brl\", \"us\":\"usd\", \"cn\":\"cad\", \"eur\":\"eur\"}\n            prices[\"Location\"] = prices[\"Location\"].apply(lambda x: currency_map[x])\n\n            prices_by_location = []\n            iter_prices = prices.groupby(\"Location\")\n\n            for p in iter_prices:\n                price_currency = p[0]\n                prices_by_location.append(self.convert_currency(p[1][list(set(p[1].columns) - {\"Location\"})], \n                                                                price_currency=price_currency, dest_currency=currency,\n                                                                usdbrl_ticker=usdbrl_ticker, force_continuous_date_range=force_continuous_date_range,\n                                                                holiday_location=holiday_location, get_intraday_prices=get_intraday_prices,\n                                                                force_month_end=force_month_end))\n\n\n            prices = pd.concat(prices_by_location)[[\"Date\", \"Asset\", \"Last_Price\"]].sort_values(by=[\"Asset\", \"Date\"])\n\n        # Finalmente, vamos seguir filtrando pelo periodo desejado.\n        prices = prices.query(\"Date &lt;= @recent_date and Date &gt;= @previous_date and Asset.isin(@tickers)\")\n\n        #if adjust_bmfxlcoc and ('bmfxclco curncy' in tickers) and (recent_date == dt.date.today()):\n        #    max_date = prices.query(\"Asset == 'bmfxclco curncy'\")[\"Date\"].max()\n        #    if max_date &lt; dt.date.today(): # vamos colocar mais um dia usando usdbrl curncy\n        #        var_usdbrl1d = self.get_pct_change('usdbrl curncy', \n        #                                           recent_date=dt.date.today(),\n        #                                           previous_date=max_date)\n        #        # vamos pegar o last_price em max_date\n        #        last_price = self.get_price('bmfxclco curncy', px_date=max_date)\n        #        # vamos ajustar com a variacao do usdbrl\n        #        last_price = last_price * (1 + var_usdbrl1d)\n        #        #vamos colocar na base na data de hoje\n        #        prices = pd.concat([prices, \n        #                    pd.DataFrame({\"Date\":[dt.date.today()],\n        #                                  \"Asset\":['bmfxclco curncy'], \"Last_Price\":[last_price]})])\n\n        return prices\n\n\n    def convert_currency(self, prices, price_currency, dest_currency, usdbrl_ticker=\"bmfxclco curncy\",\n                         force_continuous_date_range=True, holiday_location=\"all\",\n                         get_intraday_prices=False, force_month_end=False):\n\n        if price_currency == dest_currency: return prices\n\n        convertion_rule = {\"brl_usd\" : {\"currency_ticker\":usdbrl_ticker,\n                                        \"operation\":\"divide\"},\n                           \"usd_brl\" : {\"currency_ticker\":usdbrl_ticker,\n                                        \"operation\":\"multiply\"},\n                           \"eur_usd\" : {\"currency_ticker\":\"usdeur curncy\",\n                                        \"operation\":\"divide\"},\n                           \"usd_eur\" : {\"currency_ticker\":\"usdeur curncy\",\n                                        \"operation\":\"multiply\"},\n                           \"usd_cad\" : {\"currency_ticker\":\"cad curncy\",\n                                        \"operation\":\"multiply\"},\n                           \"cad_usd\" : {\"currency_ticker\":\"cad curncy\",\n                                        \"operation\":\"divide\"}, \n                                        }\n\n\n        convertion_ticker = convertion_rule[price_currency+\"_\"+dest_currency][\"currency_ticker\"]\n        convertion_operation = convertion_rule[price_currency+\"_\"+dest_currency][\"operation\"]\n\n        previous_date = prices[\"Date\"].min()\n        recent_date = prices[\"Date\"].max()\n\n        currencies = self.get_prices(convertion_ticker, previous_date=previous_date, recent_date=recent_date,\n                                     force_continuous_date_range=force_continuous_date_range, \n                                     holiday_location=holiday_location, get_intraday_prices=get_intraday_prices,\n                                     force_month_end=force_month_end).copy()\n        if convertion_operation == \"divide\":\n            currencies[\"Last_Price\"] = 1/currencies[\"Last_Price\"]\n\n        prices = pd.merge(prices, currencies[[\"Date\", \"Last_Price\"]].rename(columns={\"Last_Price\":\"Currency\"}), on=\"Date\")[list(prices.columns) + [\"Currency\"]]\n\n        prices[\"Last_Price\"] = prices[\"Last_Price\"] * prices[\"Currency\"]\n\n        return prices[list(set(prices.columns) - {\"Currency\"}) ]\n\n\n    def get_data(self, ticker, flds, data_date=None, logger_level=\"trace\"):\n\n        flds = flds.lower().replace(\" \",\"_\")\n\n        if flds == \"px_last\":\n            return self.get_price(ticker, px_date=data_date, logger_level=logger_level)\n\n        if data_date is None:\n            # Vamos buscar o dado mais recente\n            data_value = None\n            try:\n                data_value = self.last_all_flds[ticker+\"_\"+flds][\"Value\"]\n            except AttributeError:\n                self.__load_table_group([\"last_all_flds\"])\n                try:\n                    data_value = self.last_all_flds[ticker+\"_\"+flds][\"Value\"]    \n                except KeyError:\n                    if logger_level == \"trace\":\n                        logger.trace(f\"Dado de {flds} nao disponivel para o ticker '{ticker}'.\")\n                    elif logger_level == \"info\":\n                        logger.info(f\"Dado de {flds} nao disponivel para o ticker '{ticker}'.\")\n                    else: # logger_level == \"erro\":\n                        logger.error(f\"Dado de {flds} nao disponivel para o ticker '{ticker}'.\")\n                    return None\n            except KeyError:\n                if logger_level == \"trace\":\n                    logger.trace(f\"Dado de {flds} nao disponivel para o ticker '{ticker}'.\")\n                elif logger_level == \"info\":\n                    logger.info(f\"Dado de {flds} nao disponivel para o ticker '{ticker}'.\")\n                else: # logger_level == \"erro\":\n                    logger.error(f\"Dado de {flds} nao disponivel para o ticker '{ticker}'.\")\n                return None\n\n            # Formatando o valor retornado\n            data_type = self.get_table(\"field_map\").query(\"Field == @flds\")[\"Value_Type\"].squeeze()\n\n            if data_type == 'numerical':\n                return float(data_value)\n            if data_type == \"date\":\n                return dt.datetime.fromtimestamp(float(data_value))\n            if data_type != \"text\":\n                logger.warning(f\"field '{flds}' nao foi cadastrado na tabela field_map.\")\n            return data_value\n\n        # Buscamos por dado numa data especifica. \n        hist_all_flds = self.get_table(\"hist_all_flds\")\n\n        try:\n            data = hist_all_flds.query(\"Date &lt;= @data_date and Field == @flds and Ticker == @ticker\")[\"Value\"]\n\n            if len(data) &gt; 0:\n                return data.tail(1).squeeze()\n        except KeyError:\n            logger.error(f\"Dado de {flds} nao disponivel para o ticker '{ticker}'.\")\n        # Nenhum dado encontrado para a data informada\n        return None\n\n\n    def get_bdr_size(self, ticker):\n        if ticker == 'mcor34 bz equity': \n            return 4 #TODO remover quando atualizacao da table for reestabelecida\n        sizes = self.get_table(\"bdr_sizes\", index=True, index_name=\"Ticker\")\n\n        return sizes.loc[ticker.lower()].squeeze()    \n\n\n    def get_cash_movements(self, fund_name, ref_date):\n\n        cash_movements = self.get_table(\"cash_movements\")\n        #cash_movements[\"Settlement Date\"] = cash_movements[\"Settlement Date\"].dt.date\n        #cash_movements[\"Date\"] = pd.to_datetime(cash_movements[\"Date\"]).dt.date #Conversao para Date s\u00f3 funcionou dessa maneira\n        cash_movements = cash_movements.query(\"(Fund == @fund_name) and (Date &gt; @ref_date)\")[[\"Type\", \"Volume\"]]\n        #cash_movements = cash_movements.loc[ ((cash_movements[\"Fund\"] == fund_name) &amp; (cash_movements[\"Date\"] &gt; ref_date)) ] [[\"Type\", \"Volume\"]]\n        #cash_movements.to_excel(\"temmmp.xlsx\")\n        cash_movements = dict(cash_movements.groupby(\"Type\")[\"Volume\"].sum())\n\n        return cash_movements\n\n\n    def get_avg_price(self, fund_name, asset_key, date=None):\n\n        if date is None:\n            date = dt.date.today()\n\n        positions = self.get_table(\"hist_positions\").query(\"Fund == @fund_name and Asset_ID == @asset_key and Date &lt;= @date\")\n\n        if len(positions) &gt; 0:\n            return positions.tail(1).squeeze()[\"Avg_price\"]\n\n        return 0\n\n\n    def get_term_debt(self, fund_name):\n\n        term_debt = self.get_table(\"last_positions\")\n        term_debt = term_debt.loc [((term_debt[\"Asset\"].str.contains(\"term debt\"))&amp;(term_debt[\"Fund\"]==fund_name))][[\"#\", \"Avg_price\"]]\n        total_debt = -(term_debt[\"#\"] * term_debt[\"Avg_price\"]).sum()\n\n        return total_debt\n\n\n    def is_holiday(self, date,  location):\n        \"\"\"\n        Args:\n            date (dt.date): \n            location (str): \n                    'any'-&gt; se \u00e9 feriado em qualquer um dos lugares cadastrados\\n\n                    'all' -&gt; se \u00e9 feriado em todas as localidades da tabela\\n\n                    'us', 'bz', (...) -&gt; para local especifico, com codigo presente na tabela\\n\n        Returns:\n            bool: se \u00e9 ou nao feriado\n        \"\"\"\n        holidays = self.get_table(\"holidays\")\n\n        if location != \"any\" and location != \"all\":\n            holidays = holidays.loc[holidays[\"Location\"] == location]\n        if location == \"all\":\n            n_locations = len(holidays[\"Location\"].unique())\n            holidays = (holidays[\"Date\"]\n                                .value_counts().reset_index()\n                                .query(\"count == @n_locations\")\n                                )\n\n        return date in set(holidays[\"Date\"].dt.date)\n\n\n    def get_bday_offset(self, date, offset=1, location=\"bz\"):\n        \"\"\" Retorna dia util com offset dias pra frente ou pra tras.\\n\n                location (str): \n                    'any'-&gt; se \u00e9 feriado em qualquer um dos lugares cadastrados\\n\n                    'all' -&gt; se \u00e9 feriado em todas as localidades da tabela\\n\n                    'us', 'bz', (...) -&gt; para local especifico, com codigo presente na tabela\\n\n        \"\"\"\n        offset_direction = 0\n        i = 1\n        if offset != 0:\n            offset_direction = offset/abs(offset)\n            i = abs(offset)\n\n        while i &gt; 0 :\n            # Pegamos proxima data\n            date = date + dt.timedelta(days=offset_direction)\n            # Verificamos se eh dia util. Se for, contabilizamos.\n            if (date.weekday() &lt; 5) and (not self.is_holiday(date, location) ):\n                i -= 1\n            elif offset == 0:\n                offset_direction = -1\n\n        return date\n\n\n    def get_bdays_count(self, recent_date, previous_date, location=\"bz\"):\n        \"\"\"\n        Calcula quantos dias uteis existem entre recent_date e previous_date\n        (Exclusivo, ou seja, recent_date e previous_date nao sao contados)\n        \"\"\"\n        counter = 0\n        iter_date = recent_date - dt.timedelta(days=1) # primeiro dia nao eh contado\n\n        while iter_date &gt; previous_date:\n\n            if (iter_date.weekday() &lt; 5) and (not self.is_holiday(iter_date, location) ):\n                counter += 1\n            iter_date -= dt.timedelta(days=1)\n\n        return counter\n\n\n    def get_month_end(self, ref_date):\n\n        ref_date = dt.date(ref_date.year, ref_date.month, 20) + dt.timedelta(days=20)\n        return dt.date(ref_date.year, ref_date.month, 1) - dt.timedelta(days=1)\n\n\n    def get_start_period_dt(self, ref_date, period, force_bday=False, holiday_location=\"all\",\n                            force_month_end=False):\n        \"\"\" A partir da data informada e do periodo, retorna a data de inicio do periodo.\n\n        Args:\n            ref_date (dt.date): A data de referencia\n            period (str): O periodo em questao dado em meses, ou ytd ou mtd. Ex.: '12m', '6m', 'ytd', 'mtd', '24m'\n            force_bday (bool): Determina se a data retorna devera ser dia util\\n\n            holiday_location (str): \\n\n                                    \"bz\",\"us\" -&gt; considerar feriados num local especifico\\n\n                                    \"all\" -&gt; considerar feriados globais apenas\\n\n                                    \"any\" -&gt; considerar feriado em qualquer localidade (dentre bz e us)\n        \"\"\"\n        period = period.lower()\n        start_date = None\n        if period[-1] == \"m\": \n\n            n_months = int(period.split(\"m\")[0])\n\n            if force_month_end:\n                start_date = ref_date - pd.DateOffset(months=n_months)\n                start_date = start_date.date()\n                start_date = self.get_month_end(start_date)\n            else:\n\n                is_leap_date = ((ref_date.month == 2) and (ref_date.day == 29))\n\n                if is_leap_date:\n                    ref_date = ref_date-dt.timedelta(days=1)\n\n\n                year_offset = n_months//12 # obtendo o numero de anos inteiros no periodo informado\n                month_offset = n_months%12 # obtendo o numero de anos parciais \n\n                start_date = dt.date(ref_date.year-year_offset, ref_date.month, ref_date.day)\n                start_date = start_date - month_offset * dt.timedelta(days=30)\n\n                if is_leap_date:\n                    start_date = start_date + dt.timedelta(days=10) # forcando avanco ao mes seguinte\n                    # Retornando ao ultimo dia do mes anterior\n                    start_date = dt.date(start_date.year, start_date.month, 1)-dt.timedelta(days=1)\n\n\n        elif period == \"ytd\":\n            start_date = dt.date(ref_date.year-1, 12, 31)\n\n        elif period == \"mtd\":\n            start_date = dt.date(ref_date.year, ref_date.month , 1) - dt.timedelta(days=1)\n\n        # A partir de uma data pegar o trimestre anterior\n        elif period == 'qtr':\n            # Deve retornar sempre a ultima data do final do trimestre anterior\n            current_quarter = (ref_date.month - 1) // 3 + 1\n            start_of_current_quarter = dt.date(ref_date.year, (current_quarter - 1) * 3 + 1, 1)\n            start_date = start_of_current_quarter - dt.timedelta(days=1) # Last day of previous quarter\n\n            #start_date = ref_date - pd.DateOffset(months=3) \n            #start_date = start_date.date()\n            #start_date = self.get_month_end(start_date)\n\n        if force_bday:\n            start_date = self.get_bday_offset(start_date, offset=0, location=holiday_location)\n\n\n        return start_date\n\n\n    def __calculate_benchmark_spxt_cash(self, previous_date, recent_date, prop_spxt=0.8, prop_cash=0.2, \n                                                cash_ticker=\"jpmutcc lx equity\", spx_ticker=\"spxt index\", usdbrl_ticker=\"bmfxclco curncy\"):\n\n        df = self.get_benchmark_spx_cash(previous_date=previous_date-dt.timedelta(days=40), recent_date=recent_date,\n                                         spx_ticker=spx_ticker, prop_spx=prop_spxt, prop_cash=prop_cash, cash_ticker=cash_ticker,\n                                         usdbrl_ticker=usdbrl_ticker).reset_index()\n        #\"inx index\"\n        previous_value = df.query(\"Date &lt;= @previous_date\").tail(1)[\"Benchmark\"].squeeze()\n        recent_value = df.query(\"Date &lt;= @recent_date\").tail(1)[\"Benchmark\"].squeeze()\n\n        return (recent_value/previous_value) -1\n\n\n    def get_benchmark_spx_cash(self, previous_date:dt.date, recent_date:dt.date, prop_spx=0.8, prop_cash=0.2, currency=\"usd\",\n                               usdbrl_ticker=\"bmfxclco curncy\", cash_ticker=\"jpmutcc lx equity\", spx_ticker=\"spxt index\"):\n        \"\"\" \n        Obtem a serie historica do benchmark luxor a partir do indice s&amp;p e caixa nas proporcoes desejadas.\n        Args:\n            previous_date (datetime.date): Data inicial desejada\n            recent_date (datetime.date): Data final desejada\n            prop_spx (float, optional): Proporcao s&amp;p. Defaults to 0.8.\n            prop_cash (float, optional): Proporcao caixa. Defaults to 0.2.\n            currency (str, optional): Moeda desejada ('usd','brl'). Defaults to \"usd\".\n            usdbrl_ticker (str, optional): Ticker do usdbrl para conversao. Defaults to \"bmfxclco curncy\".\n            cash_ticker (str, optional): Ticker do ativo usado como caixa. Defaults to \"jpmutcc lx equity\".\n            spx_ticker (str, optional): Ticker do ativo usado como s&amp;p. Defaults to \"spxt index\".\n        Returns:\n            pandas.DataFrame: DataFrame contendo coluna Benchmark e Datas como index.\n        \"\"\"\n        complementary_previous_date=previous_date\n\n        if previous_date &lt; dt.date(2018,12,31) and cash_ticker == 'jpmutcc lx equity':\n            #logger.warning(f\"Nao ha datas anteriores a {dt.date(2018,12,31)} para o JPMUTCC. Sera usada essa.\")\n            previous_date = dt.date(2018,12,31)\n\n        if previous_date &lt; dt.date(2020,3,2) and cash_ticker == 'sofrindx index':\n            #logger.warning(f\"Nao ha datas anteriores a {dt.date(2018,12,31)} para o JPMUTCC. Sera usada essa.\")\n            previous_date = dt.date(2020,3,2)\n\n\n\n        data = self.get_prices(tickers=[spx_ticker, cash_ticker], previous_date=previous_date, currency=currency,recent_date=recent_date, \n                               usdbrl_ticker=usdbrl_ticker)\n\n        if recent_date == dt.date.today():\n            d = dt.datetime(recent_date.year, recent_date.month, recent_date.day)\n            # Precisamos ajustar o ultimo preco para o mais recente\n            last_prices = pd.DataFrame({\"Date\" : [d, d], \n                                        \"Asset\" : [\"spxt index\",cash_ticker], \n                                      \"Last_Price\" : [self.get_price(\"spxt index\"), self.get_price(cash_ticker)]\n                                    })\n            data = pd.concat([data.query(\"Date &lt; @d\").copy(), last_prices])\n\n        if complementary_previous_date != previous_date:\n            # Entrar aqui significa que o periodo precisou ser ajustado, pois eh anterior a existencia do indice usado para o caixa\n            # Nesse caso, vamos pegar todo o periodo que faltou e considerar retorno diario do FED FUND\n\n            # Primeiro, obtemos o periodo completo do s&amp;p\n            data_spx = self.get_prices(tickers=[spx_ticker], previous_date=complementary_previous_date,\n                                                 recent_date=recent_date, currency=currency, usdbrl_ticker=usdbrl_ticker)\n            # Em seguida, vamos pegar o parcial do jpmutcc e completar com o treasury, \n            # Para isso, sera necessario criar uma serie normalizada a partir dos retornos diarios\n            data_treasury = data.query(\"Asset == @cash_ticker\").copy().set_index([\"Date\",\"Asset\"]).pct_change().reset_index().dropna()\n            cash_initial_date = min(data_treasury[\"Date\"])\n            complementary_treasury = self.get_prices(tickers=[\"fedl01 index\"], previous_date=complementary_previous_date,\n                                                 recent_date=recent_date, currency=currency, usdbrl_ticker=usdbrl_ticker)\n            complementary_treasury[\"Last_Price\"] = (1 + complementary_treasury[\"Last_Price\"]/100) ** (1/360)-1\n            complementary_treasury = complementary_treasury.query(\"Date &lt; @cash_initial_date\").copy()\n            data_treasury = pd.concat([complementary_treasury, data_treasury])\n            data_treasury[\"Asset\"] = cash_ticker\n            data_treasury[\"Last_Price\"] = (data_treasury[\"Last_Price\"] + 1).cumprod()\n            data = pd.concat([data_spx, data_treasury])\n\n\n\n        # Gerando tabela de granularidade mensal, com as cotas mensais do indicador\n        df = data.pivot(columns=\"Asset\", index=\"Date\",values=\"Last_Price\").ffill().bfill().reset_index()\n        df = df.groupby(df['Date'].dt.to_period('M')).last().reset_index(drop=True).set_index(\"Date\")\n        df = df.pct_change().fillna(0).reset_index()\n        df[\"Benchmark\"] = (df[spx_ticker] * prop_spx + df[cash_ticker] * prop_cash) + 1\n        # Salvando a base mensal num novo df\n        df_benchmark = df[[\"Date\", \"Benchmark\"]].set_index(\"Date\").cumprod()\n\n        # Gerando base de granularidade diaria com o fator de retorno mtd com 80% spxt e 20% jpmutcc\n        df = data.set_index(\"Date\")\n        df[\"Last_Price\"] = df.groupby([\"Asset\"]).pct_change().fillna(0)+1\n        df = df.rename(columns={\"Last_Price\":\"Pct_Change\"})\n\n        df = df.pivot(columns=\"Asset\", values=\"Pct_Change\")\n\n        df[cash_ticker] = df[cash_ticker].fillna(1) #df[cash_ticker].ffill() # como tende a ser constante, podemos estimar os valores na\n        df[spx_ticker] = df[spx_ticker].fillna(1) # para o s&amp;p por ser variavel, preencheremos com 0 sempre\n\n        df[\"Year\"] = df.index.year\n        df[\"Month\"] = df.index.month\n        # Acumulando para encontrar o retorno MTD\n        df_mtd_w = df.groupby([\"Year\", \"Month\"]).cumprod()\n        # Ponderando para encontrar o benchmark MTD 80/20\n        df_mtd_w[\"MTD\"] = df_mtd_w[cash_ticker] * prop_cash + df_mtd_w[spx_ticker] * prop_spx\n\n        # Agora que temos o fator MTD granularidade diaria e a cota mensal\n        # Vamos colocar a cota mensal numa coluna da tabela de granulariade diaria\n        # Primeiro, precisamos de uma juncao que compreenda ano e mes\n        df_mtd_w[\"Year\"] = df_mtd_w.index.year\n        df_mtd_w[\"Month\"] = df_mtd_w.index.month\n        df_benchmark[\"Year\"] = df_benchmark.index.year\n        df_benchmark[\"Month\"] = df_benchmark.index.month\n        df_benchmark = df_benchmark.rename(columns={\"Benchmark\":\"Previous_Month_Acc\"})\n\n        # shiftando para que seja sempre a rentabilidade acumulada ate o mes anterior.\n        # Essa operacao tambem ira descartar qualquer acumulo feito numa data que nao for fim de mes.\n        df_benchmark[\"Previous_Month_Acc\"] = df_benchmark[\"Previous_Month_Acc\"].shift(1).ffill().bfill()\n\n        # Preenchemos entao uma coluna com a rentabilidade de cada mes anterior\n        df = df_mtd_w.reset_index().merge(df_benchmark, on=[\"Month\", \"Year\"], how=\"left\")\n\n        # Finalmente, o benchmark sera obtido atraves do acumulado anterior acumulado com o MTD atual\n        df[\"Benchmark\"] = df[\"Previous_Month_Acc\"] * df[\"MTD\"]\n\n        return df[[\"Date\", \"Benchmark\"]].set_index(\"Date\")\n\n    def get_quota_adjusted_by_amortization(self, fund_name, quota, date):\n\n        amortizations = {   # Amortizacao Bruta / PL antes da amortizacao\n                \"maratona\" : [{\"event_date\" : dt.date(2024,1,4), \"amortization_ratio\" : 2_517_404.21/24_165_260.40},\n                              {\"event_date\" : dt.date(2025,1,8), \"amortization_ratio\" : 950_000/27_633_373.46},\n                              ],\n            }\n        # Verificando se ha amortizacoes para o ativo e aplicando.\n        if fund_name in amortizations.keys():\n            amortization = amortizations[fund_name]\n            for amortization in amortization:\n                if date &lt; amortization[\"event_date\"]:\n                    quota =  quota * (1-amortization[\"amortization_ratio\"])\n        return quota\n\n\n    def get_pct_change(self, ticker, recent_date= dt.date.today() , previous_date=dt.date.today()-dt.timedelta(days=1),\n                        period=None, holiday_location=\"all\", adjust_amortization=True, force_month_end=False):\n\n        if period is not None:\n            previous_date = self.get_start_period_dt(previous_date, period=period, holiday_location=holiday_location,\n                                                     force_month_end=force_month_end)\n\n        if recent_date &lt; previous_date:\n            logger.warning(\"Possivel inversao dos parametros de inicio e fim do periodo.\")\n            temp = recent_date\n            recent_date = previous_date\n            previous_date = temp\n\n        if ticker == 'benchmark luxor spx cash' and recent_date == dt.date.today():\n            return self.__calculate_benchmark_spxt_cash(previous_date, recent_date, cash_ticker=\"sofrindx index\")\n        elif ticker == 'benchmark luxor sp500 85/15 cash' and recent_date == dt.date.today():\n            return self.__calculate_benchmark_spxt_cash(previous_date, recent_date, prop_spxt=0.85, prop_cash=0.15,\n                                cash_ticker='sofrindx index')\n\n\n        last_price = self.get_price(ticker, px_date=recent_date)\n\n        if last_price == 0 or last_price is None:\n            return 0\n\n        previous_price = self.get_price(ticker, px_date=previous_date)\n\n        try:\n            if (previous_price == 0) or (previous_date is None) or (previous_price is None):\n                return 0\n        except ValueError:\n            logger.error(f\"ValueError:\\nticker:{ticker} previous_price: {previous_price} previous_date:{previous_date}\")\n\n        if adjust_amortization:\n            last_price = self.get_quota_adjusted_by_amortization(ticker, last_price, recent_date)\n            previous_price = self.get_quota_adjusted_by_amortization(ticker, previous_price, previous_date)\n\n        return last_price/previous_price-1\n\n\n    def get_pct_changes(self, tickers=None, recent_date=dt.date.today(),\n            previous_date=dt.date.today()-dt.timedelta(days=1), period=None, currency=\"local\",\n            usdbrl_ticker=\"bmfxclco curncy\", adjust_amortization=False, force_month_end=False):\n        #TODO -&gt; garantir que a tabela de precos esta sendo atualizada pelos precos no intraday\n        if adjust_amortization:\n            logger.critical(\"Ajuste de amortizacao ainda NAO implementado para esse metodo.\")\n        # Aproveitando a get_prices, para realizar as filtragens\n        pct_changes = self.get_prices(tickers=tickers, recent_date=recent_date,\n                                      previous_date=previous_date, currency=currency,\n                                      period=period, usdbrl_ticker=usdbrl_ticker,\n                                       force_month_end=force_month_end, get_intraday_prices=True\n                                       ).set_index(\"Date\")\n\n        pct_changes[\"Pct_Change\"] = pct_changes.groupby(\"Asset\").pct_change().fillna(0)+1\n        pct_changes = pct_changes[[\"Asset\", \"Pct_Change\"]]\n        pct_changes[\"Pct_Change\"] = pct_changes.groupby(\"Asset\").cumprod()-1\n        pct_changes = pct_changes.groupby(\"Asset\").last()\n\n        return pct_changes\n\n\n    def __calculate_pct_table(self):\n\n\n\n        self.hist_pct_change = self.hist_prices_table.copy().set_index(\"Date\")\n        self.hist_pct_change = self.hist_pct_change.groupby(\"Asset\").pct_change().fillna(0)+1\n\n\n\n    def get_positions(self, fund_name, date=dt.date.today(), recent_date=dt.date.today(), previous_date=None, period=None,\n                      get_inner_positions=False, force_month_end=False):\n        \"\"\"\n            Fornece um dicionario com as posicoes do fundo na data informada.\n        \"\"\"\n        hist_pos = self.get_table(\"hist_positions\").query(\"Fund == @fund_name\")\n\n        if (previous_date is None) and (period is None):\n            # manter funcionamento antes da implementacao da funcionalidade de cosultar multiplas datas\n            # Separamos posicoes historicas apenas do fundo que nos interessa antes da data informada\n            hist_pos = hist_pos.loc[hist_pos[\"Date\"].dt.date &lt;= date]\n\n            visited = set()\n            positions = {}\n            rows = hist_pos.to_dict(\"records\") # Obtendo lista de rows(dicts) para iterar\n\n            rows.reverse() # vamos iterar do primeiro ao ultimo\n\n            for row in rows:\n                if row[\"Asset_ID\"] not in visited:\n                    visited.add(row[\"Asset_ID\"])\n                    if row[\"#\"] &gt; 0.000001 or row[\"#\"] &lt; -0.000001:\n                        positions[row[\"Asset_ID\"]] = row[\"#\"]\n\n            if not get_inner_positions:\n                return positions\n\n            # Vamos ver se tem fundos da luxor\n            # Vamos remover esse e explodir em posicoes internas\n            inner_funds = {\"lipizzaner_lipizzaner\" : \"lipizzaner\",\n                        \"fund a_fund a\" : \"fund a\"}\n            for inner_fund in inner_funds.keys():\n                if inner_fund in positions.keys():\n                    inner_positions = self.get_positions(inner_funds[inner_fund], date,\n                                                         get_inner_positions=True)\n                    positions.pop(inner_fund)\n                    positions.update(inner_positions)\n\n            return positions\n\n        # Obtendo data de inicio e validando datas\n        assert(recent_date is not None)\n        if period is not None:\n            previous_date = self.get_start_period_dt(recent_date, period=period, force_month_end=force_month_end)\n        assert(recent_date &gt; previous_date)\n\n        previous_or_before = hist_pos.query(\"Date &lt;= @previous_date\").groupby(\"Ticker\").last().reset_index()\n        previous_or_before[\"Date\"] = previous_date\n\n        after_previous = hist_pos.query(\"Date &gt; @previous_date and Date &lt;= @recent_date\")\n        positions = pd.concat([previous_or_before, after_previous])\n        positions[\"Date\"] = pd.to_datetime(positions[\"Date\"])\n\n        return positions\n\n\n    def get_bdr_adj_price(self, asset_id, date, usdbrl_ticker=\"bmfxclco curncy\"):\n\n        ticker = asset_id.split(\"_\")[-1]\n        price_key = self.get_price_key(asset_id)\n        bdr_size = self.get_bdr_size(ticker)\n        usdbrl = self.get_price(usdbrl_ticker, px_date=date)\n\n        return self.get_price(price_key, px_date=date) * usdbrl/bdr_size\n\n\n    def normalize_trades(self, trades, currency, asset_id_columns=\"Asset_ID\", usdbrl_ticker=\"bmfxclco curncy\"):\n        \"\"\" Para um dado dataframe de trades, converte para moeda indicada.\n            Trades devem conter as colunas Asset, Ticker e Delta_Shares.\n            BDR: Usa o peso do BDR e a quantidade de BDR operada para chegar na quantidade do ativo original.\n            Financeiro: Corrigido pelo cambio de fechamento.\n\n        Args:\n            trades (pd.DataFrame): dataframe de trades, mais especificamente o output\n                            de get_positions_and_movements.\n            currency (str): brl; usd; all -&gt; todas as moedas, no caso usd e brl\n        \"\"\"\n        assert(currency in [\"usd\", \"brl\"])\n        previous_date=trades[\"Date\"].min()\n        recent_date=trades[\"Date\"].max()\n\n        assets = self.get_table(\"assets\")\n\n        # tratando inconsistencia da Location do spx hawker\n        spx_hawker_brl_tickers = [\n            \"SPX SEG HAWKER JAN22\", \"SPX SEG HAWKER FEB22\", \"SPX HAWKER CL AMAR22\",\n            \"SPX SEG HAWKER APR22\", \"SPX HAWKER CL ASET18\", \"SPX SEG HAWKER JUN23\", \"SPX SEG HAWKER SEP23\"]\n        spx_hawker_brl_tickers = list(map(str.lower, spx_hawker_brl_tickers)) # colocando tudo para minusculo\n        assets[\"Location\"] = np.where(assets[\"Ticker\"].isin(spx_hawker_brl_tickers), \"bz\", assets[\"Location\"])\n\n\n        # Salvando os nomes das colunas antes de editar a tabela.\n        trades_columns = list(trades.columns)        \n\n\n        # Adicionando algumas colunas que vamos precisar para conseguir distinguir os ativos\n        trades = pd.merge(trades, assets[[\"Key\", \"Asset\", \"Ticker\", \"Type\", \"Location\", \"Ticker_BBG\"]], left_on=\"Asset_ID\", right_on=\"Key\")\n\n        # Achando quantidade do trade no ativo original -&gt; usando peso do bdr \n        bdr_sizes = self.get_table(\"bdr_sizes\", index=True, index_name=\"Ticker\").reset_index()\n        trades = pd.merge(trades, bdr_sizes, on=\"Ticker\", how=\"left\").rename(columns={\"adr_adr_per_sh\":\"BDR_Size\"})\n        # Achando quantidade do trade no ativo original -&gt; usando peso do bdr \n        # -&gt; Partindo da premissa que sempre atualizamos o historico quando ha alteracao do peso (inplit/split do bdr)\n        trades[\"Delta_Shares\"] = np.where(trades[\"Type\"] == 'a\u00e7\u00f5es_bdr', trades[\"Delta_Shares\"] / trades[\"BDR_Size\"], trades[\"Delta_Shares\"])\n        # Ajustando quantidades dos trades de localiza proporcionalmente:\n        #lcam_rent_ratio = 0.4388444  #0.44 rent3 para cada lcam3\n        #lzrfy_rent_ratio = 1 # 1 rent para cada lzrfy\n        #trades[\"Delta_Shares\"] = np.where(trades[\"Ticker\"] == 'lcam3 bz equity', trades[\"Delta_Shares\"] * lcam_rent_ratio, trades[\"Delta_Shares\"])\n        # Ajuste da lzrfy eh desnecessario ja que eh 1:1\n        #trades[\"Delta_Shares\"] = np.where(trades[\"Ticker\"] == 'lzrfy us equity', trades[\"Delta_Shares\"] * lzrfy_rent_ratio, trades[\"Delta_Shares\"])\n\n        #brkA_brkB_ratio = 1500\n        #trades[\"Delta_Shares\"] = np.where(trades[\"Ticker\"] == 'brk/a us equity', trades[\"Delta_Shares\"] * brkA_brkB_ratio, trades[\"Delta_Shares\"])\n\n        # Adicionando coluna de usdbrl de fechamento em cada dia\n        hist_usdbrl = self.get_prices(usdbrl_ticker, previous_date=previous_date, recent_date=recent_date).rename(columns={\"Last_Price\": \"USDBRL\"})\n\n        trades = pd.merge(trades, hist_usdbrl[[\"Date\", \"USDBRL\"]], on=\"Date\")\n\n        # Criando colunas com financeiro normalizado.\n        if currency == 'usd':\n            trades[\"Trade_Amount\"] = np.where(trades[\"Location\"] == 'bz', trades[\"Trade_Amount\"]/trades[\"USDBRL\"], trades[\"Trade_Amount\"])\n\n        elif currency == 'brl':\n            trades[\"Trade_Amount\"] = np.where(trades[\"Location\"] == 'us', trades[\"Trade_Amount\"]*trades[\"USDBRL\"], trades[\"Trade_Amount\"])\n\n        return trades[trades_columns]\n\n\n    def normalize_price_key(self, df):\n        \"\"\"\n            Adiciona a coluna 'Price_ID' no df informado representando uma chave normalizada para o preco do ativo desejado.\n        Args:\n            df (pandas.DataFrame): DataFrame que obrigatoriamente precisa conter 'Asset' e 'Ticker' como colunas. \n        \"\"\"\n\n        df_columns = list(df.columns)\n\n        assets = self.get_table(\"assets\")[[\"Key\", \"Asset\", \"Ticker\", \"Ticker_BBG\",]].rename(columns={\"Key\":\"Asset_ID\"})\n\n        assets[\"Price_Key\"] = assets[\"Asset_ID\"].apply(lambda x: self.get_price_key(x))\n        asset_price_key_map = {\"etf s&amp;p\" : 'voo us equity', \"spx hawker\": \"spx hawker usd\", \"berkshire\" : \"brk/b us equity\",\n                               \"localiza\" : \"rent3 bz equity\", \"suzano\":\"suzb3 bz equity\"}\n        assets[\"Price_Key\"] = assets.apply(lambda row: asset_price_key_map[row[\"Asset\"]] if row[\"Asset\"] in asset_price_key_map else row[\"Price_Key\"], axis=1)\n\n        df = df.rename(columns={\"Price_Key\": \"Price_Old_key\"})\n        df = pd.merge(df, assets[[\"Asset_ID\", \"Price_Key\"]], on=\"Asset_ID\")\n\n        # Casos padroes |-&gt; tratados na tabela asset, usando o get_price_key. Casos especificos adicionar no asset_price_key_map.\n        #df[\"Price_Key\"] = np.where((df[\"Ticker_BBG\"] is None) or (df[\"Ticker_BBG\"].isnull()), df[\"Ticker\"], df[\"Ticker_BBG\"])\n\n        if \"Price_Key\" not in df_columns: df_columns.append(\"Price_Key\")\n\n        return df[df_columns]\n\n\n    def calculate_average_price(self, df, asset_id=\"Asset_ID\"):\n        # Dictionary to store cumulative cost and total shares for each ticker\n        ticker_info = {}\n\n        # Function to update ticker information\n        def __update_ticker(ticker, delta_shares, trade_amount):\n            if ticker not in ticker_info:\n                ticker_info[ticker] = {'cumulative_cost': 0, 'total_shares': 0, 'avg_price': 0}\n\n            if delta_shares &gt; 0:  # Buy trade\n                ticker_info[ticker]['cumulative_cost'] += -trade_amount  # Trade amount is negative for buys\n\n            elif delta_shares &lt; 0:  # Sell trade\n                # Update cumulative cost using the last average price\n                ticker_info[ticker]['cumulative_cost'] -= ticker_info[ticker]['avg_price']* abs(delta_shares)\n\n            ticker_info[ticker]['total_shares'] += delta_shares\n\n            # Calculate average price\n            if abs(ticker_info[ticker]['total_shares']) &gt; 0.001:\n                ticker_info[ticker][\"avg_price\"] = ticker_info[ticker]['cumulative_cost'] / ticker_info[ticker]['total_shares']\n\n            else:\n                ticker_info[ticker][\"avg_price\"] = np.nan  # Avoid division by zero\n\n            return ticker_info[ticker][\"avg_price\"]\n\n        # Apply the function to each row and create a new column for the average price\n        df['Average_Price'] = df.apply(lambda row: __update_ticker(row[asset_id], row['Delta_Shares'], row['Trade_Amount']), axis=1)\n\n        return df\n\n\n\n    def calculate_adjusted_avg_price(self, df, groupby_column=\"Ticker\"):\n        \"\"\"Recebe um DataFrame contendo as colunas Date, Ticker, Delta_Shares e Trade_Amount.\n            Trade_Amount sera resultado do preco_medio anterior (ja calculado na transacao) \n            multiplicado pela quantidade operada. ATENCAO pois na venda nao tem a informacao\n            do valor pelo qual o ativo foi vendido.\n\n        Args:\n            df (pandas.DataFrame):\n\n        Returns:\n            pandas.DataFrame: Adiciona as colunas Cumulative_Shares, Open_Position_Cost e Average_Price\n        \"\"\"\n        # Sort the DataFrame by Ticker and Date for sequential processing\n        df_sorted = df.sort_values([groupby_column, \"Date\", \"Trade_Side\"]) # sort by trade_side (para processar sempre a compra primeiro)\n        df_columns = list(df.columns)\n\n        # Calculate cumulative shares and trade value for each ticker\n        df_sorted['Cumulative_Shares'] = df_sorted.groupby(groupby_column)['Delta_Shares'].cumsum()\n\n        df_sorted = self.calculate_average_price(df_sorted)\n        df_sorted[\"Average_Price\"] = df_sorted.groupby([\"Date\", \"Ticker\", \"Trade_Side\"])[\"Average_Price\"].ffill()#fillna(method=\"ffill\")\n\n        df_sorted[\"Open_Position_Cost\"] = -abs(df_sorted[\"Cumulative_Shares\"] * df_sorted[\"Average_Price\"])\n\n        return df_sorted[df_columns+[\"Cumulative_Shares\", \"Open_Position_Cost\", \"Average_Price\"]]\n\n\n    def __get_positions_and_movements(self, fund_name, tickers=None,\n            recent_date=dt.date.today(), previous_date=dt.date.today()-dt.timedelta(days=1),\n            period=None, holiday_location=\"all\", currency=\"usd\", usdbrl_ticker=\"bmfxclco curncy\"):\n        \"\"\"\n        Args:\n            fund_name (str): \n            tickers (str|list|set, optional): Defaults to None.\n            recent_date (datetime.date, optional): Defaults to dt.date.today().\n            previous_date (datetime.date, optional): _description_. Defaults to dt.date.today()-dt.timedelta(days=1).\n            period (str, optional): mtd|ytd|3m|6m|...|nm. Defaults to None.\n            holiday_location (str, optional): any|bz|us|all. Defaults to \"all\".\n\n        Returns:\n            pandas.DataFrame: \n        \"\"\"\n\n        assert(currency in [\"usd\", \"brl\"])\n\n        if period is not None:\n            previous_date = self.get_start_period_dt(recent_date, period, holiday_location=holiday_location, force_bday=True)\n\n        # Garantindo que a data inicial sera sempre dia util, assim eh certo de que havera preco de acao nessa data\n        previous_date = self.get_bday_offset(previous_date, offset=0, location=\"any\")\n        lipi_manga_incorp_date = self.lipi_manga_incorp_date\n        assets = self.get_table(\"assets\")\n\n\n        # Filtrando somente os trades desejados\n        # lembrando que lipi precisa abrir em fund_A e manga_master (quando antes da incorporacao)\n        # ha ainda um ajuste no lipi, que na data da incorporacao compra as posicoes do Manga (nao podemos tirar essas boletas) \n        trades = (self.get_table(\"trades\")[[\"Date\", \"Fund\", \"Asset\", \"Ticker\", \"#\", \"Price\", \"Total\", \"Op Type\"]]\n                .rename(columns={\"#\":\"Delta_Shares\", \"Total\" : \"Trade_Amount\", \"Op Type\":\"Op_Type\"})\n                .query(\"(Date &gt; @previous_date and Date &lt;= @recent_date) and Op_Type.isin(['a vista', 'ndf', 'termo', 'apl/resg fundos', 'vc capital call', 'vc'])\"))\n\n        fund_filter = [fund_name]\n        if fund_name == 'lipizzaner':\n            fund_filter += [\"fund a\"]\n            if previous_date &lt; lipi_manga_incorp_date:\n                fund_filter += [\"mangalarga master\"]\n        trades = trades.query(\"Fund in @fund_filter\")\n\n        trades = trades.query(\"Fund != 'lipizzaner' or Date != @lipi_manga_incorp_date\").rename(columns={\"Price\" : \"Exec_Price\"}).copy()\n        trades[\"Asset_ID\"] = trades[\"Asset\"] + \"_\" + trades[\"Ticker\"]\n\n\n        # Obtendo posicoes na data de inicio, e criando boletas de inicializacao\n        initial_positions = []\n\n        for f in fund_filter:\n            fund_initial_pos = pd.DataFrame(self.get_positions(f, date=previous_date).items(), columns=[\"Asset_ID\", \"Delta_Shares\"])\n            if len(fund_initial_pos) == 0: continue\n            fund_initial_pos[\"Fund\"] = f\n            fund_initial_pos[[\"Asset\", \"Ticker\"]] = tuple(fund_initial_pos[\"Asset_ID\"].str.split(\"_\"))\n\n            initial_positions.append(fund_initial_pos)\n\n\n        if len(initial_positions) &gt; 0:\n            initial_positions = pd.concat(initial_positions)\n\n            # Adicionando coluna de tipo, pois sera necessario tratar quando type == a\u00e7\u00f5es_bdr\n            initial_positions = pd.merge(initial_positions, assets[[\"Key\", \"Type\"]], left_on=\"Asset_ID\", right_on=\"Key\")\n\n            initial_positions[\"Price_Key\"] = initial_positions[\"Asset_ID\"].apply(lambda x: self.get_price_key(x))\n            initial_prices = (self.get_prices(tickers=list(initial_positions[\"Price_Key\"]), previous_date=previous_date,\n                                              recent_date=previous_date, force_continuous_date_range=True)\n                                        .rename(columns={\"Asset\": \"Price_Key\", \"Last_Price\":\"Exec_Price\"})[[\"Price_Key\", \"Exec_Price\"]])\n\n            initial_positions = pd.merge(initial_positions, initial_prices, on=[\"Price_Key\"])\n\n            # Corrigindo pre\u00e7o de entrada das BDRs\n            initial_positions[\"Exec_Price\"] = initial_positions.apply(lambda row: row[\"Exec_Price\"] if row[\"Type\"] != 'a\u00e7\u00f5es_bdr' else self.get_bdr_adj_price(row[\"Asset_ID\"], date=previous_date), axis=1)\n\n            initial_positions[\"Trade_Amount\"] = initial_positions[\"Delta_Shares\"] * initial_positions[\"Exec_Price\"] * (-1)\n            initial_positions[\"Date\"] = previous_date\n\n        else:\n            initial_positions = pd.DataFrame() # tratando caso inicial, onde nao ha posicoes no inicio\n\n        trades = pd.concat([initial_positions, trades])[[\"Date\", \"Asset_ID\", \"Delta_Shares\", \"Exec_Price\", \"Trade_Amount\"]]\n\n        trades[\"Date\"] = pd.to_datetime(trades[\"Date\"])\n\n        # Adicionando coluna pra marcar se eh compra ou venda. Sera usado na agregacao logo mais.\n        #trades[\"Trade_Side\"] = np.where(trades[\"Delta_Shares\"] &lt; 0, \"sell\", \"buy\")\n\n        # Ordenando boletas, por padrao vamos sempre comprar antes de vender.\n        trades = self.normalize_trades(trades, currency=currency, usdbrl_ticker=usdbrl_ticker).sort_values(by=[\"Date\"], kind=\"stable\")\n        # Apos normalizar os trades, eh necessario recalcular o preco da boleta\n\n        # Agrupando boletas de forma a ter apenas uma boleta por dia por codigo de ativo\n        #trades = trades.groupby([\"Date\", \"Asset_ID\"]).agg({\"Delta_Shares\":\"sum\", \"Exec_Price\":\"last\", \"Trade_Amount\":\"sum\"}).reset_index()\n        # Precisamos calcular novamente o preco correto de execucao do trade\n        #trades[\"Exec_Price\"] = abs(trades[\"Trade_Amount\"])/abs(trades[\"Delta_Shares\"])\n\n        # Calculando posicao ao final de cada trade, preco medio e custo da posicao\n        trades[\"Open_Quantity\"] = trades.groupby([\"Asset_ID\"])[\"Delta_Shares\"].cumsum()\n        trades[\"Open_Quantity\"] = np.where(abs(trades[\"Open_Quantity\"]) &lt; 0.00001, 0, trades[\"Open_Quantity\"])\n        trades = self.calculate_average_price(trades)\n        trades[\"Average_Price\"] = trades.groupby([\"Asset_ID\"])[\"Average_Price\"].ffill()#fillna(method=\"ffill\")\n\n        # Vamos agregar novamente, agora para tirar o 'Trade_Side', passando a ter uma linha apenas por data\n        #trades = trades.groupby([\"Date\", \"Asset_ID\"]).agg({\"Delta_Shares\":\"sum\", \"Exec_Price\":\"last\", \"Trade_Amount\":\"sum\",\n        #                                                    \"Open_Quantity\":\"last\", \"Average_Price\" : \"last\", \"Trade_Side\":\"last\"}).reset_index()\n        # Preco de execucao, necessita ser recalculado\n        #trades[\"Exec_Price\"] = abs(trades[\"Trade_Amount\"])/abs(trades[\"Delta_Shares\"])\n\n        trades[\"Open_Position_Cost\"] = (trades[\"Open_Quantity\"] * trades[\"Average_Price\"])\n\n        return trades\n\n\n    def get_positions_and_movements(self, fund_name, tickers=None,\n            recent_date=dt.date.today(), previous_date=dt.date.today()-dt.timedelta(days=1),\n            period=None, holiday_location=\"all\", currency=\"usd\", usdbrl_ticker=\"bmfxclco curncy\"):\n\n        \"\"\"\n        Args:\n            fund_name (str): \n            tickers (str|list|set, optional): Defaults to None.\n            recent_date (datetime.date, optional): Defaults to dt.date.today().\n            previous_date (datetime.date, optional): _description_. Defaults to dt.date.today()-dt.timedelta(days=1).\n            period (str, optional): mtd|ytd|3m|6m|...|nm. Defaults to None.\n            holiday_location (str, optional): any|bz|us|all. Defaults to \"all\".\n\n        Returns:\n            pandas.DataFrame: \n        \"\"\"\n\n        df = self.__run_return_analysis(fund_name, tickers=tickers,\n                                    recent_date=recent_date, previous_date=previous_date,\n                                    period=period, holiday_location=holiday_location,\n                                    currency=currency, agregate_by_name=False, usdbrl_ticker=usdbrl_ticker\n                                    )\n        # Vamos obter os precos de fechamento em cada dia\n        # Preenchendo primeiro todo o historico\n        prices_previous_date = (df[\"Date\"].min() - dt.timedelta(days=100)).date()\n        price_keys = list(df[\"Price_Key\"].unique())\n        prices = self.get_prices(\n                    tickers=price_keys, previous_date=prices_previous_date, currency=currency,\n                    get_intraday_prices=True, usdbrl_ticker=usdbrl_ticker\n                    ).rename(columns={\"Asset\":\"Price_Key\"})\n\n        df = df.merge(prices, on=[\"Date\", \"Price_Key\"], how=\"left\")\n\n        # Finalmente, vamos atualizar com o preco mais recente do ativo na data de hoje\n        # -&gt; Pode ser substituido pelo uso da flag get_intraday_prices na get_prices\n        # TODO : remover essa parte e testar\n        #df_prev = df.query(\"Date &lt; @dt.date.today()\").copy()\n        #df_today = df.query(\"Date == @dt.date.today()\").copy()\n        #assets = self.get_table(\"assets\").rename(columns={\"Key\":\"Asset_ID\"})\n        #df_today = df_today.merge(assets[[\"Asset_ID\", \"Location\"]], on=\"Asset_ID\")\n        #if len(df_today) &gt; 0:\n        #    df_today[\"Last_Price\"] = df_today.apply(lambda row: self.get_price(\n        #                                        row[\"Price_Key\"], currency=currency,\n        #                                        usdbrl_ticker=\"usdbrl curncy\",\n        #                                        asset_location=row[\"Location\"]), axis=1\n        #                                        )\n        #    df = pd.concat([df_prev, df_today])\n        #else:\n        #    df = df_prev\n\n        df[\"Current_Position_Value\"] = df[\"Open_Quantity\"] * df[\"Last_Price\"]\n\n        df = df[[\"Date\", \"Asset_ID\", \"Price_Key\", \"Open_Quantity\",\n                \"Delta_Shares\", \"Shares_Bought\", \"Shares_Bought_Cost\", \"Shares_Sold\",\n                \"Amount_Sold\", \"Last_Price\", \"Current_Position_Value\"]]\n\n        # preenchendo Last_Price nulos com o do dia anterior para os mesmos asset_id\n        df[\"Last_Price\"] = df.groupby(\"Asset_ID\")[\"Last_Price\"].ffill()\n\n        return df\n\n\n    def run_return_analysis(self, fund_name, tickers=None, recent_date=dt.date.today(), previous_date=dt.date.today()-dt.timedelta(days=1),\n                            period=None, holiday_location=\"all\", usdbrl_ticker=\"bmfxclco curncy\"):\n\n        currency = \"usd\"\n        positions_and_movements_usd = self.__run_return_analysis(fund_name, tickers=tickers, recent_date=recent_date,\n                                                                        previous_date=previous_date, period=period,\n                                                                        holiday_location=holiday_location, currency=currency,\n                                                                        usdbrl_ticker=usdbrl_ticker)\n        currency = \"brl\"\n\n        positions_and_movements_brl = self.__run_return_analysis(fund_name, tickers=tickers, recent_date=recent_date,\n                                                                        previous_date=previous_date, period=period,\n                                                                        holiday_location=holiday_location, currency=currency,\n                                                                        usdbrl_ticker=usdbrl_ticker)\n        if (positions_and_movements_usd is None) and (positions_and_movements_brl is None): return None\n\n        return pd.concat([positions_and_movements_usd, positions_and_movements_brl])\n\n\n    def __expand_hist_positions(self, hist_positions, recent_date, previous_date, holiday_location=\"all\"):\n\n        #for asset_id in positions_and_movements[\"Asset_ID\"].unique():\n        #    first_trade_date = positions_and_movements[positions_and_movements['Asset_ID'] == asset_id].index.min()\n        #    date_range = pd.date_range(start=first_trade_date, end=recent_date, freq='D')\n        #    ticker_transactions = positions_and_movements[positions_and_movements['Asset_ID'] == asset_id]\n\n        #    temp_df = pd.DataFrame({'Date': date_range, 'Asset_ID': asset_id})\n        #    temp_df = temp_df.merge(ticker_transactions, on=['Date', 'Asset_ID'], how='left')\n        #    columns_to_ffill = [\"Asset_ID\", 'Open_Quantity',\"Open_Position_Cost\", \"Average_Price\"]\n        #    temp_df[columns_to_ffill] = temp_df[columns_to_ffill].ffill()\n        #    columns_to_fill_zeros = [\"Delta_Shares\",\"Trade_Amount\", \"Exec_Price\", \"Shares_Bought\",\n        #                            \"Shares_Sold\", \"Shares_Bought_Cost\", \"Amount_Sold\", \"Cost_Of_Stocks_Sold\", \"Result_Of_Stocks_Sold\"]\n        #    \n        #    temp_df[columns_to_fill_zeros] = temp_df[columns_to_fill_zeros].fillna(0)\n        #    \n        #    daily_positions = pd.concat([daily_positions, temp_df])\n        pass\n\n\n    def __run_return_analysis(self, fund_name, tickers=None, recent_date=dt.date.today(),\n            previous_date=dt.date.today()-dt.timedelta(days=1), period=None, holiday_location=\"all\",\n            currency=\"usd\", agregate_by_name=True, usdbrl_ticker=\"bmfxclco curncy\"):\n\n        assert(currency in [\"usd\", \"brl\"])\n\n        positions_and_movements = self.__get_positions_and_movements(fund_name, tickers, recent_date, previous_date, period, holiday_location,\n                                                                     currency=currency, usdbrl_ticker=usdbrl_ticker)\n\n        # -&gt; Shares_Bought : Delta_Shares &gt; 0\n        positions_and_movements[\"Shares_Bought\"] = np.where(positions_and_movements[\"Delta_Shares\"] &gt; 0, positions_and_movements[\"Delta_Shares\"], 0)\n        # -&gt; Shares_Sold : Delta_Shares &lt; 0\n        positions_and_movements[\"Shares_Sold\"] = np.where(positions_and_movements[\"Delta_Shares\"] &lt; 0, positions_and_movements[\"Delta_Shares\"], 0)\n        # -&gt; Shares_Bought_Cost: eh o trade amount\n        positions_and_movements[\"Shares_Bought_Cost\"] = np.where(positions_and_movements[\"Delta_Shares\"] &gt; 0, abs(positions_and_movements[\"Trade_Amount\"]), 0)\n        # -&gt; Amount_Sold : Shares_Sold * Exec_Price\n        positions_and_movements[\"Amount_Sold\"] = np.where(positions_and_movements[\"Delta_Shares\"] &lt; 0, abs(positions_and_movements[\"Trade_Amount\"]), 0)\n        # -&gt; Cost_Of_Stocks_Sold : Shares_Sold * avg_price # vai dar ruim na zeragem, posso propagar o avg_price ao inves do cumulative_cost? Feito.\n        positions_and_movements[\"Cost_Of_Stocks_Sold\"] =abs( positions_and_movements[\"Shares_Sold\"] * positions_and_movements[\"Average_Price\"])\n        # -&gt; Result_Of_Stocks_Sold: Total_Sold - Cost_Of_Stocks_Sold\n        positions_and_movements[\"Result_Of_Stocks_Sold\"] = abs(positions_and_movements[\"Amount_Sold\"]) - positions_and_movements[\"Cost_Of_Stocks_Sold\"]\n\n        #positions_and_movements.to_excel(f\"testando_{currency}.xlsx\")\n        # Transformar granularidade pra diario somente aqui...\n        agg_rule = {\n            'Open_Quantity':\"last\", \"Average_Price\":\"last\",\n            \"Delta_Shares\":\"sum\",\"Trade_Amount\":\"sum\", \"Exec_Price\":\"last\", \"Open_Position_Cost\":\"last\", \"Shares_Bought\":\"sum\",\n            \"Shares_Sold\":\"sum\", \"Shares_Bought_Cost\":\"sum\", \"Amount_Sold\":\"sum\", \"Cost_Of_Stocks_Sold\":\"sum\", \"Result_Of_Stocks_Sold\":\"sum\"\n        }\n        positions_and_movements = positions_and_movements.groupby([\"Date\", \"Asset_ID\"]).agg(agg_rule).reset_index()\n\n\n        # diminuindo a granularidade de 1 linha por trade pra 1 linha por dia\n        positions_and_movements = positions_and_movements.set_index(\"Date\")\n\n        daily_positions = pd.DataFrame()\n\n        for asset_id in positions_and_movements[\"Asset_ID\"].unique():\n            first_trade_date = positions_and_movements[positions_and_movements['Asset_ID'] == asset_id].index.min()\n            date_range = pd.date_range(start=first_trade_date, end=recent_date, freq='D')\n            ticker_transactions = positions_and_movements[positions_and_movements['Asset_ID'] == asset_id]\n\n            temp_df = pd.DataFrame({'Date': date_range, 'Asset_ID': asset_id})\n            temp_df = temp_df.merge(ticker_transactions, on=['Date', 'Asset_ID'], how='left')\n            columns_to_ffill = [\"Asset_ID\", 'Open_Quantity',\"Open_Position_Cost\", \"Average_Price\"]\n\n            temp_df[columns_to_ffill] = temp_df[columns_to_ffill].infer_objects(copy=False).ffill()\n            columns_to_fill_zeros = [\"Delta_Shares\",\"Trade_Amount\", \"Exec_Price\", \"Shares_Bought\",\n                                    \"Shares_Sold\", \"Shares_Bought_Cost\", \"Amount_Sold\", \"Cost_Of_Stocks_Sold\", \"Result_Of_Stocks_Sold\"]\n\n            temp_df[columns_to_fill_zeros] = temp_df[columns_to_fill_zeros].astype(float).fillna(0)\n\n            daily_positions = pd.concat([daily_positions, temp_df])\n\n        positions_and_movements = daily_positions.copy()\n\n        # -&gt; Criar coluna price_key\n        assets = self.get_table(\"assets\")\n        assets[\"Price_Key\"] = assets[\"Key\"].apply(lambda x: self.get_price_key(x))\n\n        # -&gt; Fazer merge com Asset_ID, colocando colunas Name, Class e Price_Key\n        positions_and_movements = positions_and_movements.merge(assets[[\"Key\", \"Name\", \"Class\", \"Price_Key\"]], left_on=\"Asset_ID\", right_on=\"Key\")\n\n        if not agregate_by_name:\n            return positions_and_movements\n\n        # -&gt; pegar precos e colocar no diario\n        previous_date = positions_and_movements[\"Date\"].min().date()\n        prices = self.get_prices(tickers = list(positions_and_movements[\"Price_Key\"].unique()), previous_date=previous_date-dt.timedelta(days=100), currency=currency,\n                                                    usdbrl_ticker=usdbrl_ticker).rename(columns={\"Asset\":\"Price_Key\"})\n\n        positions_and_movements = positions_and_movements.merge(prices, on=[\"Date\", \"Price_Key\"])\n\n        # -&gt; Criar coluna Market_Value\n        positions_and_movements[\"Current_Position_Value\"] = positions_and_movements[\"Open_Quantity\"] * positions_and_movements[\"Last_Price\"]\n\n        # -&gt; Open_Result_Amount: Current_Position_Value - avg_price * Open_Quantity Se nao fizer assim vai dar problema no dia da zeragem\n        positions_and_movements[\"Open_Result_Amount\"] = positions_and_movements[\"Current_Position_Value\"] - positions_and_movements[\"Open_Position_Cost\"]\n\n        # -&gt; valido salvar uma versao nessa etapa para conferir\n        #positions_and_movements.to_excel(\"testing.xlsx\")\n        # -&gt; Apos essa agregacao, nao se pode mais usar o preco medio &lt;-\n        # -&gt; Agrupar por Name\n        positions_and_movements = positions_and_movements[[\n            'Date', 'Asset_ID', 'Trade_Amount', # 'Delta_Shares', 'Exec_Price'\n            'Open_Quantity', 'Open_Position_Cost', 'Name', #'Average_Price', 'Key'\n            'Class', 'Price_Key', 'Current_Position_Value', # 'Last_Price'\n            'Shares_Bought', 'Shares_Sold', 'Shares_Bought_Cost',\n            'Amount_Sold', 'Cost_Of_Stocks_Sold', 'Result_Of_Stocks_Sold',\n            'Open_Result_Amount']]\n\n        agg_rule = { 'Asset_ID' : \"last\", 'Trade_Amount' : \"sum\", 'Open_Quantity' : \"sum\", 'Open_Position_Cost' : \"sum\",\n            'Class' : \"last\", 'Price_Key' : \"last\", 'Current_Position_Value' : 'sum',\n            'Shares_Bought' : \"sum\", 'Shares_Sold' : \"sum\", 'Shares_Bought_Cost' : \"sum\",\n            'Amount_Sold' : \"sum\", 'Cost_Of_Stocks_Sold' : \"sum\", 'Result_Of_Stocks_Sold' : \"sum\",\n            'Open_Result_Amount' : \"sum\"}\n\n        positions_and_movements = positions_and_movements.groupby([\"Date\", \"Name\"]).agg(agg_rule).reset_index()\n\n        # -&gt; Acc_Result_Of_Stocks_Sold : Acc_Result_Of_Stocks_Sold.cumsum()\n        positions_and_movements[\"Acc_Result_Of_Stocks_Sold\"] = positions_and_movements.groupby(\"Name\")[\"Result_Of_Stocks_Sold\"].cumsum()\n        # -&gt; Tota_Result_Amount: Acc_Result_Of_Stocks_Sold + Open_Result_Amount\n        positions_and_movements[\"Total_Result_Amount\"] = positions_and_movements[\"Acc_Result_Of_Stocks_Sold\"]  + positions_and_movements[\"Open_Result_Amount\"]\n        # -&gt; Acc_Cost_Of_Stocks_Sold : Cost_Of_Stocks_Sold.cumsum()\n        positions_and_movements[\"Acc_Cost_Of_Stocks_Sold\"] = positions_and_movements.groupby(\"Name\")[\"Cost_Of_Stocks_Sold\"].cumsum()\n        # -&gt; Acc_Buy_Cost: Total_Bough.cumsum()\n        positions_and_movements[\"Acc_Shares_Bought_Cost\"] = positions_and_movements.groupby(\"Name\")[\"Shares_Bought_Cost\"].cumsum()\n\n        # -&gt; %_Closed_Result: Acc_Result_Of_Stocks_Sold/Acc_Cost_Of_Stocks_Sold\n        positions_and_movements[\"%_Closed_Result\"] = positions_and_movements[\"Acc_Result_Of_Stocks_Sold\"]/abs(positions_and_movements[\"Acc_Cost_Of_Stocks_Sold\"])\n        # -&gt; %_Open_Result: Current_Position_Value/Open_Position_Cost\n        positions_and_movements[\"%_Open_Result\"] = (positions_and_movements[\"Current_Position_Value\"]/positions_and_movements[\"Open_Position_Cost\"])-1\n        # -&gt; %_Total_Result: Total_Result_Amount/Acc_Buy_Cost\n\n        positions_and_movements[\"Avg_Cost\"] = (positions_and_movements[[\n                                                \"Name\",\"Open_Position_Cost\"]].groupby(\"Name\")\n                                                [\"Open_Position_Cost\"].cumsum())\n        positions_and_movements[\"Avg_Cost\"] = (positions_and_movements[\"Avg_Cost\"])/(positions_and_movements\n                                               .groupby(\"Name\").cumcount() + 1)\n\n        #positions_and_movements[\"%_Total_Result\"] = positions_and_movements[\"Total_Result_Amount\"]/positions_and_movements[\"Acc_Shares_Bought_Cost\"]\n        positions_and_movements[\"%_Total_Result\"] = positions_and_movements[\"Total_Result_Amount\"]/positions_and_movements[\"Avg_Cost\"]\n        # -&gt; redefinir price_key e price -&gt; Precisa tratar nessa funcao o caso a caso do ticker de preco que sera mostrado\n        positions_and_movements = self.normalize_price_key(positions_and_movements)\n        # A normalizacao pode acabar inserindo algum ticker generico que ainda nao estava calculado. Logo, temos que pegar os precos novamente.\n        prices = self.get_prices(tickers = list(positions_and_movements[\"Price_Key\"].unique()),\n                                        previous_date=previous_date-dt.timedelta(days=100), currency=currency,\n                                        usdbrl_ticker=usdbrl_ticker).rename(columns={\"Asset\":\"Price_Key\"})\n\n        positions_and_movements = positions_and_movements.merge(prices[[\"Date\", \"Price_Key\", \"Last_Price\"]], on=[\"Date\",\"Price_Key\"])\n\n        positions_and_movements[\"Period\"] = period\n        positions_and_movements[\"Currency\"] = currency\n        # Consideramos como ativos atuais aqueles que possuem posicao aberta ou que foram encerradas no dia\n        positions_and_movements[\"Is_Curr_Owned\"] = abs(positions_and_movements[\"Open_Position_Cost\"]) &gt; 0.1\n        positions_and_movements[\"Is_Curr_Owned_Prev\"] = positions_and_movements.groupby(\"Name\")[\"Is_Curr_Owned\"].shift(1,fill_value=False)\n        positions_and_movements[\"Is_Curr_Owned\"] = positions_and_movements[\"Is_Curr_Owned\"] | positions_and_movements[\"Is_Curr_Owned_Prev\"]\n        positions_and_movements = positions_and_movements.drop(columns=[\"Is_Curr_Owned_Prev\"])\n\n        positions_and_movements[\"Start_Date\"] = positions_and_movements[\"Name\"].map(positions_and_movements.groupby(\"Name\")[\"Date\"].min())\n        positions_and_movements[\"End_Date\"] = recent_date\n        positions_and_movements[\"End_Date\"] = pd.to_datetime(positions_and_movements[\"End_Date\"])\n        last_trades = positions_and_movements.query(\"Trade_Amount != 0\").groupby(\"Name\").tail(1).copy().set_index(\"Asset_ID\")\n        last_trades[\"End_Date\"] = np.where(abs(last_trades[\"Open_Quantity\"]) &lt;0.00001, last_trades[\"Date\"], last_trades[\"End_Date\"])\n        positions_and_movements[\"End_Date\"] = positions_and_movements[\"Asset_ID\"].map(last_trades[\"End_Date\"])\n\n        # Removendo erros de divisao por 0\n        positions_and_movements[\"%_Closed_Result\"] = np.where(abs(positions_and_movements[\"%_Closed_Result\"]) == np.inf, 0, positions_and_movements[\"%_Closed_Result\"])\n        positions_and_movements[\"%_Open_Result\"] = np.where(abs(positions_and_movements[\"%_Open_Result\"]) == np.inf, 0, positions_and_movements[\"%_Open_Result\"])\n        positions_and_movements[\"%_Total_Result\"] = np.where(abs(positions_and_movements[\"%_Total_Result\"]) == np.inf, 0, positions_and_movements[\"%_Total_Result\"])\n        # Refinando, para garantir dados consistentes\n        positions_and_movements[\"%_Open_Result\"] = np.where(abs(positions_and_movements[\"Open_Quantity\"]) &lt; 0.00001, 0, positions_and_movements[\"%_Open_Result\"])\n\n        positions_and_movements[\"Security_Acc_Return\"] = positions_and_movements[[\"Name\",\"Last_Price\"]].groupby([\"Name\"]).pct_change().fillna(0) + 1\n        positions_and_movements[\"Security_Acc_Return\"] = positions_and_movements[[\"Name\",\"Security_Acc_Return\"]].groupby([\"Name\"]).cumprod()-1\n\n\n        positions_and_movements[\"Days_Since_Start\"] = (positions_and_movements[\"Date\"] - positions_and_movements[\"Start_Date\"]).dt.days\n\n        spx_prices = self.get_prices(\"inx index\", recent_date=recent_date, previous_date=previous_date, currency=currency,\n                                     usdbrl_ticker=usdbrl_ticker)[[\"Date\", \"Last_Price\"]].rename(columns={\"Last_Price\" : \"spx_index\"})\n        positions_and_movements = positions_and_movements.merge(spx_prices, on=\"Date\", how=\"left\")\n\n        #positions_and_movements[\"spx_index\"] = (positions_and_movements[\"spx_index\"].pct_change().fillna(0)+1).cumprod()-1\n\n        return positions_and_movements.rename(columns={\"Price_Key\":\"Ticker\"})\n\n\n    def get_current_fx_data(self, fund, fx_ticker):\n        \"\"\"Calcula o PNL de FX de um fundo. -&gt; CONSIDERANDO QUE EH USDBRL\"\"\"\n        fx_positions = self.get_table(\"last_positions\").query(\"Fund == @fund and Ticker == @fx_ticker\").copy()\n        fx_positions[\"Last_Price\"] = fx_positions[\"Ticker\"].apply(lambda x: self.get_price(x))\n        fx_positions[\"Pnl_USD\"] = fx_positions[\"#\"] * ((fx_positions[\"Last_Price\"]/fx_positions[\"Avg_price\"]) - 1)\n        fx_positions[\"Pnl_BRL\"] = fx_positions[\"#\"] * (fx_positions[\"Last_Price\"] - fx_positions[\"Avg_price\"])\n        fx_positions[\"Exposure\"] = fx_positions[\"#\"] * fx_positions[\"Last_Price\"]\n        fx_positions = fx_positions[[\"Pnl_USD\", \"Pnl_BRL\", \"Exposure\"]].squeeze().to_dict()\n\n        return fx_positions\n\n\n    def get_hist_cash_movements(self, fund, ref_date, bdays=10, \n            holiday_location=\"all\", currency=\"usd\", usdbrl_ticker = \"bmfxclco curncy\"):\n        \"\"\"Retorna o historico de caixa do fundo.\n        Args:\n            fund (str): nome do fundo\n            currency (str): moeda (usd|brl)\n\n        Returns:\n            pandas.DataFrame: historico de caixa\n        \"\"\"\n\n        ## TODO: Remover linhas com pnl e retorno vazios!\n\n\n        fund = fund.replace(' ', '_').lower()\n        previous_date = self.get_bday_offset(ref_date, -bdays,\n                                        location=holiday_location)\n\n        cash = self.get_table(\"hist_portfolios_concentration\").query(\"Group.isin(['caixa','caixa_us']) and Fund_Name == @fund\").copy() \n        cash[\"Date\"] = cash[\"Date\"].dt.date\n        cash = cash.query(\"Date &gt;= @previous_date\").copy()\n        # cash estara com ambos os caixas em R$ ou U$.\n        # Para saber a moeda do caixa retornado, precisamos saber a moeda do fundo\n        fund_key = fund.replace('_', ' ')\n        fund_location = self.get_table(\"funds\").query(\"Fund == @fund_key\")[\"Location\"].squeeze()\n        currency_location  = \"us\" if currency == \"usd\" else \"bz\"\n        if fund_location != currency_location:\n            usdbrl = self.get_prices(usdbrl_ticker, previous_date=cash[\"Date\"].min(), recent_date=cash[\"Date\"].max(),)\n            usdbrl = self.usdbrl_clean_coupon_fix(usdbrl)\n            usdbrl[\"Date\"] = usdbrl[\"Date\"].dt.date\n            usdbrl = usdbrl.rename(columns={\"Last_Price\":\"usdbrl\"})\n            cash = cash.merge(usdbrl[[\"Date\", \"usdbrl\"]], on=\"Date\", how=\"left\")\n            if fund_location == \"bz\":\n                cash[\"Market_Value\"] = cash[\"Market_Value\"] / cash[\"usdbrl\"]\n            else:\n                cash[\"Market_Value\"] = cash[\"Market_Value\"] * cash[\"usdbrl\"]\n\n            # Removendo coluna auxiliar\n            cash = cash.drop(columns=[\"usdbrl\"])\n\n        # Vamos calcular o preco de fechamento com o rendimento do dia\n        bz_cash_index = self.get_prices(\"bzacselc index\", period=\"60m\", currency=currency, usdbrl_ticker=usdbrl_ticker)\n        bz_cash_index[\"Date\"] = bz_cash_index[\"Date\"].dt.date\n        bz_cash_index = (bz_cash_index[[\"Date\", \"Last_Price\"]]\n                            .rename(columns={\"Last_Price\":\"bz_cash_index\"})\n                            .set_index(\"Date\").pct_change()\n                            .fillna(0)+1).reset_index()\n\n        us_cash_index = self.get_prices(\"sofrindx index\", period=\"60m\", currency=currency, usdbrl_ticker=usdbrl_ticker)\n        us_cash_index[\"Date\"] = us_cash_index[\"Date\"].dt.date\n        us_cash_index = (us_cash_index[[\"Date\", \"Last_Price\"]]\n                            .rename(columns={\"Last_Price\":\"us_cash_index\"})\n                            .set_index(\"Date\").pct_change()\n                            .fillna(0)+1).reset_index()\n\n        us_margin_cost = self.get_prices(\"sofr + 75bps\", period=\"60m\", currency=currency, usdbrl_ticker=usdbrl_ticker)\n        us_margin_cost[\"Date\"] = us_margin_cost[\"Date\"].dt.date\n        us_margin_cost = (us_margin_cost[[\"Date\", \"Last_Price\"]]\n                            .rename(columns={\"Last_Price\":\"us_margin_cost\"})\n                            .set_index(\"Date\").pct_change()\n                            .fillna(0)+1).reset_index()\n\n        cash = cash.merge(bz_cash_index, on=\"Date\", how=\"left\")\n        cash = cash.merge(us_cash_index, on=\"Date\", how=\"left\")\n        cash = cash.merge(us_margin_cost, on=\"Date\", how=\"left\")\n        # Dependendo do ativo e da posicao a variacao no dia ser\u00e1 diferente\n        cash[\"Open_Price\"] = 1\n        cash[\"Close_Price\"] = cash[\"bz_cash_index\"]\n        cash[\"Close_Price\"] = np.where(cash[\"Group\"] == \"caixa_us\",\n                                       cash[\"us_cash_index\"], cash[\"Close_Price\"]\n                                       )\n        cash[\"Location\"] = 'us'\n        cash[\"Location\"] = np.where(cash[\"Group\"] == \"caixa_us\",\n                                       'us', 'bz'\n                                       )\n        cash[\"Close_Price\"] = np.where((cash[\"Market_Value\"] &lt; 0) &amp; (cash[\"Group\"] == \"caixa_us\") ,\n                                        cash[\"us_margin_cost\"], cash[\"Close_Price\"]\n                                        )\n\n        cash = (cash[[\"Date\", \"Group\", \"Location\", \"Market_Value\", \"Open_Price\", \"Close_Price\"]]\n                    .rename(columns={\"Date\":\"Today\",\n                                     \"Market_Value\":\"Close_Mkt_Value\"\n                                     }))\n        # Criando colunas necessarias para concatenar com o df do daily pnl\n        cash[\"Close_Quantity\"] = cash[\"Close_Mkt_Value\"]\n\n        cash[\"Type\"] = cash[\"Group\"]\n        cash[\"Name\"] = cash[\"Group\"].str.replace(\"_\", \" \").str.title()\n        cash[\"Asset_ID\"] = cash[\"Type\"].str.replace(\"_\",\" \")+\"_\"+cash[\"Type\"].str.replace(\"_\",\" \")\n        cash[\"Price_Key\"] = cash[\"Type\"]\n\n        cash[\"Delta_Shares\"] = cash.groupby([\"Group\"])[\"Close_Quantity\"].diff().fillna(0)\n        cash[\"Shares_Sold\"] = np.where(cash[\"Delta_Shares\"] &lt; 0, cash[\"Delta_Shares\"], 0)\n        cash[\"Amount_Sold\"] = cash[\"Shares_Sold\"]\n        cash[\"Shares_Bought\"] = np.where(cash[\"Delta_Shares\"] &gt; 0, cash[\"Delta_Shares\"], 0)\n        cash[\"Shares_Bought_Cost\"] = cash[\"Shares_Bought\"]\n\n        return cash\n\n\n    def __fix_open_price(self, df_op_fix, currency, ref_date, previous_date,\n                         usdbrl_ticker='bmfxclco curncy'):\n        if len(df_op_fix) == 0:\n            return pd.DataFrame()\n        # Guardando a lista de colunas, para remover colunas auxiliares posteriormente\n        columns_list= list(df_op_fix.columns)\n\n        asset_ids = df_op_fix[\"Asset_ID\"].unique()\n        assets = self.get_table(\"assets\").query(\"Key.isin(@asset_ids)\")\\\n                    .copy()[[\"Key\", \"Ticker\", \"Currency Exposure\"]].rename(columns={\n                        \"Key\":\"Asset_ID\", \"Currency Exposure\" : \"Currency_Exposure\"\n                    })\n\n        df_op_fix = df_op_fix.merge(assets, on='Asset_ID')\n        df_op_fix[\"Needs_Fix\"] = df_op_fix[\"Currency_Exposure\"] != currency\n\n        df_fixed = df_op_fix.query(\"~Needs_Fix\").copy()\n        df_op_fix = df_op_fix.query(\"Needs_Fix\").copy()\n\n\n        local_prices = self.get_prices(df_op_fix[\"Ticker\"].unique(), recent_date=ref_date,\n                                        previous_date=previous_date, currency='local')\n        local_prices[\"Open_Price\"] = local_prices.groupby(\"Asset\")[\"Last_Price\"]\\\n                                        .shift(1).fillna(local_prices[\"Last_Price\"])\n        usdbrl = self.get_prices(usdbrl_ticker, recent_date=ref_date,\n                                 previous_date=previous_date\n                                 )[[\"Date\", \"Last_Price\"]].rename(columns={\"Last_Price\":\"usdbrl\"})\n        local_prices = local_prices.merge(usdbrl, on=\"Date\").rename(\n                                    columns={\"Asset\":\"Ticker\", \"Date\" : \"Today\"})\n\n        # Considera apenas caso de brl e usd TODO: tratar outros casos que venham a existir\n        if currency == 'usd':\n            local_prices[\"Open_Price\"] /= local_prices[\"usdbrl\"]\n        else:\n            local_prices[\"Open_Price\"] *= local_prices['usdbrl']\n\n        # Substituindo Open_Price antigo pelo novo ja com a conversao correta de moeda\n        df_op_fix = df_op_fix.drop(columns=[\"Open_Price\"])\n        df_op_fix = df_op_fix.merge(local_prices[[\"Today\", \"Ticker\", \"Open_Price\"]], on=['Today', 'Ticker'])\n\n        df_fixed = pd.concat([df_fixed, df_op_fix])\n        # Removendo as colunas auxiliares\n        df_fixed = df_fixed[columns_list]\n\n        return df_fixed\n\n\n\n    def calculate_daily_pnl(self, fund, ref_date, currency, holiday_location=\"all\", bdays=10,\n            group_filters=[], type_filters=[], classification_filters=[], location_filters=[], asset_filters=[],\n            currency_exposure_filters=[], include_cash=True, include_cash_debt=False, ticker_filters=[], annual_adm_fee=0,\n            ignore_small_pnl=True, usdbrl_ticker=\"bmfxclco curncy\"): #test_op_fix=False):\n        \"\"\" Calcula o PnL do dia de cada ativo no fundo.\n            (Nao inclui o PnL das taxas!).\n        Args:\n            fund (str): nome do fundo\n            ref_date (str): data de refer\u00eancia do PnL\n            currency (str): moeda (usd|brl)\n            holiday_location (str, optional): refencia de feriados. Defaults to \"all\".\n            bdays (int, optional): quantidade de dias uteis a serem considerados.\n                                Defaults to 10.\n        \"\"\"\n        # dia util anterior\n        # Obtendo todas as posicoes e movimentacoes, dos ultimos dias\n        previous_date = self.get_bday_offset(ref_date, -bdays,\n                                        location=holiday_location)\n        df = self.get_positions_and_movements(fund, previous_date=previous_date,\n                                            recent_date=ref_date, currency=currency, usdbrl_ticker=usdbrl_ticker)\n        assets = self.get_table(\"assets\").rename(columns={\"Key\":\"Asset_ID\",\n                                                          \"Currency Exposure\":\"Currency_Exposure\"})\n\n        # Filtrando pelos ativos desejados\n        if len(classification_filters) &gt; 0:\n            classification_filters = [x.lower() for x in classification_filters]\n            assets = assets.query(\"Luxor_Classification.isin(@classification_filters)\")\n        elif len(group_filters) &gt; 0:\n            group_filters = [x.lower() for x in group_filters]\n            assets = assets.query(\"Group.isin(@group_filters)\")\n        elif len(type_filters) &gt; 0:\n            type_filters = [x.lower() for x in type_filters]\n            assets = assets.query(\"Type.isin(@type_filters)\")\n        elif len(location_filters) &gt; 0:\n            location_filters = [x.lower() for x in location_filters]\n            assets = assets.query(\"Location.isin(@location_filters)\")\n        elif len(asset_filters) &gt; 0:\n            asset_filters = [x.lower() for x in asset_filters]\n            assets = assets.query(\"Asset.isin(@asset_filters)\")\n        elif len(ticker_filters) &gt; 0:\n            ticker_filters = [x.lower() for x in ticker_filters]\n            assets = assets.query(\"Ticker.isin(@ticker_filters)\")\n        elif len(currency_exposure_filters) &gt; 0:\n            currency_exposure_filters = [x.lower() for x in currency_exposure_filters]\n            assets = assets.query(\"Currency_Exposure.isin(@currency_exposure_filters)\")\n        # Fazemos uma juncao interna, mantendo somente as linhas filtradas acima\n        df = df.merge(assets[[\"Asset_ID\", \"Name\", \"Group\", \"Type\", \"Location\",\n                               \"Luxor_Classification\"]], on=\"Asset_ID\")\n\n        df = df.sort_values(by=\"Date\", ascending=True)\n        df = df.rename(columns={\n            \"Date\" : \"Today\",\n            \"Open_Quantity\" : \"Close_Quantity\",\n            \"Last_Price\" : \"Close_Price\",\n            \"Current_Position_Value\" : \"Close_Mkt_Value\"\n        })\n\n        # Obtendo o preco de abertura\n        df[\"Open_Price\"] = df.groupby(\"Asset_ID\")[\"Close_Price\"].shift(1).fillna(df[\"Close_Price\"])\n        types_to_fix_open_price = ['ndf usdbrl']\n        #if test_op_fix:\n\n        df_op_fix = df.query(\"Type.isin(@types_to_fix_open_price)\").copy()\n        df_op_fix = self.__fix_open_price(df_op_fix, currency, ref_date, previous_date, usdbrl_ticker=usdbrl_ticker)\n\n        df = df.query(\"~Type.isin(@types_to_fix_open_price)\").copy()\n\n        # Juntando dfs novamente\n        df = pd.concat([df, df_op_fix])\n\n\n\n        # Precisamos consertar o open_price do FX. (Deve ser convertido pelo spot de fechamento sempre)\n\n        # Podemos puxar o caixa aqui e concatenar para as proximas operacoes\n        if include_cash:\n            cash = self.get_hist_cash_movements(fund, ref_date, currency=currency, bdays=bdays, holiday_location=holiday_location,\n                                                usdbrl_ticker=usdbrl_ticker)\n            cash[\"Today\"] = pd.to_datetime(cash[\"Today\"])\n            cash = cash.query(\"Close_Mkt_Value &gt;= 0\").copy()\n            cash[\"Luxor_Classification\"] = 'fixed income'\n\n            if len(location_filters) &gt; 0:\n                location_filters = [x.lower() for x in location_filters]\n                cash = cash.query(\"Location.isin(@location_filters)\").copy()\n\n            df = pd.concat([df, cash])\n            df = df.sort_values(by=\"Today\", ascending=True)\n\n        # Vamos segregar como d\u00edvida o caixa virado.\n        if include_cash_debt:\n            cash_debt = self.get_hist_cash_movements(fund, ref_date, currency=currency, bdays=bdays, holiday_location=holiday_location,\n                                                     usdbrl_ticker=usdbrl_ticker)\n            cash_debt[\"Today\"] = pd.to_datetime(cash_debt[\"Today\"])\n            cash_debt = cash_debt.query(\"Close_Mkt_Value &lt; 0\").copy()\n            cash_debt[\"Luxor_Classification\"] = 'debt'\n\n            # Caixa virado no Brasil, vamos desconsiderar, pois na pr\u00e1tica nao teremos.\n            #cash_debt = cash_debt.query(\"Asset_ID != 'caixa_caixa' or Location != 'bz'\").copy()\n            #Optando por manter e retirar o PnL posteriormente, para nao perder a contribuicao dele pra aporte e resgate\n\n            if len(location_filters) &gt; 0:\n                location_filters = [x.lower() for x in location_filters]\n                cash_debt = cash_debt.query(\"Location.isin(@location_filters)\").copy()\n\n            df = pd.concat([df, cash_debt])\n            df = df.sort_values(by=\"Today\", ascending=True)\n\n        # Vamos obter a data d-1\n        df[\"Yesterday\"] = (df.groupby(\"Asset_ID\")[\"Today\"].shift(1)\n                            .fillna(df[\"Today\"]-dt.timedelta(days=1))\n                            )\n        # Obtendo quantidade na abertura\n        df[\"Open_Quantity\"] = df.groupby(\"Asset_ID\")[\"Close_Quantity\"].shift(1).fillna(0)\n\n        # Obtendo valor de mercado por ativo na abertura\n        df[\"Open_Mkt_Value\"] = df[\"Open_Quantity\"] * df[\"Open_Price\"]\n        # Calculando precos de compas e vendas\n        df[\"Buy_Price\"] = np.where(df[\"Shares_Bought\"] &gt; 0, \n                            abs(df[\"Shares_Bought_Cost\"]/df[\"Shares_Bought\"]),0)\n        df[\"Sell_Price\"] = np.where(df[\"Shares_Sold\"] &lt; 0,\n                            abs(df[\"Amount_Sold\"]/df[\"Shares_Sold\"]), 0)\n\n        # Calculando PnL Di\u00e1rio (Caixa nao pode ser calculado aqui, pois o preco eh sempre 1)\n        df[\"Pnl_Bought\"] = abs(df[\"Shares_Bought\"]) * (df[\"Close_Price\"] - df[\"Buy_Price\"])\n        df[\"Pnl_Sold\"] = abs(df[\"Shares_Sold\"]) * (df[\"Sell_Price\"] - df[\"Open_Price\"])\n        # Ajustando PnL sold. Quando for debt, a venda representa a divida sendo tomada\n        # e a compra representa o pagamento da divida. Na primeira, ha juros e na segunda nao ha pnl.\n        df['Pnl_Bought'] = np.where(df['Luxor_Classification'] == 'debt', 0, df['Pnl_Bought'])\n        df['Pnl_Sold'] = np.where(df['Luxor_Classification'] == 'debt', \n                                  (df['Open_Price']-df['Close_Price'])*abs(df['Shares_Sold']), 0)\n\n        df[\"Pnl_Unchanged\"] = (df[\"Open_Quantity\"] + df[\"Shares_Sold\"]) * (df[\"Close_Price\"] - df[\"Open_Price\"])\n\n        # O caixa no BZ pode estar virado temporariamente durante aportes/resgates de caixa_us, mas isso n estar\u00e1 gerando PnL\n        df[\"Pnl_Unchanged\"] = np.where((df[\"Asset_ID\"] == 'caixa_caixa') &amp; (df[\"Location\"] == \"bz\") &amp; (df[\"Location\"] == \"bz\")&amp; (df[\"Close_Mkt_Value\"] &lt; 0), 0, df[\"Pnl_Unchanged\"])\n        df[\"Pnl_Sold\"] = np.where((df[\"Asset_ID\"] == 'caixa_caixa') &amp; (df[\"Location\"] == \"bz\") &amp; (df[\"Close_Mkt_Value\"] &lt; 0), 0, df[\"Pnl_Sold\"])\n\n\n\n        # Ajustar aqui as operacoes com logica de pnl e exposicao\n        # Para nao considerar no AUM e no Cash Flow\n        types_to_recalculate = ['ndf usdbrl']\n        # Separando os dados em dois grupos, para editar um deles\n        df_exposure = df.query(\"Type.isin(@types_to_recalculate)\").copy()\n\n        df = df.query(\"~Type.isin(@types_to_recalculate)\").copy()\n        # Calculo especifico para pnl do FX.\n        df_exposure[\"Total_Pnl\"] = df_exposure[\"Pnl_Bought\"] + df_exposure[\"Pnl_Sold\"] \\\n                                     + df_exposure[\"Pnl_Unchanged\"]\n\n        df_exposure = df_exposure.set_index([\"Asset_ID\"])\n        # Valor de mercado sera o PnL acumulado\n        df_exposure[\"Total_Pnl\"] = df_exposure.groupby([\"Asset_ID\"])[\"Total_Pnl\"].cumsum()\n        df_exposure = df_exposure.reset_index()\n        df_exposure[\"Close_Mkt_Value\"] = df_exposure[\"Total_Pnl\"]\n        df_exposure[\"Open_Mkt_Value\"] = df_exposure[\"Close_Mkt_Value\"].shift(1, fill_value=0)\n\n        # Retirando qualquer possibilidade de impacto para aporte e resgate\n        df_exposure[\"Shares_Bought_Cost\"] = 0\n        df_exposure[\"Amount_Sold\"] = 0\n        # TODO Pensar o que vai mudar no caso de uma zeragem parcial\n        df_exposure = df_exposure.drop(columns=\"Total_Pnl\")\n\n\n        # Juntando dfs novamente\n        df = pd.concat([df, df_exposure])\n\n        # Calculando net de aporte e resgate por ativo\n        df[\"Cash_Flow\"] = abs(df[\"Shares_Bought_Cost\"]) + -abs(df[\"Amount_Sold\"])\n\n        hist_dividends = self.get_dividends(fund, ref_date, previous_date, currency=currency,\n                                            holiday_location=holiday_location, usdbrl_ticker=usdbrl_ticker)\n        hist_dividends = pd.merge(assets[[\"Asset_ID\", \"Ticker\"]],\n                            hist_dividends, on=\"Ticker\",how=\"left\")[[\"Date\", \"Asset_ID\", \"Amount\"]]\n        hist_dividends = hist_dividends.rename(columns={\"Date\": \"Today\", \"Amount\":\"Dividends\"})\n\n        df = df.merge(hist_dividends, on=[\"Today\", \"Asset_ID\"], how=\"left\")\n        df[\"Dividends\"] = df[\"Dividends\"].fillna(0)\n\n\n        df[\"Net_Subscriptions_Redemptions\"] = df.groupby(\"Today\")[\"Cash_Flow\"].sum()\n\n        df = df[[\"Today\", \"Name\", \"Asset_ID\", \"Group\", \"Type\", \"Location\", \"Luxor_Classification\",\n                \"Open_Quantity\", \"Open_Mkt_Value\", \"Close_Quantity\",\n                \"Shares_Bought\", \"Buy_Price\", \"Shares_Sold\", \"Sell_Price\",\n                \"Dividends\", \"Close_Price\", \"Open_Price\", \"Close_Mkt_Value\",\n                \"Cash_Flow\", \"Pnl_Bought\", \"Pnl_Sold\", \"Net_Subscriptions_Redemptions\", \"Pnl_Unchanged\"]]\n\n\n        # Calculando AUM no fechamento\n        df = df.set_index(\"Today\")\n        df[\"Close_AUM\"] = df.groupby(\"Today\")[\"Close_Mkt_Value\"].sum()\n        df = df.reset_index()\n\n        if annual_adm_fee != 0:\n        # Incluir taxas de adm e gestao no calculo do PnL\n        # Elas ja impactam o caixa, mas precisam ser consideradas no PnL\n            daily_adm_fee = (1+abs(annual_adm_fee))**(1/365)-1\n            df_fees = df[[\"Today\", \"Close_AUM\"]].groupby(\"Today\").last()\n            df_fees[\"Pnl_Unchanged\"] = -df_fees[\"Close_AUM\"] * daily_adm_fee\n            df_fees = df_fees.reset_index()\n            df_fees[\"Asset_ID\"] = \"taxas e custos_tx adm\"\n\n            value_columns = df.columns[7:-2] # Sobrescreve com 0, menos para Pnl_Unchanged e Close_AUM\n\n            df_fees[value_columns] = 0 # Para taxas, todos os outros valores nao importam.\n            # Finalmente, falta colocar dados de name, type e group\n            all_assets = self.get_table(\"assets\").rename(columns={\"Key\":\"Asset_ID\"})\n\n            df_fees = df_fees.merge(all_assets[[\"Asset_ID\", \"Name\",\"Type\", \"Group\", \"Location\"]], on=\"Asset_ID\")\n            if len(location_filters) &gt; 0:\n                location_filters = [x.lower() for x in location_filters]\n                df_fees = df_fees.query(\"Location.isin(@location_filters)\")\n\n            df = pd.concat([df, df_fees])  \n\n\n        df[\"Daily_Pnl\"] = df[\"Pnl_Unchanged\"] + df[\"Pnl_Bought\"] + df[\"Pnl_Sold\"] + df[\"Dividends\"]\n\n        # Calculando PL Inicial Ajustado\n        df = df.set_index(\"Today\")\n        df[\"Net_Subscriptions_Redemptions\"] = df.reset_index().groupby(\"Today\")[\"Cash_Flow\"].sum()\n        # Sera o valor de mercado da abertura somado com o net de aportes.\n        # Racional: Resgates rentabilizam no dia (ja estao no inicial)\n        #           Aportes rentabilizam no dia (nao estano no inicial)\n        df[\"Open_AUM\"] = df.reset_index().groupby(\"Today\")[\"Open_Mkt_Value\"].sum()\n        df[\"Open_AUM_Adjusted\"] = (df[\"Open_AUM\"] + \n                                    df[\"Net_Subscriptions_Redemptions\"]*(df[\"Net_Subscriptions_Redemptions\"] &gt; 0)\n                                    )\n\n        df[\"Daily_Attribution\"] = df[\"Daily_Pnl\"] / df[\"Open_AUM_Adjusted\"]\n        df[\"Daily_Return\"] = df.reset_index().groupby(\"Today\")[\"Daily_Attribution\"].sum()\n\n\n        # ao retornar, filtrar port Daily_Pnl != 0 e  not Daily_Pnl.isna()\n        if ignore_small_pnl:\n            return (df.reset_index()\n                .sort_values(by=\"Today\").query(''' (Daily_Pnl &lt;= -0.01 or Daily_Pnl &gt;= 0.01)\\\n                                            and not Daily_Pnl.isna()'''))\n        return df.reset_index().sort_values(by=\"Today\")\n\n        #return df.reset_index().sort_values(by=\"Today\")\n\n\n    def get_dividends(self, fund_name, recent_date, previous_date=None, period=None, currency='usd', \n                      usdbrl_ticker='bmfxclco curncy', holiday_location='all'):\n\n        if period is not None:\n            previous_date = self.get_start_period_dt(recent_date, period=period, holiday_location=holiday_location)\n\n        df = self.get_table(\"hist_dividends\").query(\"Fund == @fund_name and Date &gt;= @previous_date and Date &lt;= @recent_date\").copy()\n        df = df.reset_index(drop=True)\n        if currency == 'local':\n            return df\n\n        df_usd = df.query(\"Currency == 'usd'\").copy()\n        df_brl = df.query(\"Currency == 'brl'\").copy()\n\n        if currency == 'usd':\n            df_brl_mod = self.convert_currency(df_brl[[\"Date\", \"Amount\"]].rename(columns={\"Amount\":\"Last_Price\"}),\n                                  price_currency=\"brl\",\n                                  dest_currency=\"usd\"\n                                  ).rename(columns={'Last_Price' : 'Amount'})\n            df_brl.loc[df_brl_mod.index, 'Amount'] = df_brl_mod['Amount']\n\n        if currency == 'brl':\n            df_usd_mod = self.convert_currency(df_usd[[\"Date\", \"Amount\"]].rename(columns={\"Amount\":\"Last_Price\"}),\n                                  price_currency=\"usd\",\n                                  dest_currency=\"brl\"\n                                  ).rename(columns={'Last_Price':'Amount'})\n            df_usd.loc[df_usd_mod.index, 'Amount'] = df_usd_mod['Amount']\n\n        df = pd.concat([df_brl, df_usd])[['Date', 'Ticker', 'Amount']]\n\n        if fund_name == 'lipizzaner':\n            df_fund_a = self.get_dividends('fund a', recent_date, previous_date, period, currency, usdbrl_ticker, holiday_location)\n            df = pd.concat([df, df_fund_a])\n\n        return df\n\n\n\n    def get_position_variation(self, fund_name, recent_date, previous_date):\n        \"\"\"\n            Fornece um dataframe com a variacao das posicoes do fundo entre as datas. \n            Inclui tambem uma coluna de flag informando se a posicao foi zerada ou nao\n\n        \"\"\"\n\n        # Pegamos as posicoes da data mais recente, transformando de dict para dataframe\n        cur_positions = self.get_positions(fund_name, recent_date)\n        cur_positions = pd.DataFrame(cur_positions.items(), columns = [\"Key\", \"#_cur\"])\n\n        # Igual para as posicoes da data mais antiga\n        prev_positions = self.get_positions(fund_name, previous_date)\n        prev_positions = pd.DataFrame(prev_positions.items(), columns=[\"Key\", \"#_prev\"])\n\n        # Realizamos juncao externa na chave, mantendo assim dados nan (zeragens e ativos novos)\n        diff = pd.merge(cur_positions, prev_positions, on=\"Key\", how=\"outer\").fillna(0)\n\n        diff[\"Variation\"] = diff[\"#_cur\"] - diff[\"#_prev\"]\n        diff[\"Closed\"] = diff[\"#_cur\"] == 0\n\n        return diff[[\"Key\", \"Variation\", \"Closed\"]]\n\n\n    def get_risk_metric(self, fund_name,  metrics=None, date=dt.date.today()):\n        \"\"\"\n            Retorna as metricas de risco numa determinada data.\n\n        Args:\n            fund_name (str): nome do fundo correspontente\n            metrics (str, list(str)): str da metrica desejada ou uma lista de strings\n             para mais de uma metrica. Defaults to None (all available).\n            date (datetime.date, optional): Data da metrica. Defaults to datetime.date.today().\n\n        Returns:\n\n            Pode retornar o valor de uma metrica especifica ou Dataframe quando \n            metrics passado for uma lista de metricas.\n\n        \"\"\"\n        fund_name = fund_name.replace(\"_\", \" \")\n\n        hist_metrics = self.get_table(\"hist_risk_metrics\")\n        hist_metrics = hist_metrics.loc[((hist_metrics[\"Fund\"] == fund_name) &amp; (hist_metrics[\"Date\"].dt.date &lt;= date) )].tail(1).reset_index(drop=True)\n\n        if metrics is None:\n            metrics = list(hist_metrics.columns)\n            metrics.remove(\"Date\")\n            metrics.remove(\"Fund\")\n\n        try:\n            if type(metrics) == list:\n\n                return hist_metrics[metrics].astype(float)\n\n            return float(hist_metrics[metrics].squeeze())\n        except KeyError:\n            logger.error(f\"Metrica(s) {metrics} indisponivel(is)\")\n\n\n    def get_risk_metric_variation(self, fund_name, recent_date, previous_date, metrics=None):\n\n        recent = self.get_risk_metric(fund_name, date=recent_date, metrics=metrics)\n        previous = self.get_risk_metric(fund_name, date=previous_date, metrics=metrics)\n\n        return recent - previous\n\n\n    def get_group_results(self, previous_date= None, recent_date = None,  segments=None, currency=\"brl\", inception_dates=None, period = None) -&gt; pd.DataFrame:\n        \"\"\"Calcula e retorna o resultado de cada segmento.\n\n        Args:\n            recent_date (dt.date): Data inicial NAO inclusiva (sera filtrado por uma data anterior a essa.)\n                                    Ex. Para considerar retorno ate o mes de marco, deve ser passada alguma data em abril.\n            previous_date (dt.date): Data final\n            segments (_type_, optional): Lista ou string com nome dos segmentos \n                desejados. Defaults to None. Quando None, retorna todos os segmentos existentes.\n\n        Returns:\n            pd.DataFrame: Retorno por segmento.\n        \"\"\"\n\n        if (previous_date is not None) and (previous_date &gt; recent_date):\n            logger.error(\"Data inicial \u00e9 menor que a data final. Parametros invertidos?\")\n            sys.exit()\n\n        # Obtendo tabela de retornos historicos extraida do retorno consolidado\n        group_results = self.get_table(\"luxor group results\")[[\"Date\", \"Segment\", \"Return_Multiplier\"]].copy()\n        group_results[\"Date\"] = pd.to_datetime(group_results[\"Date\"])\n\n        # Selecionando o periodo de tempo desejado\n        if period is not None:\n            recent_date = dt.date(recent_date.year,recent_date.month,20)+dt.timedelta(days = 15)\n            previous_date = self.get_start_period_dt(recent_date, period=period)\n            if period.lower() == \"ytd\":\n                if recent_date.month == 1: # fechamento de dezembro! Vamos ter que ajustar o previous_date\n                    previous_date = self.get_start_period_dt(recent_date-dt.timedelta(days=35), period=period)\n                previous_date += dt.timedelta(days=1)\n\n        group_results = group_results.query(\"Date &gt;= @previous_date and Date &lt; @recent_date\").sort_values(by=(\"Date\")).copy()\n\n        if segments is not None:\n            # Vamos deixar apenas os segmentos informados no parametro 'segments'\n            if type(segments) is str:\n                segments = [segments]\n            if type(segments) is list:\n                seg_filter = segments\n            else:\n                logger.error(\"'segments' precisa ser do tipo 'list' ou 'str'\")\n                sys.exit()\n\n            group_results = group_results.query(\" Segment.isin(@seg_filter)\").copy()\n\n\n        group_results = group_results.set_index(\"Date\").groupby(\"Segment\").prod() -1\n        group_results = group_results.reset_index()\n\n        # Retornos dos segmentos, por padrao, estarao em R$.\n        # Para ver em US$ precisa ser feita a conversao.\n        if currency == \"usd\":\n\n            def __get_usdbrl_variation(segment):\n                # ajustando janelas de inicio e fim, para pegar a variacao correta de usd no periodo -&gt;  A base do grupo eh de retornos, a de usd eh de pre\u00e7os                \n                usd_recent_date  = dt.date(recent_date.year, recent_date.month, 1) - dt.timedelta(days=1)\n                usd_previous_date = dt.date(previous_date.year, previous_date.month, 1) -dt.timedelta(days=1)\n                # Ainda, se a data for anterior ou igual ao inceptio date, vamos usar o incepion date\n                #print(f\"conversao usdbrl usando: previous_date:{usd_previous_date}  recent_date: {usd_recent_date}\")\n                if inception_dates is not None and segment in inception_dates.keys():\n\n                    sgmnt_inception = inception_dates[segment] -dt.timedelta(days=1) # subtraindo 1 dia para usar o fechamento do dia anterior ao inicio\n                    usd_previous_date = sgmnt_inception if ((sgmnt_inception.year == previous_date.year) and (sgmnt_inception.month == previous_date.month)) else usd_previous_date\n                                            #\"bmfxclco curncy\" -&gt; converter usando usd cupom limpo\n\n                delta_usdbrl = self.get_pct_change(\"bmfxclco curncy\", previous_date=usd_previous_date, recent_date=usd_recent_date)\n                #print(f\"Segmento: {segment}  Retorno_BRL:{multiplier}  delta usdbrl={delta_usdbrl}   Retorno_USD: {(1+multiplier)/(1+delta_usdbrl)-1}\")\n                return delta_usdbrl\n\n\n            # Convertendo para usd o que estiver em brl\n\n            group_results[\"Return_Multiplier\"] = group_results.apply(lambda row: (1 + row[\"Return_Multiplier\"])/(1+__get_usdbrl_variation(row[\"Segment\"])) -1, axis=1)\n\n        group_results = group_results.set_index(\"Segment\")\n\n        return group_results\n\n    def calculate_fund_particip(self, fund_name, fund_owned, date):\n        \"\"\"\n            Informa o percentual que 'fund_name' possui do 'fund_owned'.\n\n        Args:\n            fund_name (str) \n            fund_owned (str)\n            date (dt.date) \n        Returns:\n            (float)\n        \"\"\"\n        fund_name = fund_name.replace(\"_\", \" \")\n        fund_owned_key = fund_owned + \"_\" + fund_owned\n\n        # antes da data de incorporacao do manga master pelo lipi, 100% do manga master era do manga fic\n        if fund_name == \"mangalarga fic fia\" and fund_owned == \"mangalarga master\":\n            if date &lt; dt.date(2022,12,9):\n                return 1.0\n            else: return 0\n        # antes da data de incorporacao do manga master pelo lipi, 100% do fund_a era do manga master\n        if fund_name == \"mangalarga master\" and fund_owned == \"fund a\":\n            if date &lt; dt.date(2022,12,9):\n                return 1.0\n            else: return 0\n        # Adicionando mais um nivel de calculo \n        if fund_name == \"mangalarga fic fia\" and fund_owned == \"fund a\":\n            if date &lt; dt.date(2022,12,9):\n                return self.calculate_fund_particip(fund_name, \"mangalarga master\", date) * self.calculate_fund_particip(\"mangalarga master\", fund_owned, date)\n            else:\n                return self.calculate_fund_particip(fund_name, \"lipizzaner\", date) * self.calculate_fund_particip(\"lipizzaner\", fund_owned, date)\n\n        if fund_name == \"mangalarga ii fic fia\" and fund_owned == \"fund a\":\n            if date &lt; dt.date(2023,2,7):\n                return self.calculate_fund_particip(\"mangalarga fic fia\", \"mangalarga master\", date)\n            else:\n                return self.calculate_fund_particip(fund_name, \"lipizzaner\", date) * self.calculate_fund_particip(\"lipizzaner\", fund_owned, date)\n\n        position = self.get_table(\"hist_positions\")\n\n        # Busca otimizada pela posicao\n        try:        \n            position = (position[\"#\"].to_numpy()[(\n                            (position[\"Fund\"].to_numpy() == fund_name)\n                            &amp;\n                            (position[\"Asset_ID\"].to_numpy() == fund_owned_key)\n                            &amp;\n                            (position[\"Date\"].dt.date.to_numpy() &lt;= date)\n                        )].item(-1))\n        except (IndexError , KeyError):\n            # nao havia participacao do fundo em questao na data informada\n            return 0\n\n\n        fund_owned_total_amount = self.get_table(\"all_funds_quotas\")\n\n        try:\n            fund_owned_total_amount = (fund_owned_total_amount[\"#\"].to_numpy()[(\n                                            (fund_owned_total_amount[\"Fund\"].to_numpy() == fund_owned)\n                                            &amp;\n                                            (fund_owned_total_amount[\"Date\"].dt.date.to_numpy() &lt;= date)\n                                            )].item(-1))\n        except (IndexError , KeyError):\n\n            # Algo deu errado\n            return None\n\n        return round(position/fund_owned_total_amount, 5) # Mantem 5 casas decimais de precisao\n\n    def get_fund_aum(self, fund_name, date):\n\n        hist_quotas = self.get_table(\"all_funds_quotas\").query(\"Fund == @fund_name\")\n\n        return hist_quotas[\"#\"].to_numpy()[hist_quotas[\"Date\"].dt.date.to_numpy()==date].item(-1) * self.get_price(fund_name, px_date=date)\n\n\n    def get_fund_numb_shares(self, fund_name, date):\n\n        hist_quotas = self.get_table(fund_name+\"_quotas\")\n        return hist_quotas[\"#\"].to_numpy()[hist_quotas[\"Date\"].dt.date.to_numpy()==date].item(-1)\n\n\n    def get_delta_subscription_redemption(self, fund, previous_date, recent_date):\n\n        hist_quotas = (self.get_table(\"all_funds_quotas\")\n                           .query(\"Fund == @fund and Date&gt;= @previous_date and Date &lt;= @recent_date\")\n                           .rename(columns={\"#\" : \"Quota_Amnt\"})[[\"Quota_Amnt\", \"Quota\"]]\n                           )\n        hist_quotas[\"Delta_Quotas\"] = hist_quotas[\"Quota_Amnt\"].diff().fillna(0)\n        # Nao eh necessario usar a cota do dia anterior. Caso precise, basta fazer o shift abaixo\n        # hist_quotas[\"Quota\"] = hist_quotas[\"Quota\"].shift(1).fillna(0)\n        delta = (hist_quotas[\"Delta_Quotas\"] * hist_quotas[\"Quota\"]).sum()\n        return delta\n\n\n    def is_month_end(self, date):\n        cur_m = date.month\n        return (date + dt.timedelta(days=1)).month != cur_m\n\n\n    def get_fund_pnl(self, fund, previous_date, recent_date): #, orig_curncy=None, dest_curncy=None):\n\n        # Nao ha nenhuma conversao de moeda a ser feita\n        #if orig_curncy is None or dest_curncy is None or orig_curncy == dest_curncy:\n\n        start_aum = self.get_fund_aum(fund, date = previous_date)\n        end_aum = self.get_fund_aum(fund, date = recent_date)\n        period_cash_flow = self.get_delta_subscription_redemption(fund, previous_date, recent_date)\n\n        #if fund == \"fund a\":\n        #    print()\n        #    print(f\"{fund}, prev:{previous_date} rec:{recent_date} pnl:{end_aum - start_aum - period_cash_flow} = {end_aum} - {start_aum} - {period_cash_flow}\")\n\n        return end_aum - start_aum - period_cash_flow\n\n        # Desativando pois nao esta computando o resultado corretamente\n        # Eh necessario converter de uma moeda para outra\n        #location = self.get_table(\"funds\").query(\"Fund == @fund\")[\"Location\"].squeeze()\n        #end_date = recent_date\n        #recent_date = previous_date\n        #\n        #total_pnl = 0\n#\n        #while recent_date &lt; end_date:\n#\n        #    recent_date = self.get_next_attr_date(fund, previous_date, location, mode=\"week\")\n#\n        #    if recent_date &gt; end_date:\n        #        recent_date = end_date\n#\n        #    # chamada recursiva, que roda apenas a primeira parte\n        #    pnl = self.get_fund_pnl(fund, previous_date, recent_date)\n        #    \n        #    end_usdbrl = self.get_price(\"bmfxclco curncy\", px_date=recent_date)\n        #    \n        #    if orig_curncy == \"brl\" and dest_curncy ==\"usd\":\n        #        total_pnl += pnl / end_usdbrl\n        #    elif orig_curncy == \"usd\" and dest_curncy == \"brl\":\n        #        total_pnl += pnl * end_usdbrl\n        #    \n        #    previous_date = recent_date\n#\n        #return total_pnl\n\n\n    def get_eduardo_lipi_particip(self, date=dt.date.today()):\n\n        funds_particip = (self.calculate_fund_particip(\"mangalarga fic fia\", \"lipizzaner\", date=date) \n                        + self.calculate_fund_particip(\"mangalarga ii fic fia\", \"lipizzaner\", date=date) \n                        + self.calculate_fund_particip(\"maratona\", \"lipizzaner\", date=date)\n                        + self.calculate_fund_particip(\"fund c\", \"lipizzaner\", date=date)) \n\n        if funds_particip &gt; 1:\n            print(\" ----&gt; Aten\u00e7\u00e3o! Calculo de participacao ficou errado para o eduardo. &lt;----\")\n\n        return 1 - funds_particip\n\n\n    def get_next_attr_date(self, fund, start_date, location, mode): #funds_info):\n        # Funcao auxiliar do attribution, permite pegar a proxima data valida para contabilizacao do attribution\n        # Levar em consideracao que lipizzaner, antes de incorporar o manga, tinha tamb\u00e9m cota diaria.\n        if mode == \"day\" or (fund == \"lipizzaner\" and start_date &lt; dt.date(2022,12,9)):\n            return self.get_bday_offset(date=start_date, offset=1, location=location)\n        if mode == \"week\":\n            thursday = 3 #weekday de quinta-feira (base da cota semanal)\n            start_date_weekday = start_date.weekday()\n            # Vamos setar a data como a proxima sexta e pegar o primeiro dia util anterior\n            friday_offset = 8 if start_date_weekday == thursday else -(start_date_weekday - thursday - 1)\n            next_date = self.get_bday_offset(date=start_date + dt.timedelta(days=friday_offset), offset=-1, location=location)\n\n            # Caso com muitos feriados proximos. Vamos resolver pegando o proximo dia util, para facilitar.\n            if next_date &lt;= start_date:\n                return self.get_bday_offset(date=start_date, offset=1, location=location)\n\n            if next_date == dt.date(2023,1,12): next_date = dt.date(2023,1,16)\n\n            if next_date.month != start_date.month:\n                # houve troca de mes. Se o ultimo dia do mes nao for start_date, vamos retornos o final do mes\n                month_ending = dt.date(start_date.year, start_date.month, 15) + dt.timedelta(days=25)\n                month_ending = dt.date(month_ending.year, month_ending.month, 1) - dt.timedelta(days=1)\n                # mas se start_date for o month_ending, entao esta correto retornar next_date.\n                if month_ending == start_date:\n                    return next_date \n                return month_ending\n            return next_date\n        logger.critical(\"'mode' desconhecido.\")\n\n\n    def calculate_attribution(self, fund, previous_date, recent_date, fund_currency):\n        \"\"\" Calcula o attribution do fundo informado para qualquer periodo.\n\n        Args:\n            fund (str): nome do fundo (lipizzaner, fund a, fund b, ...)\n            previous_date (datetime.date): data inicial\n            recent_date (datetime.date): data final\n            fund_currency (str): 'brl' ou 'usd\n\n        Returns:\n            pandas.DataFrame: Dataframe contendo o attribution (% e pnl) por ativo no periodo.\n        \"\"\"\n\n        # Acumulando os p&amp;l's do periodo\n        hist_attr = (self.get_table(\"hist_attr_base\", index=True, index_name=\"Key\")\n                         .query(\"Fund == @fund and Start_Date&gt;= @previous_date and End_Date &lt;= @recent_date\")\n                         )\n        # Corrigindo datas de inicio e fim, para datas validas considerando as janelas de attribution calculadas.\n        previous_date = hist_attr[\"Start_Date\"].min().date()\n        recent_date = hist_attr[\"End_Date\"].max().date()\n\n        hist_attr = hist_attr[[\"p&amp;l_brl\",\"p&amp;l_brl_curncy\",\"p&amp;l_usd\",\"p&amp;l_usd_curncy\",  \"attr_brl\",  \"attr_brl_curncy\", \"attr_usd\", \"attr_usd_curncy\"]].groupby(\"Key\").sum()\n\n        #hist_attr = hist_attr[[\"p&amp;l_brl\",\"p&amp;l_brl_curncy\",\"p&amp;l_usd\",\"p&amp;l_usd_curncy\"]].groupby(\"Key\").sum()\n\n\n        # Vamos encontrar o total de p&amp;l do periodo e a rentabilidade no periodo, em reais e em dolares\n        end_usdbrl = self.get_price(\"bmfxclco curncy\", recent_date)\n        average_usdbrl = self.get_table(\"hist_px_last\").query(\"Asset == 'usdbrl curncy' and Date &gt;= @previous_date and Date &lt;= @recent_date\")[\"Last_Price\"].mean()\n        usdbrl_variation = self.get_pct_change(\"bmfxclco curncy\", previous_date=previous_date, recent_date=recent_date)\n\n        # Calculando pnl e rentab totais (ainda nao sabemos em qual moeda esta)\n        total_pnl = self.get_fund_pnl(fund, previous_date, recent_date)\n        total_rentab = self.get_pct_change(fund, previous_date=previous_date, recent_date=recent_date)\n        others_pnl = total_pnl - hist_attr[\"p&amp;l_\"+fund_currency].sum()\n        others_pnl_curncy = total_pnl - hist_attr[\"p&amp;l_\"+fund_currency+\"_curncy\"].sum()\n\n        # Inicializando todas as possibilidades\n        total_pnl_brl, total_pnl_usd, total_rentab_brl, total_rentab_usd = total_pnl, total_pnl, total_rentab, total_rentab\n        others_pnl_brl, others_pnl_usd = others_pnl, others_pnl\n        others_pnl_brl_curncy, others_pnl_usd_curncy = others_pnl_curncy, others_pnl_curncy\n\n        # Encontrando moeda e corrigindo valores.\n        if fund_currency == \"usd\":\n            others_pnl_brl = others_pnl_brl * end_usdbrl\n            others_pnl_brl_curncy = others_pnl_brl_curncy * end_usdbrl\n            total_pnl_brl = hist_attr[\"p&amp;l_brl\"].sum() + others_pnl_brl\n            total_rentab_brl = (1+total_rentab_usd)*(1+usdbrl_variation)-1\n            # Nesse caso:\n            # O pnl total precisa ter o mesmo sinal da rentabilidade total, senao algo esta errado.\n            if total_pnl_brl * total_rentab_brl &lt; 0:\n                print(f\"Rentabilidade e pnl com sinais trocados. Conferir '{fund}' de '{previous_date}' ate '{recent_date}'.\")\n                print(f\"Obtendo pnl total a partir da rentabilidade e PL medio do periodo\")\n                fund_quotas = self.get_table(\"all_funds_quotas\").query(\"Fund ==@fund and Date &gt;= @previous_date and Date &lt;= @recent_date\")\n                total_pnl_brl = (fund_quotas[\"#\"] * fund_quotas[\"Quota\"]).mean() * average_usdbrl * total_rentab_brl\n\n        elif fund_currency == \"brl\":\n            others_pnl_usd =others_pnl_usd / end_usdbrl \n            others_pnl_usd_curncy = others_pnl_usd_curncy / end_usdbrl\n            total_pnl_usd = hist_attr[\"p&amp;l_usd\"].sum() + others_pnl_usd\n            total_rentab_usd = ((1+total_rentab_brl) / (1+usdbrl_variation))-1\n            # Nesse caso:\n            # O pnl total precisa ter o mesmo sinal da rentabilidade total, senao algo esta errado.\n            if total_pnl_usd * total_rentab_usd &lt; 0:\n                print(f\"Rentabilidade e pnl com sinais trocados. Conferir '{fund}' de '{previous_date}' ate '{recent_date}'.\")\n                print(f\"Obtendo pnl total a partir da rentabilidade e PL medio do periodo\")\n                fund_quotas = self.get_table(\"all_funds_quotas\").query(\"Fund ==@fund and Date &gt;= @previous_date and Date &lt;= @recent_date\")\n                total_pnl_usd = (fund_quotas[\"#\"] * fund_quotas[\"Quota\"]).mean() / average_usdbrl * total_rentab_usd\n\n        others_attr_brl = total_rentab_brl - hist_attr[\"attr_brl\"].sum()\n        others_attr_brl_curncy = total_rentab_brl - hist_attr[\"attr_brl_curncy\"].sum()\n        others_attr_usd = total_rentab_usd - hist_attr[\"attr_usd\"].sum()\n        others_attr_usd_curncy = total_rentab_usd - hist_attr[\"attr_usd_curncy\"].sum()\n\n        # Vamos inserir o row 'outros' com o que falta de pnl para atingir a rentabilidade do periodo\n        hist_attr.loc[\"outros_outros\"] = [others_pnl_brl, others_pnl_brl_curncy, others_pnl_usd, others_pnl_usd_curncy, others_attr_brl, others_attr_brl_curncy, others_attr_usd, others_attr_usd_curncy]\n\n        #hist_attr.loc[\"outros_outros\"] = [others_pnl_brl, others_pnl_brl_curncy, others_pnl_usd, others_pnl_usd_curncy]\n\n\n        # Vamos calcular o % de atribuicao, proporcional a variacao da cota no periodo\n        # Em reais\n        #hist_attr[\"attr_brl\"] = hist_attr.apply(lambda row: ((row[\"p&amp;l_brl\"])/abs(row[\"p&amp;l_brl\"])) * abs(row[\"p&amp;l_brl\"] * total_rentab_brl/total_pnl_brl), axis=1)\n        #hist_attr[\"attr_brl_curncy\"] = hist_attr.apply(lambda row: row[\"p&amp;l_brl_curncy\"] * total_rentab_brl/total_pnl_brl, axis=1)\n        # Em dolares\n        #hist_attr[\"attr_usd\"] = hist_attr.apply(lambda row: row[\"p&amp;l_usd\"] * total_rentab_usd/total_pnl_usd, axis=1)\n        #hist_attr[\"attr_usd_curncy\"] = hist_attr.apply(lambda row: row[\"p&amp;l_usd_curncy\"] * total_rentab_usd/total_pnl_usd, axis=1)\n\n\n        hist_attr[\"Start_Date\"] = previous_date\n        hist_attr[\"End_Date\"] = recent_date\n\n        return hist_attr        \n\n\n    def get_coinvestors_pos(self, date):\n\n        # Obtendo dados de cotistas na data informada ou data anterior mais proxima\n        hist_pos_cotistas = self.get_table(\"hist_pos_cotistas\").reset_index().query(\"Date &lt;= @date\")    \n\n        most_recent_date = hist_pos_cotistas[\"Date\"].max()\n        hist_pos_cotistas = hist_pos_cotistas.query(\"Date == @most_recent_date and Quantidade &gt; 0\")\n\n        ID_MANGA_I = 37\n        ID_MANGA_II = 507161\n\n        # Definindo IDs dos membros da familia\n        family_members_ids = {\n\n        100000009 : \"Thomas Ribas\",\n        100000007 : \"Marcelo Ribas Grabowsky\",\n        100000008 : \"Arthur Ribas Ferrer\",\n        100000006 : \"Eduardo Ribas Grabowsky\",\n        100000010 : \"Victoria Ribas Ferrer\",\n        100000003 : \"Carla Maria Flores Ribas\",\n        100000004 : \"Cristina Maria Flores Ribas\",\n        100000005 : \"Claudia Maria Fllores Ribas\",\n\n        }\n\n        luxor_companies_ids = {\n        100000021 : \"Jobi Holdings\",\n        100000050 : \"Luxor Investimentos\",\n        100000012 : \"Luxor Holding LLC\",\n        100000022 : \"Luxor Holdings\",\n        100000011 : \"Luxor Participa\u00e7\u00e3o S/A\",\n        1000000000 : \"Cerros\",\n        1000000029 : \"RB1\",\n        100000120 : \"Nepal Trust\", # Nao vamos contabilizar Nepal Trust, esta contabilizado para a claudia. \n                                   # No futuro, entrarao 2 novos ids no family members\n        }\n\n        # Definindo IDs dos funcionarios da Luxor\n        luxor_team_ids = {\n            100000083 : \"Thiago Estrella\",\n            100000094 : \"Daniel Baeta\",\n            100000101 : \"Bruna Louren\u00e7o\",\n            100000084 : \"Ivan Azevedo\",\n            100000091 : \"Carolina Schnarndorf\",\n            100000096 : \"Ana Luiza Tramontin\",\n            100000090 : \"Antonio Azevedo\",\n            100000089 : \"Maur\u00edcio Mesquita\", \n            100000087 : \"Aline Pezzi\",\n            10000050  : \"Thawana Arbues\",\n            10000043  : \"Sergio Barbosa\",\n        }\n\n        # Separando cada grupo num dataframe\n        family_members = hist_pos_cotistas.query(\"Id_Cotista.isin(@family_members_ids.keys())\")\n\n        companies = hist_pos_cotistas.query(\"Id_Cotista.isin(@luxor_companies_ids.keys())\")\n        luxor_team = hist_pos_cotistas.query(\"Id_Cotista.isin(@luxor_team_ids.keys())\")\n\n        # Encontrando o nmr de ids em cada grupo\n        numb_of_ids = len(hist_pos_cotistas[\"Id_Cotista\"].unique())\n        numb_of_family_ids = len(family_members[\"Id_Cotista\"].unique())\n        numb_of_companies_ids = len(companies)\n        numb_of_team_ids = len(luxor_team)\n        numb_of_coinvestors_ids = numb_of_ids - numb_of_family_ids - numb_of_companies_ids - numb_of_team_ids\n\n        # Encontrando totais investidos de cada grupo.\n        # Luxor Team -&gt; Todos investem pelo Manga I\n        # Luxor Family -&gt; Investem pelo Manga I e Manga II. Nepal Trust tambem entra aqui, porem nao eh contabilizado como um cotista a mais (considera-se cotista como sendo a claudia)\n\n        # Obtendo a cota do fundo nessa data\n        #fund_quota = self.get_price(\"mangalarga fic fia\", px_date=date) #TODO -- Verificar se o manga ii fic altera o resultado aqui.\n        manga_i_quota = self.get_price(\"mangalarga fic fia\", px_date=date)\n        manga_ii_quota = self.get_price(\"mangalarga ii fic fia\", px_date=date) \n\n        # Obtendo a posicao total de cada parte na data informada\n\n        total_nepal_trust_manga_i = companies.query(\"Id_Carteira == @ID_MANGA_I and Id_Cotista == 100000120\")[\"Quantidade\"].sum() * manga_i_quota\n        total_family_members_manga_i  = family_members.query(\"Id_Carteira == @ID_MANGA_I\")[\"Quantidade\"].sum() * manga_i_quota + total_nepal_trust_manga_i\n        total_companies_manga_i = companies.query(\"Id_Carteira == @ID_MANGA_I\")[\"Quantidade\"].sum() * manga_i_quota - total_nepal_trust_manga_i\n\n        total_nepal_trust_manga_ii = companies.query(\"Id_Carteira == @ID_MANGA_II and Id_Cotista == 100000120\")[\"Quantidade\"].sum() * manga_ii_quota\n        total_family_members_manga_ii = family_members.query(\"Id_Carteira == @ID_MANGA_II\")[\"Quantidade\"].sum() * manga_ii_quota + total_nepal_trust_manga_ii\n        total_companies_manga_ii = companies.query(\"Id_Carteira == @ID_MANGA_II\")[\"Quantidade\"].sum() * manga_ii_quota - total_nepal_trust_manga_ii\n\n        total_nepal_trust = total_nepal_trust_manga_i + total_nepal_trust_manga_ii\n        total_family_members = total_family_members_manga_i + total_family_members_manga_ii\n        total_companies = total_companies_manga_i + total_companies_manga_ii\n\n        total_luxor_team = luxor_team.query(\"Id_Carteira == @ID_MANGA_I\")[\"Quantidade\"].sum() * manga_i_quota\n\n        total_coinvestors = hist_pos_cotistas.query(\"Id_Carteira == @ID_MANGA_I\")[\"Quantidade\"].sum() * manga_i_quota - total_family_members_manga_i - total_luxor_team - total_companies_manga_i\n\n        print(f\"family manga i: {total_family_members_manga_i}, family manga ii: {total_family_members_manga_ii}, companies ii: {total_companies_manga_ii}\")\n\n        # Se Nepal Trust estiver sendo considerado como luxor company, precisamos subtrair do total de membros antes de retornar (ja contabilizado no luxor family)\n        if \"Nepal Trust\" in luxor_companies_ids.values():\n            numb_of_companies_ids = numb_of_companies_ids -1\n\n        return {\"family\": [numb_of_family_ids, total_family_members] , \n                \"luxor_team\" : [numb_of_team_ids, total_luxor_team],\n                \"luxor_companies\" : [numb_of_companies_ids, total_companies],\n                \"coinvestors\" : [numb_of_coinvestors_ids, total_coinvestors]}\n\n\n    def calculate_drawdown(self, tickers, previous_date, recent_date, currency=\"local\"):\n        \"\"\"Calcula o drawdown para um ativo ou um conjunto de ativos em cada data do perido fornecido.\n\n        Args:\n            tickers (str|list|set): ticker ou lista de tickers\n            previous_date (dt.date): _description_\n            recent_date (dt.date):\n\n        Returns:\n            pd.DataFrame: Seguinte DataFrame usando Date como index -&gt; [[Asset, Period_Change, Previous_Peak, Drawdowns]]\n        \"\"\"\n\n        df = self.get_prices(tickers=tickers, previous_date=previous_date, recent_date=recent_date, currency=currency).set_index(\"Date\")\n\n        df[\"Period_Change\"] = df.groupby(\"Asset\").pct_change().fillna(0) + 1\n        df[\"Period_Change\"] = df[[\"Asset\", \"Period_Change\"]].groupby(\"Asset\").cumprod()\n        df[\"Previous_Peak\"] = df[[\"Asset\", \"Period_Change\"]].groupby(\"Asset\").cummax()\n        df[\"Drawdowns\"] = (df[\"Period_Change\"] - df[\"Previous_Peak\"])/df[\"Previous_Peak\"]\n\n        return df\n\n\n    def calculate_tracking_error(self, returns_ticker, benchmark_ticker, previous_date=dt.date.today()-dt.timedelta(days=365*2), recent_date=dt.date.today()-dt.timedelta(days=1), period=None, holiday_location=\"all\"):\n        \"\"\" Calcula o tracking error de uma serie de retorno com relacao ao benchmark.\n\n        Args:\n            returns_ticker (str): ticker da serie de retorno que sera testada\n            benchmark_ticker (str): ticker da serie de retorno que sera usada para comparacao\n            previous_date (dt.date): data de inicio do teste\n            recent_date (dt.date): data de fim do teste\n        Returns:\n            _type_: _description_\n        \"\"\"\n\n        if period is not None:\n            previous_date = self.get_start_period_dt(recent_date, period=period, holiday_location=holiday_location)\n\n\n        returns = self.get_prices([returns_ticker, benchmark_ticker], previous_date=previous_date, recent_date=recent_date, currency=\"usd\")\n\n        returns = returns.set_index(\"Date\").pivot(columns=\"Asset\").dropna(how=\"any\")\n        returns = returns.pct_change().dropna()\n\n        returns.columns = [returns_ticker, benchmark_ticker]\n\n        n_periods = len(returns)\n        returns[\"Diff\"] = (returns[returns_ticker] - returns[benchmark_ticker])**2\n\n        tracking_error = 0\n        if (n_periods - 1) &gt; 0:\n            tracking_error = (returns[\"Diff\"].sum()/(n_periods - 1)) ** (1/2)\n\n\n        return tracking_error\n\n\n    def xirr(self, cashflows, dates):\n        \"\"\"\n            Calcula o XIRR para uma serie de cash flows em datas especificas.\n\n            :cashflows (numpy.array of float): cash flows (positive para entrada, negativo para saida)\n            :dates (numpy.array of datetime64): datas correspondentes aos cash flows\n            :return: XIRR\n        \"\"\"\n        # Garantindo que dates eh um numpy array de datetime64\n        if not isinstance(dates, np.ndarray) or dates.dtype.type is not np.datetime64:\n            dates = np.array(pd.to_datetime(dates))\n\n        t0 = dates[0]  # Reference date\n        years = np.array([(date - t0).astype('timedelta64[D]').astype(int) / 365.25 for date in dates])\n\n        def xnpv(rate):\n            # Limiting the rate to avoid overflow and division by zero\n            rate = max(min(rate, 1), -0.9999)\n            return sum([cf / (1 + rate) ** yr for cf, yr in zip(cashflows, years)])\n\n        def xnpv_prime(rate):\n            rate = max(min(rate, 1), -0.9999)\n            return sum([-yr * cf / (1 + rate) ** (yr + 1) for cf, yr in zip(cashflows, years)])\n\n        try:\n            # Using a conservative initial guess\n            initial_guess = 0.1\n            return newton(xnpv, x0=initial_guess, fprime=xnpv_prime)\n        except (RuntimeError, OverflowError):\n            # Return NaN if the calculation fails\n            return np.nan\n\n\n    def calculate_price_correlation(self, assets_data:dict, sample_frequency=\"monthly\", period=\"12m\", ref_date=dt.date.today()):\n        \"\"\"\n        Calculate the correlation between assets in the columns of 'asset_prices'. The correlation is calculated using the formula:\n        corr = (1 + r1*r2 + r1*r2*corr) / sqrt(1+r1**2) * sqrt(1+r2**2)\n        where r1 and r2 are the returns of the assets and corr is the correlation between the returns.\n        The correlation is calculated using the formula above and the returns are calculated using the formula:\n        r = (price[t] - price[t-1]) / price[t-1]\n        where t is the time period.\n        The returns are calculated using the sample frequency and the lookback in months.\n        assets_data: dict mapeando ticker para {Name:str, Key:str e Location:str(bz ou us)}\n        sample_frequency: str, default \"monthly\". Frequencia de amostragem dos precos. Pode ser \"daily\", \"weekly\" ou \"monthly\"\n        period: str, default '12m'. Periodo de calculo da correlacao. Deve ser 'ytd' ou o numero de meses seguido por 'm'\n        \"\"\"\n        sample_frequency = sample_frequency.lower()\n        period = period.lower()\n        asset_prices = self.get_prices(tickers=set(assets_data.keys()) - {\"usdbrl curncy\"}, recent_date=ref_date,\n                                        period=period, currency=\"usd\", usdbrl_ticker=\"usdbrl curncy\")\n        usdbrl_prices = self.get_prices(tickers={\"usdbrl curncy\"}, period=period)\n        asset_prices = pd.concat([asset_prices, usdbrl_prices])\n\n        asset_prices = asset_prices.pivot(index=\"Date\", columns=\"Asset\", values=\"Last_Price\").ffill()\n\n\n        frequencies = {\"daily\":\"B\", \"weekly\":\"W\", \"monthly\":\"ME\"}\n        asset_prices = asset_prices.resample(frequencies[sample_frequency]).last()\n        #returns = asset_prices.pct_change(lookback_in_months)\n        #returns = returns.dropna()\n        correlations = asset_prices.corr()\n        correlations.index.name = \"Asset_Corr\"\n        correlations = correlations.reset_index()\n\n        value_vars = correlations.columns\n        correlations = correlations.melt(id_vars=[\"Asset_Corr\"], value_vars=value_vars, value_name=\"Correlation\", var_name=\"Asset\").sort_values(by=\"Correlation\").reset_index(drop=True)\n        correlations[\"Key_Asset_Corr\"] = correlations[\"Asset_Corr\"].apply(lambda x: assets_data[x][\"Key\"])\n        correlations[\"Key_Asset\"] = correlations[\"Asset\"].apply(lambda x: assets_data[x][\"Key\"])\n        correlations.index.name=\"Order\"\n        correlations = correlations.reset_index()\n        correlations[\"Comp_Key\"] = correlations[\"Key_Asset_Corr\"]+\"_\"+correlations[\"Key_Asset\"]\n        correlations[\"Frequency\"] = sample_frequency.replace(\"ly\",\"\").title()\n        period = period if int(period.replace(\"m\",\"\").replace(\"ytd\",\"0\")) &lt; 12 else str(int(period.replace(\"m\",\"\"))/12).replace(\".0\",\"\")+\"y\"\n        correlations[\"Period\"] = period.upper()\n\n        return correlations\n\n\n    def calculate_volatility(self, tickers, min_rolling_period_months, analysis_period_days, currency=\"local\", sample_frequency=\"monthly\", usdbrl_ticker=\"usdbrl curncy\"):\n\n        period = min_rolling_period_months + analysis_period_days//60 + 3\n        prices = self.get_prices(tickers=tickers, period=str(period)+\"m\", currency=currency, usdbrl_ticker=usdbrl_ticker)\n\n        year_size = 260\n\n        if sample_frequency == \"monthly\":\n            analysis_period_days = analysis_period_days//30 # numero de meses para rolling\n            prices = prices.groupby(\"Asset\").resample(\"ME\", on=\"Date\").last().reset_index(level=1).reset_index(drop=True).set_index(\"Date\")\n            year_size = 12\n\n        elif sample_frequency == 'weekly':\n            analysis_period_days = analysis_period_days//7 # numero de semanas para rolling\n            prices = prices.groupby(\"Asset\").resample(\"W\", on=\"Date\").last().reset_index(level=1).reset_index(drop=True).set_index(\"Date\")\n            year_size = 52\n\n        elif sample_frequency == \"daily\":\n            prices[\"Yesterday\"] = prices[\"Date\"] - dt.timedelta(days=1)\n            prices[\"Equals_To_Yesterday\"] = (prices[\"Last_Price\"] == prices[\"Last_Price\"].shift(1)) &amp; (prices[\"Yesterday\"] == prices[\"Date\"].shift(1))\n            prices = prices.query(\"~Equals_To_Yesterday\")[[\"Date\", \"Asset\", \"Last_Price\"]].copy().set_index(\"Date\")\n\n\n        prices[\"Previous_Price\"] = prices.groupby(\"Asset\")[\"Last_Price\"].shift(1)\n        prices[\"Return\"] =np.log((prices[\"Last_Price\"]/prices[\"Previous_Price\"])).fillna(0)\n\n        prices = prices[[\"Asset\", \"Return\"]].groupby(\"Asset\").rolling(window=analysis_period_days).std()*np.sqrt(year_size)*100\n\n        prices = prices.reset_index().dropna().rename(columns={\"Asset\":\"Ticker\", \"Return\":\"Volatility\"})\n\n        return prices\n\n\n    def get_asset_name(self, ticker):\n\n        assets = self.get_table(\"assets\")\n        name = assets.query(\"Ticker == @ticker\")[\"Name\"].squeeze()\n        return name\n\n\n    def list_blob_files(self, container, sub_dir, ends_with=None):\n        \"\"\"\n            Lista todos os arquivos dentro de um diretorio no blob storage.\n        \"\"\"\n\n        connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n        blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n\n        # Vamos listar os arquivos do diretorio\n        blob_files = blob_service_client\\\n                    .get_container_client(container)\\\n                    .list_blobs(name_starts_with=sub_dir)\n\n        if ends_with is not None:\n            return [blob_file.name for blob_file in blob_files if blob_file.name.endswith(ends_with)]\n        return [blob_file.name for blob_file in blob_files]\n\n\n    def simulate_portfolio_performance(self, portfolio: dict, portfolio_date: dt.date, adm_fee: float, performance_fee: float = 0):\n            \"\"\"\n            Simula o desempenho de um portf\u00f3lio com base em um dicion\u00e1rio de ativos e suas aloca\u00e7\u00f5es iniciais.\n\n            Par\u00e2metros:\n            - portfolio: dicion\u00e1rio com os ativos e suas aloca\u00e7\u00f5es iniciais. Dicion\u00e1rio deve seguir o formato:\\n\n                {\n                    \"ticker1\": peso1,\n                    \"ticker2\": peso2,\n                    ...\n                    \"tickerN\": pesoN\n                }, onde os pesos devem somar 1.\n            - portfolio_date: data inicial do portf\u00f3lio.\n            - adm_fee: taxa de administra\u00e7\u00e3o %.a.a\n            - performance_fee: taxa de performance (opcional).\n\n            Retorna:\n            - DataFrame com o fator de corre\u00e7\u00e3o da cota para cada dia a partir da data inicial do portfolio.\n            \"\"\"\n\n            # Formatar os tickers para minusculas\n            portfolio = {k.lower(): v for k, v in portfolio.items()}\n\n            initial_portfolio = {\n                \"date\" : portfolio_date,\n                \"assets\" : portfolio,\n            }\n\n            # Criando dataframe com as colunas  Date|Ticker|Weight|\n            positions = pd.DataFrame(initial_portfolio[\"assets\"].items(), columns=[\"Ticker\", \"Weight\"])\n            positions[\"Date\"] = initial_portfolio[\"date\"]\n            positions[\"Returns\"] = 0\n            positions[\"Daily_Attribution\"] = 0.0\n            positions[\"Daily_Portfolio_Return\"] = 0.0\n\n            tickers = positions[\"Ticker\"].tolist()\n\n            max_date = portfolio_date\n\n            while max_date &lt; dt.date.today():\n                # Adicionando 1 dia\n                new_date = max_date + dt.timedelta(days=1)\n                daily_returns = self.get_pct_changes(tickers=tickers, previous_date=max_date, recent_date=new_date, currency=\"usd\")\n\n                daily_returns.index.name = \"Ticker\"\n\n                new_day = positions.query(\"Date == @max_date\").copy().set_index(\"Ticker\")\n                new_day[\"Returns\"] = daily_returns\n                new_day = new_day.reset_index()\n                # Ajusta pesos considerando a atribuicao de retorno do dia anterior\n                new_day[\"Weight\"] = new_day[\"Weight\"] + new_day[\"Daily_Attribution\"]\n                new_day[\"Date\"]  = new_date\n\n                new_day[\"Daily_Attribution\"] = new_day[\"Weight\"] * new_day[\"Returns\"]\n                new_day[\"Daily_Portfolio_Return\"] = new_day[\"Daily_Attribution\"].sum()\n\n                positions = pd.concat([positions, new_day])\n                max_date = new_date\n\n            positions[\"Date\"] = pd.to_datetime(positions[\"Date\"])\n            #positions.to_excel(\"positions_tci.xlsx\")\n            daily_returns = positions[[\"Date\", \"Daily_Portfolio_Return\"]].groupby(\"Date\").last()\n\n            daily_returns[\"Acc_Returns\"] = (1 + daily_returns[\"Daily_Portfolio_Return\"]).cumprod()\n            daily_returns[\"Acc_Returns_Adjusted_by_Taxes\"] = np.where(daily_returns[\"Acc_Returns\"] &gt; 1,\n                                                            ((daily_returns[\"Acc_Returns\"] -1 ) * (1-performance_fee) + 1) - adm_fee/12,\n                                                            daily_returns[\"Acc_Returns\"] - adm_fee/12\n                                                            )\n\n            return daily_returns.reset_index()[[\"Date\", \"Acc_Returns_Adjusted_by_Taxes\"]].rename(columns={\"Acc_Returns_Adjusted_by_Taxes\": \"Factor\"})\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.__get_positions_and_movements","title":"<code>__get_positions_and_movements(fund_name, tickers=None, recent_date=dt.date.today(), previous_date=dt.date.today() - dt.timedelta(days=1), period=None, holiday_location='all', currency='usd', usdbrl_ticker='bmfxclco curncy')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fund_name</code> <code>str</code> required <code>tickers</code> <code>str | list | set</code> <p>Defaults to None.</p> <code>None</code> <code>recent_date</code> <code>date</code> <p>Defaults to dt.date.today().</p> <code>today()</code> <code>previous_date</code> <code>date</code> <p>description. Defaults to dt.date.today()-dt.timedelta(days=1).</p> <code>today() - timedelta(days=1)</code> <code>period</code> <code>str</code> <p>mtd|ytd|3m|6m|...|nm. Defaults to None.</p> <code>None</code> <code>holiday_location</code> <code>str</code> <p>any|bz|us|all. Defaults to \"all\".</p> <code>'all'</code> <p>Returns:</p> Type Description <p>pandas.DataFrame:</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def __get_positions_and_movements(self, fund_name, tickers=None,\n        recent_date=dt.date.today(), previous_date=dt.date.today()-dt.timedelta(days=1),\n        period=None, holiday_location=\"all\", currency=\"usd\", usdbrl_ticker=\"bmfxclco curncy\"):\n    \"\"\"\n    Args:\n        fund_name (str): \n        tickers (str|list|set, optional): Defaults to None.\n        recent_date (datetime.date, optional): Defaults to dt.date.today().\n        previous_date (datetime.date, optional): _description_. Defaults to dt.date.today()-dt.timedelta(days=1).\n        period (str, optional): mtd|ytd|3m|6m|...|nm. Defaults to None.\n        holiday_location (str, optional): any|bz|us|all. Defaults to \"all\".\n\n    Returns:\n        pandas.DataFrame: \n    \"\"\"\n\n    assert(currency in [\"usd\", \"brl\"])\n\n    if period is not None:\n        previous_date = self.get_start_period_dt(recent_date, period, holiday_location=holiday_location, force_bday=True)\n\n    # Garantindo que a data inicial sera sempre dia util, assim eh certo de que havera preco de acao nessa data\n    previous_date = self.get_bday_offset(previous_date, offset=0, location=\"any\")\n    lipi_manga_incorp_date = self.lipi_manga_incorp_date\n    assets = self.get_table(\"assets\")\n\n\n    # Filtrando somente os trades desejados\n    # lembrando que lipi precisa abrir em fund_A e manga_master (quando antes da incorporacao)\n    # ha ainda um ajuste no lipi, que na data da incorporacao compra as posicoes do Manga (nao podemos tirar essas boletas) \n    trades = (self.get_table(\"trades\")[[\"Date\", \"Fund\", \"Asset\", \"Ticker\", \"#\", \"Price\", \"Total\", \"Op Type\"]]\n            .rename(columns={\"#\":\"Delta_Shares\", \"Total\" : \"Trade_Amount\", \"Op Type\":\"Op_Type\"})\n            .query(\"(Date &gt; @previous_date and Date &lt;= @recent_date) and Op_Type.isin(['a vista', 'ndf', 'termo', 'apl/resg fundos', 'vc capital call', 'vc'])\"))\n\n    fund_filter = [fund_name]\n    if fund_name == 'lipizzaner':\n        fund_filter += [\"fund a\"]\n        if previous_date &lt; lipi_manga_incorp_date:\n            fund_filter += [\"mangalarga master\"]\n    trades = trades.query(\"Fund in @fund_filter\")\n\n    trades = trades.query(\"Fund != 'lipizzaner' or Date != @lipi_manga_incorp_date\").rename(columns={\"Price\" : \"Exec_Price\"}).copy()\n    trades[\"Asset_ID\"] = trades[\"Asset\"] + \"_\" + trades[\"Ticker\"]\n\n\n    # Obtendo posicoes na data de inicio, e criando boletas de inicializacao\n    initial_positions = []\n\n    for f in fund_filter:\n        fund_initial_pos = pd.DataFrame(self.get_positions(f, date=previous_date).items(), columns=[\"Asset_ID\", \"Delta_Shares\"])\n        if len(fund_initial_pos) == 0: continue\n        fund_initial_pos[\"Fund\"] = f\n        fund_initial_pos[[\"Asset\", \"Ticker\"]] = tuple(fund_initial_pos[\"Asset_ID\"].str.split(\"_\"))\n\n        initial_positions.append(fund_initial_pos)\n\n\n    if len(initial_positions) &gt; 0:\n        initial_positions = pd.concat(initial_positions)\n\n        # Adicionando coluna de tipo, pois sera necessario tratar quando type == a\u00e7\u00f5es_bdr\n        initial_positions = pd.merge(initial_positions, assets[[\"Key\", \"Type\"]], left_on=\"Asset_ID\", right_on=\"Key\")\n\n        initial_positions[\"Price_Key\"] = initial_positions[\"Asset_ID\"].apply(lambda x: self.get_price_key(x))\n        initial_prices = (self.get_prices(tickers=list(initial_positions[\"Price_Key\"]), previous_date=previous_date,\n                                          recent_date=previous_date, force_continuous_date_range=True)\n                                    .rename(columns={\"Asset\": \"Price_Key\", \"Last_Price\":\"Exec_Price\"})[[\"Price_Key\", \"Exec_Price\"]])\n\n        initial_positions = pd.merge(initial_positions, initial_prices, on=[\"Price_Key\"])\n\n        # Corrigindo pre\u00e7o de entrada das BDRs\n        initial_positions[\"Exec_Price\"] = initial_positions.apply(lambda row: row[\"Exec_Price\"] if row[\"Type\"] != 'a\u00e7\u00f5es_bdr' else self.get_bdr_adj_price(row[\"Asset_ID\"], date=previous_date), axis=1)\n\n        initial_positions[\"Trade_Amount\"] = initial_positions[\"Delta_Shares\"] * initial_positions[\"Exec_Price\"] * (-1)\n        initial_positions[\"Date\"] = previous_date\n\n    else:\n        initial_positions = pd.DataFrame() # tratando caso inicial, onde nao ha posicoes no inicio\n\n    trades = pd.concat([initial_positions, trades])[[\"Date\", \"Asset_ID\", \"Delta_Shares\", \"Exec_Price\", \"Trade_Amount\"]]\n\n    trades[\"Date\"] = pd.to_datetime(trades[\"Date\"])\n\n    # Adicionando coluna pra marcar se eh compra ou venda. Sera usado na agregacao logo mais.\n    #trades[\"Trade_Side\"] = np.where(trades[\"Delta_Shares\"] &lt; 0, \"sell\", \"buy\")\n\n    # Ordenando boletas, por padrao vamos sempre comprar antes de vender.\n    trades = self.normalize_trades(trades, currency=currency, usdbrl_ticker=usdbrl_ticker).sort_values(by=[\"Date\"], kind=\"stable\")\n    # Apos normalizar os trades, eh necessario recalcular o preco da boleta\n\n    # Agrupando boletas de forma a ter apenas uma boleta por dia por codigo de ativo\n    #trades = trades.groupby([\"Date\", \"Asset_ID\"]).agg({\"Delta_Shares\":\"sum\", \"Exec_Price\":\"last\", \"Trade_Amount\":\"sum\"}).reset_index()\n    # Precisamos calcular novamente o preco correto de execucao do trade\n    #trades[\"Exec_Price\"] = abs(trades[\"Trade_Amount\"])/abs(trades[\"Delta_Shares\"])\n\n    # Calculando posicao ao final de cada trade, preco medio e custo da posicao\n    trades[\"Open_Quantity\"] = trades.groupby([\"Asset_ID\"])[\"Delta_Shares\"].cumsum()\n    trades[\"Open_Quantity\"] = np.where(abs(trades[\"Open_Quantity\"]) &lt; 0.00001, 0, trades[\"Open_Quantity\"])\n    trades = self.calculate_average_price(trades)\n    trades[\"Average_Price\"] = trades.groupby([\"Asset_ID\"])[\"Average_Price\"].ffill()#fillna(method=\"ffill\")\n\n    # Vamos agregar novamente, agora para tirar o 'Trade_Side', passando a ter uma linha apenas por data\n    #trades = trades.groupby([\"Date\", \"Asset_ID\"]).agg({\"Delta_Shares\":\"sum\", \"Exec_Price\":\"last\", \"Trade_Amount\":\"sum\",\n    #                                                    \"Open_Quantity\":\"last\", \"Average_Price\" : \"last\", \"Trade_Side\":\"last\"}).reset_index()\n    # Preco de execucao, necessita ser recalculado\n    #trades[\"Exec_Price\"] = abs(trades[\"Trade_Amount\"])/abs(trades[\"Delta_Shares\"])\n\n    trades[\"Open_Position_Cost\"] = (trades[\"Open_Quantity\"] * trades[\"Average_Price\"])\n\n    return trades\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.__hist_prices_intraday_extensor","title":"<code>__hist_prices_intraday_extensor(hist_prices)</code>","text":"<p>Completa a tabela de precos historicos com o preco intraday mais recente.</p> <p>Parameters:</p> Name Type Description Default <code>hist_prices</code> <code>DataFrame</code> <p>dataframe de precos historicos</p> required Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def __hist_prices_intraday_extensor(self, hist_prices):\n    \"\"\" Completa a tabela de precos historicos com o preco intraday mais recente.\n\n    Args:\n        hist_prices (pd.DataFrame): dataframe de precos historicos\n    \"\"\"\n\n    # Verificar cada Asset unico existente na tabela\n    assets = hist_prices[\"Asset\"].unique()\n    today = dt.date.today()\n    updated_dfs = []\n    # vamos fazer cada ativo por vez\n    for asset in assets:\n        # vamos pegar a tabela de precos do ativo\n        asset_prices = hist_prices.query(\"Asset == @asset\")\n        # Vamos pegar a data mais recente\n        last_date = asset_prices[\"Date\"].max().date()\n        if last_date == today:\n            last_price = self.get_price(asset)\n            asset_prices.loc[asset_prices[\"Date\"] == last_date, \"Last_Price\"] = last_price\n            updated_dfs.append(asset_prices.copy())\n            continue\n        # vamos pegar o ultimo preco presente no df\n        last_price = asset_prices.query(\"Date == @last_date\")[\"Last_Price\"].squeeze()\n        # vamos pegar a variacao ate o momento mais recente\n        query_asset = asset\n        if asset == 'bmfxclco curncy':\n            query_asset = 'usdbrl curncy'\n        variation = 0\n        if today &gt; last_date:\n            variation = self.get_pct_change(query_asset, recent_date=today, previous_date=last_date)\n        # vamos ajustar o preco com a variacao\n        last_price = last_price * (1 + variation)\n        # vamos colocar na base na data de hoje\n        updated_dfs.append(pd.concat([asset_prices, \n                    pd.DataFrame({\"Date\":[today],\n                                  \"Asset\":[asset], \"Last_Price\":[last_price]})]))\n\n    updated_df = pd.concat(updated_dfs)\n    updated_df[\"Date\"] = pd.to_datetime(updated_df[\"Date\"])\n\n    return updated_df\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.__init__","title":"<code>__init__(update_mode='optimized', is_develop_mode=False, tables_path=None, blob_directory='enriched/parquet')</code>","text":"update_mode <p>'standard' - Carrega todas as tabelas disponiveis 'optimized' - Carrega apenas as tabelas utilizadas sob demanda</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def __init__(self, update_mode=\"optimized\", is_develop_mode=False, tables_path=None, \n             blob_directory='enriched/parquet'):\n\n    \"\"\"\n        update_mode: \n            'standard' - Carrega todas as tabelas disponiveis\n            'optimized' - Carrega apenas as tabelas utilizadas sob demanda\n    \"\"\"\n\n    self.modified_tables = []\n    self.is_develop_mode = is_develop_mode\n    self.blob_directory = blob_directory\n\n    self.tables_path = tables_path\n    if tables_path is None:\n        self.tables_path = self.__set_tables_path()\n\n    self.tables_in_use = {}\n    self.asset_last_prices = {}\n    self.price_cache = {}  # otimizacao da consulta de preco\n    self.price_tables_loaded = {}\n\n\n    self.lipi_manga_incorp_date = dt.date(2022,12,9)\n\n\n    self.update_modes_name = {\"standard\" : 0, \"optimized\" : 1}\n    self.update_mode = self.update_modes_name[update_mode]\n    self.update() # Nessa 1\u00b0 exec. vai inicializar os dicionarios acima\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.__is_table_modified","title":"<code>__is_table_modified(table_name)</code>","text":"<p>Retorna 'True' ou 'False' informando se a tabela informada em 'table_name' foi criada ou modificada.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>nome tabela</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True se foi criada ou modificada</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def __is_table_modified(self, table_name):\n    \"\"\" Retorna 'True' ou 'False' informando se a tabela informada em 'table_name' foi criada ou modificada.\n\n    Args:\n        table_name (str): nome tabela\n\n    Returns:\n        bool: True se foi criada ou modificada\n    \"\"\"\n\n    if table_name not in self.tables_in_use:\n        return True\n\n    try:\n        file_path = self.tables_in_use[table_name][\"table_path\"]\n        file_last_update = os.path.getmtime(file_path)\n\n        return file_last_update &gt; self.tables_in_use[table_name][\"update_time\"]\n\n    except:\n        logger.critical(f\"Arquivo &lt;{file_path}&gt; n\u00e3o encontrado.\")\n\n    return False\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.calculate_adjusted_avg_price","title":"<code>calculate_adjusted_avg_price(df, groupby_column='Ticker')</code>","text":"<p>Recebe um DataFrame contendo as colunas Date, Ticker, Delta_Shares e Trade_Amount.     Trade_Amount sera resultado do preco_medio anterior (ja calculado na transacao)      multiplicado pela quantidade operada. ATENCAO pois na venda nao tem a informacao     do valor pelo qual o ativo foi vendido.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <p>Returns:</p> Type Description <p>pandas.DataFrame: Adiciona as colunas Cumulative_Shares, Open_Position_Cost e Average_Price</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def calculate_adjusted_avg_price(self, df, groupby_column=\"Ticker\"):\n    \"\"\"Recebe um DataFrame contendo as colunas Date, Ticker, Delta_Shares e Trade_Amount.\n        Trade_Amount sera resultado do preco_medio anterior (ja calculado na transacao) \n        multiplicado pela quantidade operada. ATENCAO pois na venda nao tem a informacao\n        do valor pelo qual o ativo foi vendido.\n\n    Args:\n        df (pandas.DataFrame):\n\n    Returns:\n        pandas.DataFrame: Adiciona as colunas Cumulative_Shares, Open_Position_Cost e Average_Price\n    \"\"\"\n    # Sort the DataFrame by Ticker and Date for sequential processing\n    df_sorted = df.sort_values([groupby_column, \"Date\", \"Trade_Side\"]) # sort by trade_side (para processar sempre a compra primeiro)\n    df_columns = list(df.columns)\n\n    # Calculate cumulative shares and trade value for each ticker\n    df_sorted['Cumulative_Shares'] = df_sorted.groupby(groupby_column)['Delta_Shares'].cumsum()\n\n    df_sorted = self.calculate_average_price(df_sorted)\n    df_sorted[\"Average_Price\"] = df_sorted.groupby([\"Date\", \"Ticker\", \"Trade_Side\"])[\"Average_Price\"].ffill()#fillna(method=\"ffill\")\n\n    df_sorted[\"Open_Position_Cost\"] = -abs(df_sorted[\"Cumulative_Shares\"] * df_sorted[\"Average_Price\"])\n\n    return df_sorted[df_columns+[\"Cumulative_Shares\", \"Open_Position_Cost\", \"Average_Price\"]]\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.calculate_attribution","title":"<code>calculate_attribution(fund, previous_date, recent_date, fund_currency)</code>","text":"<p>Calcula o attribution do fundo informado para qualquer periodo.</p> <p>Parameters:</p> Name Type Description Default <code>fund</code> <code>str</code> <p>nome do fundo (lipizzaner, fund a, fund b, ...)</p> required <code>previous_date</code> <code>date</code> <p>data inicial</p> required <code>recent_date</code> <code>date</code> <p>data final</p> required <code>fund_currency</code> <code>str</code> <p>'brl' ou 'usd</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame: Dataframe contendo o attribution (% e pnl) por ativo no periodo.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def calculate_attribution(self, fund, previous_date, recent_date, fund_currency):\n    \"\"\" Calcula o attribution do fundo informado para qualquer periodo.\n\n    Args:\n        fund (str): nome do fundo (lipizzaner, fund a, fund b, ...)\n        previous_date (datetime.date): data inicial\n        recent_date (datetime.date): data final\n        fund_currency (str): 'brl' ou 'usd\n\n    Returns:\n        pandas.DataFrame: Dataframe contendo o attribution (% e pnl) por ativo no periodo.\n    \"\"\"\n\n    # Acumulando os p&amp;l's do periodo\n    hist_attr = (self.get_table(\"hist_attr_base\", index=True, index_name=\"Key\")\n                     .query(\"Fund == @fund and Start_Date&gt;= @previous_date and End_Date &lt;= @recent_date\")\n                     )\n    # Corrigindo datas de inicio e fim, para datas validas considerando as janelas de attribution calculadas.\n    previous_date = hist_attr[\"Start_Date\"].min().date()\n    recent_date = hist_attr[\"End_Date\"].max().date()\n\n    hist_attr = hist_attr[[\"p&amp;l_brl\",\"p&amp;l_brl_curncy\",\"p&amp;l_usd\",\"p&amp;l_usd_curncy\",  \"attr_brl\",  \"attr_brl_curncy\", \"attr_usd\", \"attr_usd_curncy\"]].groupby(\"Key\").sum()\n\n    #hist_attr = hist_attr[[\"p&amp;l_brl\",\"p&amp;l_brl_curncy\",\"p&amp;l_usd\",\"p&amp;l_usd_curncy\"]].groupby(\"Key\").sum()\n\n\n    # Vamos encontrar o total de p&amp;l do periodo e a rentabilidade no periodo, em reais e em dolares\n    end_usdbrl = self.get_price(\"bmfxclco curncy\", recent_date)\n    average_usdbrl = self.get_table(\"hist_px_last\").query(\"Asset == 'usdbrl curncy' and Date &gt;= @previous_date and Date &lt;= @recent_date\")[\"Last_Price\"].mean()\n    usdbrl_variation = self.get_pct_change(\"bmfxclco curncy\", previous_date=previous_date, recent_date=recent_date)\n\n    # Calculando pnl e rentab totais (ainda nao sabemos em qual moeda esta)\n    total_pnl = self.get_fund_pnl(fund, previous_date, recent_date)\n    total_rentab = self.get_pct_change(fund, previous_date=previous_date, recent_date=recent_date)\n    others_pnl = total_pnl - hist_attr[\"p&amp;l_\"+fund_currency].sum()\n    others_pnl_curncy = total_pnl - hist_attr[\"p&amp;l_\"+fund_currency+\"_curncy\"].sum()\n\n    # Inicializando todas as possibilidades\n    total_pnl_brl, total_pnl_usd, total_rentab_brl, total_rentab_usd = total_pnl, total_pnl, total_rentab, total_rentab\n    others_pnl_brl, others_pnl_usd = others_pnl, others_pnl\n    others_pnl_brl_curncy, others_pnl_usd_curncy = others_pnl_curncy, others_pnl_curncy\n\n    # Encontrando moeda e corrigindo valores.\n    if fund_currency == \"usd\":\n        others_pnl_brl = others_pnl_brl * end_usdbrl\n        others_pnl_brl_curncy = others_pnl_brl_curncy * end_usdbrl\n        total_pnl_brl = hist_attr[\"p&amp;l_brl\"].sum() + others_pnl_brl\n        total_rentab_brl = (1+total_rentab_usd)*(1+usdbrl_variation)-1\n        # Nesse caso:\n        # O pnl total precisa ter o mesmo sinal da rentabilidade total, senao algo esta errado.\n        if total_pnl_brl * total_rentab_brl &lt; 0:\n            print(f\"Rentabilidade e pnl com sinais trocados. Conferir '{fund}' de '{previous_date}' ate '{recent_date}'.\")\n            print(f\"Obtendo pnl total a partir da rentabilidade e PL medio do periodo\")\n            fund_quotas = self.get_table(\"all_funds_quotas\").query(\"Fund ==@fund and Date &gt;= @previous_date and Date &lt;= @recent_date\")\n            total_pnl_brl = (fund_quotas[\"#\"] * fund_quotas[\"Quota\"]).mean() * average_usdbrl * total_rentab_brl\n\n    elif fund_currency == \"brl\":\n        others_pnl_usd =others_pnl_usd / end_usdbrl \n        others_pnl_usd_curncy = others_pnl_usd_curncy / end_usdbrl\n        total_pnl_usd = hist_attr[\"p&amp;l_usd\"].sum() + others_pnl_usd\n        total_rentab_usd = ((1+total_rentab_brl) / (1+usdbrl_variation))-1\n        # Nesse caso:\n        # O pnl total precisa ter o mesmo sinal da rentabilidade total, senao algo esta errado.\n        if total_pnl_usd * total_rentab_usd &lt; 0:\n            print(f\"Rentabilidade e pnl com sinais trocados. Conferir '{fund}' de '{previous_date}' ate '{recent_date}'.\")\n            print(f\"Obtendo pnl total a partir da rentabilidade e PL medio do periodo\")\n            fund_quotas = self.get_table(\"all_funds_quotas\").query(\"Fund ==@fund and Date &gt;= @previous_date and Date &lt;= @recent_date\")\n            total_pnl_usd = (fund_quotas[\"#\"] * fund_quotas[\"Quota\"]).mean() / average_usdbrl * total_rentab_usd\n\n    others_attr_brl = total_rentab_brl - hist_attr[\"attr_brl\"].sum()\n    others_attr_brl_curncy = total_rentab_brl - hist_attr[\"attr_brl_curncy\"].sum()\n    others_attr_usd = total_rentab_usd - hist_attr[\"attr_usd\"].sum()\n    others_attr_usd_curncy = total_rentab_usd - hist_attr[\"attr_usd_curncy\"].sum()\n\n    # Vamos inserir o row 'outros' com o que falta de pnl para atingir a rentabilidade do periodo\n    hist_attr.loc[\"outros_outros\"] = [others_pnl_brl, others_pnl_brl_curncy, others_pnl_usd, others_pnl_usd_curncy, others_attr_brl, others_attr_brl_curncy, others_attr_usd, others_attr_usd_curncy]\n\n    #hist_attr.loc[\"outros_outros\"] = [others_pnl_brl, others_pnl_brl_curncy, others_pnl_usd, others_pnl_usd_curncy]\n\n\n    # Vamos calcular o % de atribuicao, proporcional a variacao da cota no periodo\n    # Em reais\n    #hist_attr[\"attr_brl\"] = hist_attr.apply(lambda row: ((row[\"p&amp;l_brl\"])/abs(row[\"p&amp;l_brl\"])) * abs(row[\"p&amp;l_brl\"] * total_rentab_brl/total_pnl_brl), axis=1)\n    #hist_attr[\"attr_brl_curncy\"] = hist_attr.apply(lambda row: row[\"p&amp;l_brl_curncy\"] * total_rentab_brl/total_pnl_brl, axis=1)\n    # Em dolares\n    #hist_attr[\"attr_usd\"] = hist_attr.apply(lambda row: row[\"p&amp;l_usd\"] * total_rentab_usd/total_pnl_usd, axis=1)\n    #hist_attr[\"attr_usd_curncy\"] = hist_attr.apply(lambda row: row[\"p&amp;l_usd_curncy\"] * total_rentab_usd/total_pnl_usd, axis=1)\n\n\n    hist_attr[\"Start_Date\"] = previous_date\n    hist_attr[\"End_Date\"] = recent_date\n\n    return hist_attr        \n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.calculate_daily_pnl","title":"<code>calculate_daily_pnl(fund, ref_date, currency, holiday_location='all', bdays=10, group_filters=[], type_filters=[], classification_filters=[], location_filters=[], asset_filters=[], currency_exposure_filters=[], include_cash=True, include_cash_debt=False, ticker_filters=[], annual_adm_fee=0, ignore_small_pnl=True, usdbrl_ticker='bmfxclco curncy')</code>","text":"<p>Calcula o PnL do dia de cada ativo no fundo.     (Nao inclui o PnL das taxas!). Args:     fund (str): nome do fundo     ref_date (str): data de refer\u00eancia do PnL     currency (str): moeda (usd|brl)     holiday_location (str, optional): refencia de feriados. Defaults to \"all\".     bdays (int, optional): quantidade de dias uteis a serem considerados.                         Defaults to 10.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def calculate_daily_pnl(self, fund, ref_date, currency, holiday_location=\"all\", bdays=10,\n        group_filters=[], type_filters=[], classification_filters=[], location_filters=[], asset_filters=[],\n        currency_exposure_filters=[], include_cash=True, include_cash_debt=False, ticker_filters=[], annual_adm_fee=0,\n        ignore_small_pnl=True, usdbrl_ticker=\"bmfxclco curncy\"): #test_op_fix=False):\n    \"\"\" Calcula o PnL do dia de cada ativo no fundo.\n        (Nao inclui o PnL das taxas!).\n    Args:\n        fund (str): nome do fundo\n        ref_date (str): data de refer\u00eancia do PnL\n        currency (str): moeda (usd|brl)\n        holiday_location (str, optional): refencia de feriados. Defaults to \"all\".\n        bdays (int, optional): quantidade de dias uteis a serem considerados.\n                            Defaults to 10.\n    \"\"\"\n    # dia util anterior\n    # Obtendo todas as posicoes e movimentacoes, dos ultimos dias\n    previous_date = self.get_bday_offset(ref_date, -bdays,\n                                    location=holiday_location)\n    df = self.get_positions_and_movements(fund, previous_date=previous_date,\n                                        recent_date=ref_date, currency=currency, usdbrl_ticker=usdbrl_ticker)\n    assets = self.get_table(\"assets\").rename(columns={\"Key\":\"Asset_ID\",\n                                                      \"Currency Exposure\":\"Currency_Exposure\"})\n\n    # Filtrando pelos ativos desejados\n    if len(classification_filters) &gt; 0:\n        classification_filters = [x.lower() for x in classification_filters]\n        assets = assets.query(\"Luxor_Classification.isin(@classification_filters)\")\n    elif len(group_filters) &gt; 0:\n        group_filters = [x.lower() for x in group_filters]\n        assets = assets.query(\"Group.isin(@group_filters)\")\n    elif len(type_filters) &gt; 0:\n        type_filters = [x.lower() for x in type_filters]\n        assets = assets.query(\"Type.isin(@type_filters)\")\n    elif len(location_filters) &gt; 0:\n        location_filters = [x.lower() for x in location_filters]\n        assets = assets.query(\"Location.isin(@location_filters)\")\n    elif len(asset_filters) &gt; 0:\n        asset_filters = [x.lower() for x in asset_filters]\n        assets = assets.query(\"Asset.isin(@asset_filters)\")\n    elif len(ticker_filters) &gt; 0:\n        ticker_filters = [x.lower() for x in ticker_filters]\n        assets = assets.query(\"Ticker.isin(@ticker_filters)\")\n    elif len(currency_exposure_filters) &gt; 0:\n        currency_exposure_filters = [x.lower() for x in currency_exposure_filters]\n        assets = assets.query(\"Currency_Exposure.isin(@currency_exposure_filters)\")\n    # Fazemos uma juncao interna, mantendo somente as linhas filtradas acima\n    df = df.merge(assets[[\"Asset_ID\", \"Name\", \"Group\", \"Type\", \"Location\",\n                           \"Luxor_Classification\"]], on=\"Asset_ID\")\n\n    df = df.sort_values(by=\"Date\", ascending=True)\n    df = df.rename(columns={\n        \"Date\" : \"Today\",\n        \"Open_Quantity\" : \"Close_Quantity\",\n        \"Last_Price\" : \"Close_Price\",\n        \"Current_Position_Value\" : \"Close_Mkt_Value\"\n    })\n\n    # Obtendo o preco de abertura\n    df[\"Open_Price\"] = df.groupby(\"Asset_ID\")[\"Close_Price\"].shift(1).fillna(df[\"Close_Price\"])\n    types_to_fix_open_price = ['ndf usdbrl']\n    #if test_op_fix:\n\n    df_op_fix = df.query(\"Type.isin(@types_to_fix_open_price)\").copy()\n    df_op_fix = self.__fix_open_price(df_op_fix, currency, ref_date, previous_date, usdbrl_ticker=usdbrl_ticker)\n\n    df = df.query(\"~Type.isin(@types_to_fix_open_price)\").copy()\n\n    # Juntando dfs novamente\n    df = pd.concat([df, df_op_fix])\n\n\n\n    # Precisamos consertar o open_price do FX. (Deve ser convertido pelo spot de fechamento sempre)\n\n    # Podemos puxar o caixa aqui e concatenar para as proximas operacoes\n    if include_cash:\n        cash = self.get_hist_cash_movements(fund, ref_date, currency=currency, bdays=bdays, holiday_location=holiday_location,\n                                            usdbrl_ticker=usdbrl_ticker)\n        cash[\"Today\"] = pd.to_datetime(cash[\"Today\"])\n        cash = cash.query(\"Close_Mkt_Value &gt;= 0\").copy()\n        cash[\"Luxor_Classification\"] = 'fixed income'\n\n        if len(location_filters) &gt; 0:\n            location_filters = [x.lower() for x in location_filters]\n            cash = cash.query(\"Location.isin(@location_filters)\").copy()\n\n        df = pd.concat([df, cash])\n        df = df.sort_values(by=\"Today\", ascending=True)\n\n    # Vamos segregar como d\u00edvida o caixa virado.\n    if include_cash_debt:\n        cash_debt = self.get_hist_cash_movements(fund, ref_date, currency=currency, bdays=bdays, holiday_location=holiday_location,\n                                                 usdbrl_ticker=usdbrl_ticker)\n        cash_debt[\"Today\"] = pd.to_datetime(cash_debt[\"Today\"])\n        cash_debt = cash_debt.query(\"Close_Mkt_Value &lt; 0\").copy()\n        cash_debt[\"Luxor_Classification\"] = 'debt'\n\n        # Caixa virado no Brasil, vamos desconsiderar, pois na pr\u00e1tica nao teremos.\n        #cash_debt = cash_debt.query(\"Asset_ID != 'caixa_caixa' or Location != 'bz'\").copy()\n        #Optando por manter e retirar o PnL posteriormente, para nao perder a contribuicao dele pra aporte e resgate\n\n        if len(location_filters) &gt; 0:\n            location_filters = [x.lower() for x in location_filters]\n            cash_debt = cash_debt.query(\"Location.isin(@location_filters)\").copy()\n\n        df = pd.concat([df, cash_debt])\n        df = df.sort_values(by=\"Today\", ascending=True)\n\n    # Vamos obter a data d-1\n    df[\"Yesterday\"] = (df.groupby(\"Asset_ID\")[\"Today\"].shift(1)\n                        .fillna(df[\"Today\"]-dt.timedelta(days=1))\n                        )\n    # Obtendo quantidade na abertura\n    df[\"Open_Quantity\"] = df.groupby(\"Asset_ID\")[\"Close_Quantity\"].shift(1).fillna(0)\n\n    # Obtendo valor de mercado por ativo na abertura\n    df[\"Open_Mkt_Value\"] = df[\"Open_Quantity\"] * df[\"Open_Price\"]\n    # Calculando precos de compas e vendas\n    df[\"Buy_Price\"] = np.where(df[\"Shares_Bought\"] &gt; 0, \n                        abs(df[\"Shares_Bought_Cost\"]/df[\"Shares_Bought\"]),0)\n    df[\"Sell_Price\"] = np.where(df[\"Shares_Sold\"] &lt; 0,\n                        abs(df[\"Amount_Sold\"]/df[\"Shares_Sold\"]), 0)\n\n    # Calculando PnL Di\u00e1rio (Caixa nao pode ser calculado aqui, pois o preco eh sempre 1)\n    df[\"Pnl_Bought\"] = abs(df[\"Shares_Bought\"]) * (df[\"Close_Price\"] - df[\"Buy_Price\"])\n    df[\"Pnl_Sold\"] = abs(df[\"Shares_Sold\"]) * (df[\"Sell_Price\"] - df[\"Open_Price\"])\n    # Ajustando PnL sold. Quando for debt, a venda representa a divida sendo tomada\n    # e a compra representa o pagamento da divida. Na primeira, ha juros e na segunda nao ha pnl.\n    df['Pnl_Bought'] = np.where(df['Luxor_Classification'] == 'debt', 0, df['Pnl_Bought'])\n    df['Pnl_Sold'] = np.where(df['Luxor_Classification'] == 'debt', \n                              (df['Open_Price']-df['Close_Price'])*abs(df['Shares_Sold']), 0)\n\n    df[\"Pnl_Unchanged\"] = (df[\"Open_Quantity\"] + df[\"Shares_Sold\"]) * (df[\"Close_Price\"] - df[\"Open_Price\"])\n\n    # O caixa no BZ pode estar virado temporariamente durante aportes/resgates de caixa_us, mas isso n estar\u00e1 gerando PnL\n    df[\"Pnl_Unchanged\"] = np.where((df[\"Asset_ID\"] == 'caixa_caixa') &amp; (df[\"Location\"] == \"bz\") &amp; (df[\"Location\"] == \"bz\")&amp; (df[\"Close_Mkt_Value\"] &lt; 0), 0, df[\"Pnl_Unchanged\"])\n    df[\"Pnl_Sold\"] = np.where((df[\"Asset_ID\"] == 'caixa_caixa') &amp; (df[\"Location\"] == \"bz\") &amp; (df[\"Close_Mkt_Value\"] &lt; 0), 0, df[\"Pnl_Sold\"])\n\n\n\n    # Ajustar aqui as operacoes com logica de pnl e exposicao\n    # Para nao considerar no AUM e no Cash Flow\n    types_to_recalculate = ['ndf usdbrl']\n    # Separando os dados em dois grupos, para editar um deles\n    df_exposure = df.query(\"Type.isin(@types_to_recalculate)\").copy()\n\n    df = df.query(\"~Type.isin(@types_to_recalculate)\").copy()\n    # Calculo especifico para pnl do FX.\n    df_exposure[\"Total_Pnl\"] = df_exposure[\"Pnl_Bought\"] + df_exposure[\"Pnl_Sold\"] \\\n                                 + df_exposure[\"Pnl_Unchanged\"]\n\n    df_exposure = df_exposure.set_index([\"Asset_ID\"])\n    # Valor de mercado sera o PnL acumulado\n    df_exposure[\"Total_Pnl\"] = df_exposure.groupby([\"Asset_ID\"])[\"Total_Pnl\"].cumsum()\n    df_exposure = df_exposure.reset_index()\n    df_exposure[\"Close_Mkt_Value\"] = df_exposure[\"Total_Pnl\"]\n    df_exposure[\"Open_Mkt_Value\"] = df_exposure[\"Close_Mkt_Value\"].shift(1, fill_value=0)\n\n    # Retirando qualquer possibilidade de impacto para aporte e resgate\n    df_exposure[\"Shares_Bought_Cost\"] = 0\n    df_exposure[\"Amount_Sold\"] = 0\n    # TODO Pensar o que vai mudar no caso de uma zeragem parcial\n    df_exposure = df_exposure.drop(columns=\"Total_Pnl\")\n\n\n    # Juntando dfs novamente\n    df = pd.concat([df, df_exposure])\n\n    # Calculando net de aporte e resgate por ativo\n    df[\"Cash_Flow\"] = abs(df[\"Shares_Bought_Cost\"]) + -abs(df[\"Amount_Sold\"])\n\n    hist_dividends = self.get_dividends(fund, ref_date, previous_date, currency=currency,\n                                        holiday_location=holiday_location, usdbrl_ticker=usdbrl_ticker)\n    hist_dividends = pd.merge(assets[[\"Asset_ID\", \"Ticker\"]],\n                        hist_dividends, on=\"Ticker\",how=\"left\")[[\"Date\", \"Asset_ID\", \"Amount\"]]\n    hist_dividends = hist_dividends.rename(columns={\"Date\": \"Today\", \"Amount\":\"Dividends\"})\n\n    df = df.merge(hist_dividends, on=[\"Today\", \"Asset_ID\"], how=\"left\")\n    df[\"Dividends\"] = df[\"Dividends\"].fillna(0)\n\n\n    df[\"Net_Subscriptions_Redemptions\"] = df.groupby(\"Today\")[\"Cash_Flow\"].sum()\n\n    df = df[[\"Today\", \"Name\", \"Asset_ID\", \"Group\", \"Type\", \"Location\", \"Luxor_Classification\",\n            \"Open_Quantity\", \"Open_Mkt_Value\", \"Close_Quantity\",\n            \"Shares_Bought\", \"Buy_Price\", \"Shares_Sold\", \"Sell_Price\",\n            \"Dividends\", \"Close_Price\", \"Open_Price\", \"Close_Mkt_Value\",\n            \"Cash_Flow\", \"Pnl_Bought\", \"Pnl_Sold\", \"Net_Subscriptions_Redemptions\", \"Pnl_Unchanged\"]]\n\n\n    # Calculando AUM no fechamento\n    df = df.set_index(\"Today\")\n    df[\"Close_AUM\"] = df.groupby(\"Today\")[\"Close_Mkt_Value\"].sum()\n    df = df.reset_index()\n\n    if annual_adm_fee != 0:\n    # Incluir taxas de adm e gestao no calculo do PnL\n    # Elas ja impactam o caixa, mas precisam ser consideradas no PnL\n        daily_adm_fee = (1+abs(annual_adm_fee))**(1/365)-1\n        df_fees = df[[\"Today\", \"Close_AUM\"]].groupby(\"Today\").last()\n        df_fees[\"Pnl_Unchanged\"] = -df_fees[\"Close_AUM\"] * daily_adm_fee\n        df_fees = df_fees.reset_index()\n        df_fees[\"Asset_ID\"] = \"taxas e custos_tx adm\"\n\n        value_columns = df.columns[7:-2] # Sobrescreve com 0, menos para Pnl_Unchanged e Close_AUM\n\n        df_fees[value_columns] = 0 # Para taxas, todos os outros valores nao importam.\n        # Finalmente, falta colocar dados de name, type e group\n        all_assets = self.get_table(\"assets\").rename(columns={\"Key\":\"Asset_ID\"})\n\n        df_fees = df_fees.merge(all_assets[[\"Asset_ID\", \"Name\",\"Type\", \"Group\", \"Location\"]], on=\"Asset_ID\")\n        if len(location_filters) &gt; 0:\n            location_filters = [x.lower() for x in location_filters]\n            df_fees = df_fees.query(\"Location.isin(@location_filters)\")\n\n        df = pd.concat([df, df_fees])  \n\n\n    df[\"Daily_Pnl\"] = df[\"Pnl_Unchanged\"] + df[\"Pnl_Bought\"] + df[\"Pnl_Sold\"] + df[\"Dividends\"]\n\n    # Calculando PL Inicial Ajustado\n    df = df.set_index(\"Today\")\n    df[\"Net_Subscriptions_Redemptions\"] = df.reset_index().groupby(\"Today\")[\"Cash_Flow\"].sum()\n    # Sera o valor de mercado da abertura somado com o net de aportes.\n    # Racional: Resgates rentabilizam no dia (ja estao no inicial)\n    #           Aportes rentabilizam no dia (nao estano no inicial)\n    df[\"Open_AUM\"] = df.reset_index().groupby(\"Today\")[\"Open_Mkt_Value\"].sum()\n    df[\"Open_AUM_Adjusted\"] = (df[\"Open_AUM\"] + \n                                df[\"Net_Subscriptions_Redemptions\"]*(df[\"Net_Subscriptions_Redemptions\"] &gt; 0)\n                                )\n\n    df[\"Daily_Attribution\"] = df[\"Daily_Pnl\"] / df[\"Open_AUM_Adjusted\"]\n    df[\"Daily_Return\"] = df.reset_index().groupby(\"Today\")[\"Daily_Attribution\"].sum()\n\n\n    # ao retornar, filtrar port Daily_Pnl != 0 e  not Daily_Pnl.isna()\n    if ignore_small_pnl:\n        return (df.reset_index()\n            .sort_values(by=\"Today\").query(''' (Daily_Pnl &lt;= -0.01 or Daily_Pnl &gt;= 0.01)\\\n                                        and not Daily_Pnl.isna()'''))\n    return df.reset_index().sort_values(by=\"Today\")\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.calculate_drawdown","title":"<code>calculate_drawdown(tickers, previous_date, recent_date, currency='local')</code>","text":"<p>Calcula o drawdown para um ativo ou um conjunto de ativos em cada data do perido fornecido.</p> <p>Parameters:</p> Name Type Description Default <code>tickers</code> <code>str | list | set</code> <p>ticker ou lista de tickers</p> required <code>previous_date</code> <code>date</code> <p>description</p> required <code>recent_date</code> <code>date</code> required <p>Returns:</p> Type Description <p>pd.DataFrame: Seguinte DataFrame usando Date como index -&gt; [[Asset, Period_Change, Previous_Peak, Drawdowns]]</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def calculate_drawdown(self, tickers, previous_date, recent_date, currency=\"local\"):\n    \"\"\"Calcula o drawdown para um ativo ou um conjunto de ativos em cada data do perido fornecido.\n\n    Args:\n        tickers (str|list|set): ticker ou lista de tickers\n        previous_date (dt.date): _description_\n        recent_date (dt.date):\n\n    Returns:\n        pd.DataFrame: Seguinte DataFrame usando Date como index -&gt; [[Asset, Period_Change, Previous_Peak, Drawdowns]]\n    \"\"\"\n\n    df = self.get_prices(tickers=tickers, previous_date=previous_date, recent_date=recent_date, currency=currency).set_index(\"Date\")\n\n    df[\"Period_Change\"] = df.groupby(\"Asset\").pct_change().fillna(0) + 1\n    df[\"Period_Change\"] = df[[\"Asset\", \"Period_Change\"]].groupby(\"Asset\").cumprod()\n    df[\"Previous_Peak\"] = df[[\"Asset\", \"Period_Change\"]].groupby(\"Asset\").cummax()\n    df[\"Drawdowns\"] = (df[\"Period_Change\"] - df[\"Previous_Peak\"])/df[\"Previous_Peak\"]\n\n    return df\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.calculate_fund_particip","title":"<code>calculate_fund_particip(fund_name, fund_owned, date)</code>","text":"<pre><code>Informa o percentual que 'fund_name' possui do 'fund_owned'.\n</code></pre> <p>Returns:     (float)</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def calculate_fund_particip(self, fund_name, fund_owned, date):\n    \"\"\"\n        Informa o percentual que 'fund_name' possui do 'fund_owned'.\n\n    Args:\n        fund_name (str) \n        fund_owned (str)\n        date (dt.date) \n    Returns:\n        (float)\n    \"\"\"\n    fund_name = fund_name.replace(\"_\", \" \")\n    fund_owned_key = fund_owned + \"_\" + fund_owned\n\n    # antes da data de incorporacao do manga master pelo lipi, 100% do manga master era do manga fic\n    if fund_name == \"mangalarga fic fia\" and fund_owned == \"mangalarga master\":\n        if date &lt; dt.date(2022,12,9):\n            return 1.0\n        else: return 0\n    # antes da data de incorporacao do manga master pelo lipi, 100% do fund_a era do manga master\n    if fund_name == \"mangalarga master\" and fund_owned == \"fund a\":\n        if date &lt; dt.date(2022,12,9):\n            return 1.0\n        else: return 0\n    # Adicionando mais um nivel de calculo \n    if fund_name == \"mangalarga fic fia\" and fund_owned == \"fund a\":\n        if date &lt; dt.date(2022,12,9):\n            return self.calculate_fund_particip(fund_name, \"mangalarga master\", date) * self.calculate_fund_particip(\"mangalarga master\", fund_owned, date)\n        else:\n            return self.calculate_fund_particip(fund_name, \"lipizzaner\", date) * self.calculate_fund_particip(\"lipizzaner\", fund_owned, date)\n\n    if fund_name == \"mangalarga ii fic fia\" and fund_owned == \"fund a\":\n        if date &lt; dt.date(2023,2,7):\n            return self.calculate_fund_particip(\"mangalarga fic fia\", \"mangalarga master\", date)\n        else:\n            return self.calculate_fund_particip(fund_name, \"lipizzaner\", date) * self.calculate_fund_particip(\"lipizzaner\", fund_owned, date)\n\n    position = self.get_table(\"hist_positions\")\n\n    # Busca otimizada pela posicao\n    try:        \n        position = (position[\"#\"].to_numpy()[(\n                        (position[\"Fund\"].to_numpy() == fund_name)\n                        &amp;\n                        (position[\"Asset_ID\"].to_numpy() == fund_owned_key)\n                        &amp;\n                        (position[\"Date\"].dt.date.to_numpy() &lt;= date)\n                    )].item(-1))\n    except (IndexError , KeyError):\n        # nao havia participacao do fundo em questao na data informada\n        return 0\n\n\n    fund_owned_total_amount = self.get_table(\"all_funds_quotas\")\n\n    try:\n        fund_owned_total_amount = (fund_owned_total_amount[\"#\"].to_numpy()[(\n                                        (fund_owned_total_amount[\"Fund\"].to_numpy() == fund_owned)\n                                        &amp;\n                                        (fund_owned_total_amount[\"Date\"].dt.date.to_numpy() &lt;= date)\n                                        )].item(-1))\n    except (IndexError , KeyError):\n\n        # Algo deu errado\n        return None\n\n    return round(position/fund_owned_total_amount, 5) # Mantem 5 casas decimais de precisao\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.calculate_price_correlation","title":"<code>calculate_price_correlation(assets_data, sample_frequency='monthly', period='12m', ref_date=dt.date.today())</code>","text":"<p>Calculate the correlation between assets in the columns of 'asset_prices'. The correlation is calculated using the formula: corr = (1 + r1r2 + r1r2corr) / sqrt(1+r12) * sqrt(1+r2*2) where r1 and r2 are the returns of the assets and corr is the correlation between the returns. The correlation is calculated using the formula above and the returns are calculated using the formula: r = (price[t] - price[t-1]) / price[t-1] where t is the time period. The returns are calculated using the sample frequency and the lookback in months. assets_data: dict mapeando ticker para {Name:str, Key:str e Location:str(bz ou us)} sample_frequency: str, default \"monthly\". Frequencia de amostragem dos precos. Pode ser \"daily\", \"weekly\" ou \"monthly\" period: str, default '12m'. Periodo de calculo da correlacao. Deve ser 'ytd' ou o numero de meses seguido por 'm'</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def calculate_price_correlation(self, assets_data:dict, sample_frequency=\"monthly\", period=\"12m\", ref_date=dt.date.today()):\n    \"\"\"\n    Calculate the correlation between assets in the columns of 'asset_prices'. The correlation is calculated using the formula:\n    corr = (1 + r1*r2 + r1*r2*corr) / sqrt(1+r1**2) * sqrt(1+r2**2)\n    where r1 and r2 are the returns of the assets and corr is the correlation between the returns.\n    The correlation is calculated using the formula above and the returns are calculated using the formula:\n    r = (price[t] - price[t-1]) / price[t-1]\n    where t is the time period.\n    The returns are calculated using the sample frequency and the lookback in months.\n    assets_data: dict mapeando ticker para {Name:str, Key:str e Location:str(bz ou us)}\n    sample_frequency: str, default \"monthly\". Frequencia de amostragem dos precos. Pode ser \"daily\", \"weekly\" ou \"monthly\"\n    period: str, default '12m'. Periodo de calculo da correlacao. Deve ser 'ytd' ou o numero de meses seguido por 'm'\n    \"\"\"\n    sample_frequency = sample_frequency.lower()\n    period = period.lower()\n    asset_prices = self.get_prices(tickers=set(assets_data.keys()) - {\"usdbrl curncy\"}, recent_date=ref_date,\n                                    period=period, currency=\"usd\", usdbrl_ticker=\"usdbrl curncy\")\n    usdbrl_prices = self.get_prices(tickers={\"usdbrl curncy\"}, period=period)\n    asset_prices = pd.concat([asset_prices, usdbrl_prices])\n\n    asset_prices = asset_prices.pivot(index=\"Date\", columns=\"Asset\", values=\"Last_Price\").ffill()\n\n\n    frequencies = {\"daily\":\"B\", \"weekly\":\"W\", \"monthly\":\"ME\"}\n    asset_prices = asset_prices.resample(frequencies[sample_frequency]).last()\n    #returns = asset_prices.pct_change(lookback_in_months)\n    #returns = returns.dropna()\n    correlations = asset_prices.corr()\n    correlations.index.name = \"Asset_Corr\"\n    correlations = correlations.reset_index()\n\n    value_vars = correlations.columns\n    correlations = correlations.melt(id_vars=[\"Asset_Corr\"], value_vars=value_vars, value_name=\"Correlation\", var_name=\"Asset\").sort_values(by=\"Correlation\").reset_index(drop=True)\n    correlations[\"Key_Asset_Corr\"] = correlations[\"Asset_Corr\"].apply(lambda x: assets_data[x][\"Key\"])\n    correlations[\"Key_Asset\"] = correlations[\"Asset\"].apply(lambda x: assets_data[x][\"Key\"])\n    correlations.index.name=\"Order\"\n    correlations = correlations.reset_index()\n    correlations[\"Comp_Key\"] = correlations[\"Key_Asset_Corr\"]+\"_\"+correlations[\"Key_Asset\"]\n    correlations[\"Frequency\"] = sample_frequency.replace(\"ly\",\"\").title()\n    period = period if int(period.replace(\"m\",\"\").replace(\"ytd\",\"0\")) &lt; 12 else str(int(period.replace(\"m\",\"\"))/12).replace(\".0\",\"\")+\"y\"\n    correlations[\"Period\"] = period.upper()\n\n    return correlations\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.calculate_tracking_error","title":"<code>calculate_tracking_error(returns_ticker, benchmark_ticker, previous_date=dt.date.today() - dt.timedelta(days=(365 * 2)), recent_date=dt.date.today() - dt.timedelta(days=1), period=None, holiday_location='all')</code>","text":"<p>Calcula o tracking error de uma serie de retorno com relacao ao benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>returns_ticker</code> <code>str</code> <p>ticker da serie de retorno que sera testada</p> required <code>benchmark_ticker</code> <code>str</code> <p>ticker da serie de retorno que sera usada para comparacao</p> required <code>previous_date</code> <code>date</code> <p>data de inicio do teste</p> <code>today() - timedelta(days=365 * 2)</code> <code>recent_date</code> <code>date</code> <p>data de fim do teste</p> <code>today() - timedelta(days=1)</code> <p>Returns:     type: description</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def calculate_tracking_error(self, returns_ticker, benchmark_ticker, previous_date=dt.date.today()-dt.timedelta(days=365*2), recent_date=dt.date.today()-dt.timedelta(days=1), period=None, holiday_location=\"all\"):\n    \"\"\" Calcula o tracking error de uma serie de retorno com relacao ao benchmark.\n\n    Args:\n        returns_ticker (str): ticker da serie de retorno que sera testada\n        benchmark_ticker (str): ticker da serie de retorno que sera usada para comparacao\n        previous_date (dt.date): data de inicio do teste\n        recent_date (dt.date): data de fim do teste\n    Returns:\n        _type_: _description_\n    \"\"\"\n\n    if period is not None:\n        previous_date = self.get_start_period_dt(recent_date, period=period, holiday_location=holiday_location)\n\n\n    returns = self.get_prices([returns_ticker, benchmark_ticker], previous_date=previous_date, recent_date=recent_date, currency=\"usd\")\n\n    returns = returns.set_index(\"Date\").pivot(columns=\"Asset\").dropna(how=\"any\")\n    returns = returns.pct_change().dropna()\n\n    returns.columns = [returns_ticker, benchmark_ticker]\n\n    n_periods = len(returns)\n    returns[\"Diff\"] = (returns[returns_ticker] - returns[benchmark_ticker])**2\n\n    tracking_error = 0\n    if (n_periods - 1) &gt; 0:\n        tracking_error = (returns[\"Diff\"].sum()/(n_periods - 1)) ** (1/2)\n\n\n    return tracking_error\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_bday_offset","title":"<code>get_bday_offset(date, offset=1, location='bz')</code>","text":"<p>Retorna dia util com offset dias pra frente ou pra tras.</p> <p>location (str):      'any'-&gt; se \u00e9 feriado em qualquer um dos lugares cadastrados</p> <pre><code>'all' -&gt; se \u00e9 feriado em todas as localidades da tabela\n\n'us', 'bz', (...) -&gt; para local especifico, com codigo presente na tabela\n</code></pre> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_bday_offset(self, date, offset=1, location=\"bz\"):\n    \"\"\" Retorna dia util com offset dias pra frente ou pra tras.\\n\n            location (str): \n                'any'-&gt; se \u00e9 feriado em qualquer um dos lugares cadastrados\\n\n                'all' -&gt; se \u00e9 feriado em todas as localidades da tabela\\n\n                'us', 'bz', (...) -&gt; para local especifico, com codigo presente na tabela\\n\n    \"\"\"\n    offset_direction = 0\n    i = 1\n    if offset != 0:\n        offset_direction = offset/abs(offset)\n        i = abs(offset)\n\n    while i &gt; 0 :\n        # Pegamos proxima data\n        date = date + dt.timedelta(days=offset_direction)\n        # Verificamos se eh dia util. Se for, contabilizamos.\n        if (date.weekday() &lt; 5) and (not self.is_holiday(date, location) ):\n            i -= 1\n        elif offset == 0:\n            offset_direction = -1\n\n    return date\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_bdays_count","title":"<code>get_bdays_count(recent_date, previous_date, location='bz')</code>","text":"<p>Calcula quantos dias uteis existem entre recent_date e previous_date (Exclusivo, ou seja, recent_date e previous_date nao sao contados)</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_bdays_count(self, recent_date, previous_date, location=\"bz\"):\n    \"\"\"\n    Calcula quantos dias uteis existem entre recent_date e previous_date\n    (Exclusivo, ou seja, recent_date e previous_date nao sao contados)\n    \"\"\"\n    counter = 0\n    iter_date = recent_date - dt.timedelta(days=1) # primeiro dia nao eh contado\n\n    while iter_date &gt; previous_date:\n\n        if (iter_date.weekday() &lt; 5) and (not self.is_holiday(iter_date, location) ):\n            counter += 1\n        iter_date -= dt.timedelta(days=1)\n\n    return counter\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_benchmark_spx_cash","title":"<code>get_benchmark_spx_cash(previous_date, recent_date, prop_spx=0.8, prop_cash=0.2, currency='usd', usdbrl_ticker='bmfxclco curncy', cash_ticker='jpmutcc lx equity', spx_ticker='spxt index')</code>","text":"<p>Obtem a serie historica do benchmark luxor a partir do indice s&amp;p e caixa nas proporcoes desejadas. Args:     previous_date (datetime.date): Data inicial desejada     recent_date (datetime.date): Data final desejada     prop_spx (float, optional): Proporcao s&amp;p. Defaults to 0.8.     prop_cash (float, optional): Proporcao caixa. Defaults to 0.2.     currency (str, optional): Moeda desejada ('usd','brl'). Defaults to \"usd\".     usdbrl_ticker (str, optional): Ticker do usdbrl para conversao. Defaults to \"bmfxclco curncy\".     cash_ticker (str, optional): Ticker do ativo usado como caixa. Defaults to \"jpmutcc lx equity\".     spx_ticker (str, optional): Ticker do ativo usado como s&amp;p. Defaults to \"spxt index\". Returns:     pandas.DataFrame: DataFrame contendo coluna Benchmark e Datas como index.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_benchmark_spx_cash(self, previous_date:dt.date, recent_date:dt.date, prop_spx=0.8, prop_cash=0.2, currency=\"usd\",\n                           usdbrl_ticker=\"bmfxclco curncy\", cash_ticker=\"jpmutcc lx equity\", spx_ticker=\"spxt index\"):\n    \"\"\" \n    Obtem a serie historica do benchmark luxor a partir do indice s&amp;p e caixa nas proporcoes desejadas.\n    Args:\n        previous_date (datetime.date): Data inicial desejada\n        recent_date (datetime.date): Data final desejada\n        prop_spx (float, optional): Proporcao s&amp;p. Defaults to 0.8.\n        prop_cash (float, optional): Proporcao caixa. Defaults to 0.2.\n        currency (str, optional): Moeda desejada ('usd','brl'). Defaults to \"usd\".\n        usdbrl_ticker (str, optional): Ticker do usdbrl para conversao. Defaults to \"bmfxclco curncy\".\n        cash_ticker (str, optional): Ticker do ativo usado como caixa. Defaults to \"jpmutcc lx equity\".\n        spx_ticker (str, optional): Ticker do ativo usado como s&amp;p. Defaults to \"spxt index\".\n    Returns:\n        pandas.DataFrame: DataFrame contendo coluna Benchmark e Datas como index.\n    \"\"\"\n    complementary_previous_date=previous_date\n\n    if previous_date &lt; dt.date(2018,12,31) and cash_ticker == 'jpmutcc lx equity':\n        #logger.warning(f\"Nao ha datas anteriores a {dt.date(2018,12,31)} para o JPMUTCC. Sera usada essa.\")\n        previous_date = dt.date(2018,12,31)\n\n    if previous_date &lt; dt.date(2020,3,2) and cash_ticker == 'sofrindx index':\n        #logger.warning(f\"Nao ha datas anteriores a {dt.date(2018,12,31)} para o JPMUTCC. Sera usada essa.\")\n        previous_date = dt.date(2020,3,2)\n\n\n\n    data = self.get_prices(tickers=[spx_ticker, cash_ticker], previous_date=previous_date, currency=currency,recent_date=recent_date, \n                           usdbrl_ticker=usdbrl_ticker)\n\n    if recent_date == dt.date.today():\n        d = dt.datetime(recent_date.year, recent_date.month, recent_date.day)\n        # Precisamos ajustar o ultimo preco para o mais recente\n        last_prices = pd.DataFrame({\"Date\" : [d, d], \n                                    \"Asset\" : [\"spxt index\",cash_ticker], \n                                  \"Last_Price\" : [self.get_price(\"spxt index\"), self.get_price(cash_ticker)]\n                                })\n        data = pd.concat([data.query(\"Date &lt; @d\").copy(), last_prices])\n\n    if complementary_previous_date != previous_date:\n        # Entrar aqui significa que o periodo precisou ser ajustado, pois eh anterior a existencia do indice usado para o caixa\n        # Nesse caso, vamos pegar todo o periodo que faltou e considerar retorno diario do FED FUND\n\n        # Primeiro, obtemos o periodo completo do s&amp;p\n        data_spx = self.get_prices(tickers=[spx_ticker], previous_date=complementary_previous_date,\n                                             recent_date=recent_date, currency=currency, usdbrl_ticker=usdbrl_ticker)\n        # Em seguida, vamos pegar o parcial do jpmutcc e completar com o treasury, \n        # Para isso, sera necessario criar uma serie normalizada a partir dos retornos diarios\n        data_treasury = data.query(\"Asset == @cash_ticker\").copy().set_index([\"Date\",\"Asset\"]).pct_change().reset_index().dropna()\n        cash_initial_date = min(data_treasury[\"Date\"])\n        complementary_treasury = self.get_prices(tickers=[\"fedl01 index\"], previous_date=complementary_previous_date,\n                                             recent_date=recent_date, currency=currency, usdbrl_ticker=usdbrl_ticker)\n        complementary_treasury[\"Last_Price\"] = (1 + complementary_treasury[\"Last_Price\"]/100) ** (1/360)-1\n        complementary_treasury = complementary_treasury.query(\"Date &lt; @cash_initial_date\").copy()\n        data_treasury = pd.concat([complementary_treasury, data_treasury])\n        data_treasury[\"Asset\"] = cash_ticker\n        data_treasury[\"Last_Price\"] = (data_treasury[\"Last_Price\"] + 1).cumprod()\n        data = pd.concat([data_spx, data_treasury])\n\n\n\n    # Gerando tabela de granularidade mensal, com as cotas mensais do indicador\n    df = data.pivot(columns=\"Asset\", index=\"Date\",values=\"Last_Price\").ffill().bfill().reset_index()\n    df = df.groupby(df['Date'].dt.to_period('M')).last().reset_index(drop=True).set_index(\"Date\")\n    df = df.pct_change().fillna(0).reset_index()\n    df[\"Benchmark\"] = (df[spx_ticker] * prop_spx + df[cash_ticker] * prop_cash) + 1\n    # Salvando a base mensal num novo df\n    df_benchmark = df[[\"Date\", \"Benchmark\"]].set_index(\"Date\").cumprod()\n\n    # Gerando base de granularidade diaria com o fator de retorno mtd com 80% spxt e 20% jpmutcc\n    df = data.set_index(\"Date\")\n    df[\"Last_Price\"] = df.groupby([\"Asset\"]).pct_change().fillna(0)+1\n    df = df.rename(columns={\"Last_Price\":\"Pct_Change\"})\n\n    df = df.pivot(columns=\"Asset\", values=\"Pct_Change\")\n\n    df[cash_ticker] = df[cash_ticker].fillna(1) #df[cash_ticker].ffill() # como tende a ser constante, podemos estimar os valores na\n    df[spx_ticker] = df[spx_ticker].fillna(1) # para o s&amp;p por ser variavel, preencheremos com 0 sempre\n\n    df[\"Year\"] = df.index.year\n    df[\"Month\"] = df.index.month\n    # Acumulando para encontrar o retorno MTD\n    df_mtd_w = df.groupby([\"Year\", \"Month\"]).cumprod()\n    # Ponderando para encontrar o benchmark MTD 80/20\n    df_mtd_w[\"MTD\"] = df_mtd_w[cash_ticker] * prop_cash + df_mtd_w[spx_ticker] * prop_spx\n\n    # Agora que temos o fator MTD granularidade diaria e a cota mensal\n    # Vamos colocar a cota mensal numa coluna da tabela de granulariade diaria\n    # Primeiro, precisamos de uma juncao que compreenda ano e mes\n    df_mtd_w[\"Year\"] = df_mtd_w.index.year\n    df_mtd_w[\"Month\"] = df_mtd_w.index.month\n    df_benchmark[\"Year\"] = df_benchmark.index.year\n    df_benchmark[\"Month\"] = df_benchmark.index.month\n    df_benchmark = df_benchmark.rename(columns={\"Benchmark\":\"Previous_Month_Acc\"})\n\n    # shiftando para que seja sempre a rentabilidade acumulada ate o mes anterior.\n    # Essa operacao tambem ira descartar qualquer acumulo feito numa data que nao for fim de mes.\n    df_benchmark[\"Previous_Month_Acc\"] = df_benchmark[\"Previous_Month_Acc\"].shift(1).ffill().bfill()\n\n    # Preenchemos entao uma coluna com a rentabilidade de cada mes anterior\n    df = df_mtd_w.reset_index().merge(df_benchmark, on=[\"Month\", \"Year\"], how=\"left\")\n\n    # Finalmente, o benchmark sera obtido atraves do acumulado anterior acumulado com o MTD atual\n    df[\"Benchmark\"] = df[\"Previous_Month_Acc\"] * df[\"MTD\"]\n\n    return df[[\"Date\", \"Benchmark\"]].set_index(\"Date\")\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_current_fx_data","title":"<code>get_current_fx_data(fund, fx_ticker)</code>","text":"<p>Calcula o PNL de FX de um fundo. -&gt; CONSIDERANDO QUE EH USDBRL</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_current_fx_data(self, fund, fx_ticker):\n    \"\"\"Calcula o PNL de FX de um fundo. -&gt; CONSIDERANDO QUE EH USDBRL\"\"\"\n    fx_positions = self.get_table(\"last_positions\").query(\"Fund == @fund and Ticker == @fx_ticker\").copy()\n    fx_positions[\"Last_Price\"] = fx_positions[\"Ticker\"].apply(lambda x: self.get_price(x))\n    fx_positions[\"Pnl_USD\"] = fx_positions[\"#\"] * ((fx_positions[\"Last_Price\"]/fx_positions[\"Avg_price\"]) - 1)\n    fx_positions[\"Pnl_BRL\"] = fx_positions[\"#\"] * (fx_positions[\"Last_Price\"] - fx_positions[\"Avg_price\"])\n    fx_positions[\"Exposure\"] = fx_positions[\"#\"] * fx_positions[\"Last_Price\"]\n    fx_positions = fx_positions[[\"Pnl_USD\", \"Pnl_BRL\", \"Exposure\"]].squeeze().to_dict()\n\n    return fx_positions\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_group_results","title":"<code>get_group_results(previous_date=None, recent_date=None, segments=None, currency='brl', inception_dates=None, period=None)</code>","text":"<p>Calcula e retorna o resultado de cada segmento.</p> <p>Parameters:</p> Name Type Description Default <code>recent_date</code> <code>date</code> <p>Data inicial NAO inclusiva (sera filtrado por uma data anterior a essa.)                     Ex. Para considerar retorno ate o mes de marco, deve ser passada alguma data em abril.</p> <code>None</code> <code>previous_date</code> <code>date</code> <p>Data final</p> <code>None</code> <code>segments</code> <code>_type_</code> <p>Lista ou string com nome dos segmentos  desejados. Defaults to None. Quando None, retorna todos os segmentos existentes.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Retorno por segmento.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_group_results(self, previous_date= None, recent_date = None,  segments=None, currency=\"brl\", inception_dates=None, period = None) -&gt; pd.DataFrame:\n    \"\"\"Calcula e retorna o resultado de cada segmento.\n\n    Args:\n        recent_date (dt.date): Data inicial NAO inclusiva (sera filtrado por uma data anterior a essa.)\n                                Ex. Para considerar retorno ate o mes de marco, deve ser passada alguma data em abril.\n        previous_date (dt.date): Data final\n        segments (_type_, optional): Lista ou string com nome dos segmentos \n            desejados. Defaults to None. Quando None, retorna todos os segmentos existentes.\n\n    Returns:\n        pd.DataFrame: Retorno por segmento.\n    \"\"\"\n\n    if (previous_date is not None) and (previous_date &gt; recent_date):\n        logger.error(\"Data inicial \u00e9 menor que a data final. Parametros invertidos?\")\n        sys.exit()\n\n    # Obtendo tabela de retornos historicos extraida do retorno consolidado\n    group_results = self.get_table(\"luxor group results\")[[\"Date\", \"Segment\", \"Return_Multiplier\"]].copy()\n    group_results[\"Date\"] = pd.to_datetime(group_results[\"Date\"])\n\n    # Selecionando o periodo de tempo desejado\n    if period is not None:\n        recent_date = dt.date(recent_date.year,recent_date.month,20)+dt.timedelta(days = 15)\n        previous_date = self.get_start_period_dt(recent_date, period=period)\n        if period.lower() == \"ytd\":\n            if recent_date.month == 1: # fechamento de dezembro! Vamos ter que ajustar o previous_date\n                previous_date = self.get_start_period_dt(recent_date-dt.timedelta(days=35), period=period)\n            previous_date += dt.timedelta(days=1)\n\n    group_results = group_results.query(\"Date &gt;= @previous_date and Date &lt; @recent_date\").sort_values(by=(\"Date\")).copy()\n\n    if segments is not None:\n        # Vamos deixar apenas os segmentos informados no parametro 'segments'\n        if type(segments) is str:\n            segments = [segments]\n        if type(segments) is list:\n            seg_filter = segments\n        else:\n            logger.error(\"'segments' precisa ser do tipo 'list' ou 'str'\")\n            sys.exit()\n\n        group_results = group_results.query(\" Segment.isin(@seg_filter)\").copy()\n\n\n    group_results = group_results.set_index(\"Date\").groupby(\"Segment\").prod() -1\n    group_results = group_results.reset_index()\n\n    # Retornos dos segmentos, por padrao, estarao em R$.\n    # Para ver em US$ precisa ser feita a conversao.\n    if currency == \"usd\":\n\n        def __get_usdbrl_variation(segment):\n            # ajustando janelas de inicio e fim, para pegar a variacao correta de usd no periodo -&gt;  A base do grupo eh de retornos, a de usd eh de pre\u00e7os                \n            usd_recent_date  = dt.date(recent_date.year, recent_date.month, 1) - dt.timedelta(days=1)\n            usd_previous_date = dt.date(previous_date.year, previous_date.month, 1) -dt.timedelta(days=1)\n            # Ainda, se a data for anterior ou igual ao inceptio date, vamos usar o incepion date\n            #print(f\"conversao usdbrl usando: previous_date:{usd_previous_date}  recent_date: {usd_recent_date}\")\n            if inception_dates is not None and segment in inception_dates.keys():\n\n                sgmnt_inception = inception_dates[segment] -dt.timedelta(days=1) # subtraindo 1 dia para usar o fechamento do dia anterior ao inicio\n                usd_previous_date = sgmnt_inception if ((sgmnt_inception.year == previous_date.year) and (sgmnt_inception.month == previous_date.month)) else usd_previous_date\n                                        #\"bmfxclco curncy\" -&gt; converter usando usd cupom limpo\n\n            delta_usdbrl = self.get_pct_change(\"bmfxclco curncy\", previous_date=usd_previous_date, recent_date=usd_recent_date)\n            #print(f\"Segmento: {segment}  Retorno_BRL:{multiplier}  delta usdbrl={delta_usdbrl}   Retorno_USD: {(1+multiplier)/(1+delta_usdbrl)-1}\")\n            return delta_usdbrl\n\n\n        # Convertendo para usd o que estiver em brl\n\n        group_results[\"Return_Multiplier\"] = group_results.apply(lambda row: (1 + row[\"Return_Multiplier\"])/(1+__get_usdbrl_variation(row[\"Segment\"])) -1, axis=1)\n\n    group_results = group_results.set_index(\"Segment\")\n\n    return group_results\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_hist_cash_movements","title":"<code>get_hist_cash_movements(fund, ref_date, bdays=10, holiday_location='all', currency='usd', usdbrl_ticker='bmfxclco curncy')</code>","text":"<p>Retorna o historico de caixa do fundo. Args:     fund (str): nome do fundo     currency (str): moeda (usd|brl)</p> <p>Returns:</p> Type Description <p>pandas.DataFrame: historico de caixa</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_hist_cash_movements(self, fund, ref_date, bdays=10, \n        holiday_location=\"all\", currency=\"usd\", usdbrl_ticker = \"bmfxclco curncy\"):\n    \"\"\"Retorna o historico de caixa do fundo.\n    Args:\n        fund (str): nome do fundo\n        currency (str): moeda (usd|brl)\n\n    Returns:\n        pandas.DataFrame: historico de caixa\n    \"\"\"\n\n    ## TODO: Remover linhas com pnl e retorno vazios!\n\n\n    fund = fund.replace(' ', '_').lower()\n    previous_date = self.get_bday_offset(ref_date, -bdays,\n                                    location=holiday_location)\n\n    cash = self.get_table(\"hist_portfolios_concentration\").query(\"Group.isin(['caixa','caixa_us']) and Fund_Name == @fund\").copy() \n    cash[\"Date\"] = cash[\"Date\"].dt.date\n    cash = cash.query(\"Date &gt;= @previous_date\").copy()\n    # cash estara com ambos os caixas em R$ ou U$.\n    # Para saber a moeda do caixa retornado, precisamos saber a moeda do fundo\n    fund_key = fund.replace('_', ' ')\n    fund_location = self.get_table(\"funds\").query(\"Fund == @fund_key\")[\"Location\"].squeeze()\n    currency_location  = \"us\" if currency == \"usd\" else \"bz\"\n    if fund_location != currency_location:\n        usdbrl = self.get_prices(usdbrl_ticker, previous_date=cash[\"Date\"].min(), recent_date=cash[\"Date\"].max(),)\n        usdbrl = self.usdbrl_clean_coupon_fix(usdbrl)\n        usdbrl[\"Date\"] = usdbrl[\"Date\"].dt.date\n        usdbrl = usdbrl.rename(columns={\"Last_Price\":\"usdbrl\"})\n        cash = cash.merge(usdbrl[[\"Date\", \"usdbrl\"]], on=\"Date\", how=\"left\")\n        if fund_location == \"bz\":\n            cash[\"Market_Value\"] = cash[\"Market_Value\"] / cash[\"usdbrl\"]\n        else:\n            cash[\"Market_Value\"] = cash[\"Market_Value\"] * cash[\"usdbrl\"]\n\n        # Removendo coluna auxiliar\n        cash = cash.drop(columns=[\"usdbrl\"])\n\n    # Vamos calcular o preco de fechamento com o rendimento do dia\n    bz_cash_index = self.get_prices(\"bzacselc index\", period=\"60m\", currency=currency, usdbrl_ticker=usdbrl_ticker)\n    bz_cash_index[\"Date\"] = bz_cash_index[\"Date\"].dt.date\n    bz_cash_index = (bz_cash_index[[\"Date\", \"Last_Price\"]]\n                        .rename(columns={\"Last_Price\":\"bz_cash_index\"})\n                        .set_index(\"Date\").pct_change()\n                        .fillna(0)+1).reset_index()\n\n    us_cash_index = self.get_prices(\"sofrindx index\", period=\"60m\", currency=currency, usdbrl_ticker=usdbrl_ticker)\n    us_cash_index[\"Date\"] = us_cash_index[\"Date\"].dt.date\n    us_cash_index = (us_cash_index[[\"Date\", \"Last_Price\"]]\n                        .rename(columns={\"Last_Price\":\"us_cash_index\"})\n                        .set_index(\"Date\").pct_change()\n                        .fillna(0)+1).reset_index()\n\n    us_margin_cost = self.get_prices(\"sofr + 75bps\", period=\"60m\", currency=currency, usdbrl_ticker=usdbrl_ticker)\n    us_margin_cost[\"Date\"] = us_margin_cost[\"Date\"].dt.date\n    us_margin_cost = (us_margin_cost[[\"Date\", \"Last_Price\"]]\n                        .rename(columns={\"Last_Price\":\"us_margin_cost\"})\n                        .set_index(\"Date\").pct_change()\n                        .fillna(0)+1).reset_index()\n\n    cash = cash.merge(bz_cash_index, on=\"Date\", how=\"left\")\n    cash = cash.merge(us_cash_index, on=\"Date\", how=\"left\")\n    cash = cash.merge(us_margin_cost, on=\"Date\", how=\"left\")\n    # Dependendo do ativo e da posicao a variacao no dia ser\u00e1 diferente\n    cash[\"Open_Price\"] = 1\n    cash[\"Close_Price\"] = cash[\"bz_cash_index\"]\n    cash[\"Close_Price\"] = np.where(cash[\"Group\"] == \"caixa_us\",\n                                   cash[\"us_cash_index\"], cash[\"Close_Price\"]\n                                   )\n    cash[\"Location\"] = 'us'\n    cash[\"Location\"] = np.where(cash[\"Group\"] == \"caixa_us\",\n                                   'us', 'bz'\n                                   )\n    cash[\"Close_Price\"] = np.where((cash[\"Market_Value\"] &lt; 0) &amp; (cash[\"Group\"] == \"caixa_us\") ,\n                                    cash[\"us_margin_cost\"], cash[\"Close_Price\"]\n                                    )\n\n    cash = (cash[[\"Date\", \"Group\", \"Location\", \"Market_Value\", \"Open_Price\", \"Close_Price\"]]\n                .rename(columns={\"Date\":\"Today\",\n                                 \"Market_Value\":\"Close_Mkt_Value\"\n                                 }))\n    # Criando colunas necessarias para concatenar com o df do daily pnl\n    cash[\"Close_Quantity\"] = cash[\"Close_Mkt_Value\"]\n\n    cash[\"Type\"] = cash[\"Group\"]\n    cash[\"Name\"] = cash[\"Group\"].str.replace(\"_\", \" \").str.title()\n    cash[\"Asset_ID\"] = cash[\"Type\"].str.replace(\"_\",\" \")+\"_\"+cash[\"Type\"].str.replace(\"_\",\" \")\n    cash[\"Price_Key\"] = cash[\"Type\"]\n\n    cash[\"Delta_Shares\"] = cash.groupby([\"Group\"])[\"Close_Quantity\"].diff().fillna(0)\n    cash[\"Shares_Sold\"] = np.where(cash[\"Delta_Shares\"] &lt; 0, cash[\"Delta_Shares\"], 0)\n    cash[\"Amount_Sold\"] = cash[\"Shares_Sold\"]\n    cash[\"Shares_Bought\"] = np.where(cash[\"Delta_Shares\"] &gt; 0, cash[\"Delta_Shares\"], 0)\n    cash[\"Shares_Bought_Cost\"] = cash[\"Shares_Bought\"]\n\n    return cash\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_position_variation","title":"<code>get_position_variation(fund_name, recent_date, previous_date)</code>","text":"<p>Fornece um dataframe com a variacao das posicoes do fundo entre as datas.  Inclui tambem uma coluna de flag informando se a posicao foi zerada ou nao</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_position_variation(self, fund_name, recent_date, previous_date):\n    \"\"\"\n        Fornece um dataframe com a variacao das posicoes do fundo entre as datas. \n        Inclui tambem uma coluna de flag informando se a posicao foi zerada ou nao\n\n    \"\"\"\n\n    # Pegamos as posicoes da data mais recente, transformando de dict para dataframe\n    cur_positions = self.get_positions(fund_name, recent_date)\n    cur_positions = pd.DataFrame(cur_positions.items(), columns = [\"Key\", \"#_cur\"])\n\n    # Igual para as posicoes da data mais antiga\n    prev_positions = self.get_positions(fund_name, previous_date)\n    prev_positions = pd.DataFrame(prev_positions.items(), columns=[\"Key\", \"#_prev\"])\n\n    # Realizamos juncao externa na chave, mantendo assim dados nan (zeragens e ativos novos)\n    diff = pd.merge(cur_positions, prev_positions, on=\"Key\", how=\"outer\").fillna(0)\n\n    diff[\"Variation\"] = diff[\"#_cur\"] - diff[\"#_prev\"]\n    diff[\"Closed\"] = diff[\"#_cur\"] == 0\n\n    return diff[[\"Key\", \"Variation\", \"Closed\"]]\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_positions","title":"<code>get_positions(fund_name, date=dt.date.today(), recent_date=dt.date.today(), previous_date=None, period=None, get_inner_positions=False, force_month_end=False)</code>","text":"<p>Fornece um dicionario com as posicoes do fundo na data informada.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_positions(self, fund_name, date=dt.date.today(), recent_date=dt.date.today(), previous_date=None, period=None,\n                  get_inner_positions=False, force_month_end=False):\n    \"\"\"\n        Fornece um dicionario com as posicoes do fundo na data informada.\n    \"\"\"\n    hist_pos = self.get_table(\"hist_positions\").query(\"Fund == @fund_name\")\n\n    if (previous_date is None) and (period is None):\n        # manter funcionamento antes da implementacao da funcionalidade de cosultar multiplas datas\n        # Separamos posicoes historicas apenas do fundo que nos interessa antes da data informada\n        hist_pos = hist_pos.loc[hist_pos[\"Date\"].dt.date &lt;= date]\n\n        visited = set()\n        positions = {}\n        rows = hist_pos.to_dict(\"records\") # Obtendo lista de rows(dicts) para iterar\n\n        rows.reverse() # vamos iterar do primeiro ao ultimo\n\n        for row in rows:\n            if row[\"Asset_ID\"] not in visited:\n                visited.add(row[\"Asset_ID\"])\n                if row[\"#\"] &gt; 0.000001 or row[\"#\"] &lt; -0.000001:\n                    positions[row[\"Asset_ID\"]] = row[\"#\"]\n\n        if not get_inner_positions:\n            return positions\n\n        # Vamos ver se tem fundos da luxor\n        # Vamos remover esse e explodir em posicoes internas\n        inner_funds = {\"lipizzaner_lipizzaner\" : \"lipizzaner\",\n                    \"fund a_fund a\" : \"fund a\"}\n        for inner_fund in inner_funds.keys():\n            if inner_fund in positions.keys():\n                inner_positions = self.get_positions(inner_funds[inner_fund], date,\n                                                     get_inner_positions=True)\n                positions.pop(inner_fund)\n                positions.update(inner_positions)\n\n        return positions\n\n    # Obtendo data de inicio e validando datas\n    assert(recent_date is not None)\n    if period is not None:\n        previous_date = self.get_start_period_dt(recent_date, period=period, force_month_end=force_month_end)\n    assert(recent_date &gt; previous_date)\n\n    previous_or_before = hist_pos.query(\"Date &lt;= @previous_date\").groupby(\"Ticker\").last().reset_index()\n    previous_or_before[\"Date\"] = previous_date\n\n    after_previous = hist_pos.query(\"Date &gt; @previous_date and Date &lt;= @recent_date\")\n    positions = pd.concat([previous_or_before, after_previous])\n    positions[\"Date\"] = pd.to_datetime(positions[\"Date\"])\n\n    return positions\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_positions_and_movements","title":"<code>get_positions_and_movements(fund_name, tickers=None, recent_date=dt.date.today(), previous_date=dt.date.today() - dt.timedelta(days=1), period=None, holiday_location='all', currency='usd', usdbrl_ticker='bmfxclco curncy')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fund_name</code> <code>str</code> required <code>tickers</code> <code>str | list | set</code> <p>Defaults to None.</p> <code>None</code> <code>recent_date</code> <code>date</code> <p>Defaults to dt.date.today().</p> <code>today()</code> <code>previous_date</code> <code>date</code> <p>description. Defaults to dt.date.today()-dt.timedelta(days=1).</p> <code>today() - timedelta(days=1)</code> <code>period</code> <code>str</code> <p>mtd|ytd|3m|6m|...|nm. Defaults to None.</p> <code>None</code> <code>holiday_location</code> <code>str</code> <p>any|bz|us|all. Defaults to \"all\".</p> <code>'all'</code> <p>Returns:</p> Type Description <p>pandas.DataFrame:</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_positions_and_movements(self, fund_name, tickers=None,\n        recent_date=dt.date.today(), previous_date=dt.date.today()-dt.timedelta(days=1),\n        period=None, holiday_location=\"all\", currency=\"usd\", usdbrl_ticker=\"bmfxclco curncy\"):\n\n    \"\"\"\n    Args:\n        fund_name (str): \n        tickers (str|list|set, optional): Defaults to None.\n        recent_date (datetime.date, optional): Defaults to dt.date.today().\n        previous_date (datetime.date, optional): _description_. Defaults to dt.date.today()-dt.timedelta(days=1).\n        period (str, optional): mtd|ytd|3m|6m|...|nm. Defaults to None.\n        holiday_location (str, optional): any|bz|us|all. Defaults to \"all\".\n\n    Returns:\n        pandas.DataFrame: \n    \"\"\"\n\n    df = self.__run_return_analysis(fund_name, tickers=tickers,\n                                recent_date=recent_date, previous_date=previous_date,\n                                period=period, holiday_location=holiday_location,\n                                currency=currency, agregate_by_name=False, usdbrl_ticker=usdbrl_ticker\n                                )\n    # Vamos obter os precos de fechamento em cada dia\n    # Preenchendo primeiro todo o historico\n    prices_previous_date = (df[\"Date\"].min() - dt.timedelta(days=100)).date()\n    price_keys = list(df[\"Price_Key\"].unique())\n    prices = self.get_prices(\n                tickers=price_keys, previous_date=prices_previous_date, currency=currency,\n                get_intraday_prices=True, usdbrl_ticker=usdbrl_ticker\n                ).rename(columns={\"Asset\":\"Price_Key\"})\n\n    df = df.merge(prices, on=[\"Date\", \"Price_Key\"], how=\"left\")\n\n    # Finalmente, vamos atualizar com o preco mais recente do ativo na data de hoje\n    # -&gt; Pode ser substituido pelo uso da flag get_intraday_prices na get_prices\n    # TODO : remover essa parte e testar\n    #df_prev = df.query(\"Date &lt; @dt.date.today()\").copy()\n    #df_today = df.query(\"Date == @dt.date.today()\").copy()\n    #assets = self.get_table(\"assets\").rename(columns={\"Key\":\"Asset_ID\"})\n    #df_today = df_today.merge(assets[[\"Asset_ID\", \"Location\"]], on=\"Asset_ID\")\n    #if len(df_today) &gt; 0:\n    #    df_today[\"Last_Price\"] = df_today.apply(lambda row: self.get_price(\n    #                                        row[\"Price_Key\"], currency=currency,\n    #                                        usdbrl_ticker=\"usdbrl curncy\",\n    #                                        asset_location=row[\"Location\"]), axis=1\n    #                                        )\n    #    df = pd.concat([df_prev, df_today])\n    #else:\n    #    df = df_prev\n\n    df[\"Current_Position_Value\"] = df[\"Open_Quantity\"] * df[\"Last_Price\"]\n\n    df = df[[\"Date\", \"Asset_ID\", \"Price_Key\", \"Open_Quantity\",\n            \"Delta_Shares\", \"Shares_Bought\", \"Shares_Bought_Cost\", \"Shares_Sold\",\n            \"Amount_Sold\", \"Last_Price\", \"Current_Position_Value\"]]\n\n    # preenchendo Last_Price nulos com o do dia anterior para os mesmos asset_id\n    df[\"Last_Price\"] = df.groupby(\"Asset_ID\")[\"Last_Price\"].ffill()\n\n    return df\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_price","title":"<code>get_price(ticker, px_date=None, logger_level='trace', dr_adjusted=False, currency='local', usdbrl_ticker='bmfxclco curncy', asset_location='us')</code>","text":"<p>Informa o pre\u00e7o do ativo. Quando px_date nao eh informado, retorna o pre\u00e7o mais recente.</p> <p>Parameters:</p> Name Type Description Default <code>ticker</code> <code>str</code> <p>identificador do ativo.</p> required <code>px_date</code> <code>date</code> <p>Data de referencia.</p> <code>None</code> <code>logger_level</code> <code>str</code> <p>Defaults to \"trace\".</p> <code>'trace'</code> <p>Returns:</p> Name Type Description <code>float</code> <p>pre\u00e7o do ativo.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_price(self, ticker, px_date=None, logger_level=\"trace\", dr_adjusted=False,\n    currency=\"local\", usdbrl_ticker=\"bmfxclco curncy\", asset_location=\"us\"):\n    \"\"\" Informa o pre\u00e7o do ativo. Quando px_date nao eh informado, retorna o\n    pre\u00e7o mais recente.\n\n    Args:\n        ticker (str): identificador do ativo.\n        px_date (dt.date, optional): Data de referencia.\n        logger_level (str, optional): Defaults to \"trace\".\n\n    Returns:\n        float: pre\u00e7o do ativo.\n    \"\"\"\n\n    if pd.isna(ticker): return None\n\n\n    currency_factor = 1\n    if currency != \"local\":\n        try:\n            if asset_location == \"bz\" and currency == \"usd\":\n                usdbrl = self.get_price(usdbrl_ticker, px_date=px_date)\n                currency_factor = 1/usdbrl\n            elif asset_location == \"us\" and currency == \"brl\":\n                usdbrl = self.get_price(usdbrl_ticker, px_date=px_date)\n                currency_factor = usdbrl\n        except ValueError:\n            logger.error(f\"Erro ao converter moeda para {currency} para o ticker '{ticker}'.\")\n            currency_factor = 1\n\n    ticker = ticker.lower()\n\n    # dados do usd cupom limpo comecam em abril de 2011. Antes disso vamos pergar usdbrl normalmente\n    if (ticker == \"bmfxclco curncy\") and (px_date is not None) and (px_date &lt; dt.date(2011,4,14) or (px_date is not None and px_date == dt.date.today())) :\n        ticker = \"usdbrl curncy\"\n\n    px_last_at_date = None\n\n    cache_key = ticker + str(px_date) # chave unica do preco para essa data\n    # se o preco foi consultado recentemente, podemos retorna-lo rapidamente.\n    if cache_key in self.price_cache:\n        return self.price_cache[cache_key] * currency_factor\n\n    if (px_date is None) or (px_date&gt;= dt.date.today()):\n        # Nesse caso retornamos o mais recente utilizando o dicionario\n\n        if \"px_last\" not in self.tables_in_use:\n            self.__load_table_group([\"px_last\"])\n\n        #if ticker in self.asset_last_prices.keys():\n        try:\n            px_last_at_date = self.asset_last_prices[ticker][\"px_last\"]\n            self.price_cache[cache_key] = px_last_at_date\n        except:\n            price_1_tickers = ['fip mission 1.1', 'caixa', 'caixa us']\n            if ticker in price_1_tickers :\n                px_last_at_date = 1\n                self.price_cache[cache_key] = px_last_at_date\n            else:\n            #if px_last_at_date is None:\n                if logger_level == \"trace\":\n                    logger.trace(f\"Pre\u00e7o nao disponivel para o ticker '{ticker}'. Pre\u00e7o setado para 0.\")\n                elif logger_level == \"info\":\n                    logger.info(f\"Pre\u00e7o nao disponivel para o ticker '{ticker}'. Pre\u00e7o setado para 0.\")\n                else: # logger_level == \"erro\":\n                    logger.error(f\"Pre\u00e7o nao disponivel para o ticker '{ticker}'. Pre\u00e7o setado para 0.\")\n                px_last_at_date = 0\n\n\n        return px_last_at_date * currency_factor\n\n    # Vamos olhar em cada tabela de precos procurando pelo ticker informado\n    if not self.price_tables_loaded:\n        self.__load_table_group([\"hist_px_last\", \"hist_non_bbg_px_last\", \"hist_vc_px_last\",\n                                 \"all_funds_quotas\", \"custom_funds_quotas\", \"custom_prices\"])\n\n\n    try:\n        # Busca otimizada do preco pelo ativo e pela data informada\n        #px_last_at_date = (self.hist_prices_table[\"Last_Price\"]\n        #                    .to_numpy()[(\n        #                            (self.hist_prices_table[\"Date\"].dt.date.to_numpy() &lt;= px_date) \n        #                            &amp; \n        #                            (self.hist_prices_table[\"Asset\"].to_numpy() == ticker) \n        #                            )].item(-1)\n        #                    )\n        #return  px_last_at_date\n\n        px_last_at_date = self.hist_prices_table.query(\"Date &lt;= @px_date and Asset == @ticker\")\n\n        return px_last_at_date.tail(1)[\"Last_Price\"].squeeze() * currency_factor if len(px_last_at_date) &gt; 0 else 0\n\n    except (IndexError , KeyError):\n        # Nao achou o ativo em nenhuma das tabelas, retorna 0\n        if logger_level == \"trace\":\n            logger.trace(f\"Pre\u00e7o nao disponivel para o tikcker '{ticker}'. Pre\u00e7o setado para 0.\")\n        elif logger_level == \"info\":\n            logger.info(f\"Pre\u00e7o nao disponivel para o tikcker '{ticker}'. Pre\u00e7o setado para 0.\")\n        else: # logger_level == \"erro\":\n            logger.error(f\"Pre\u00e7o nao disponivel para o tikcker '{ticker}'. Pre\u00e7o setado para 0.\")\n\n        logger.trace(f\"Pre\u00e7o nao disponivel para o tikcker '{ticker}'. Pre\u00e7o setado para 0.\")\n        self.price_cache[cache_key] = px_last_at_date\n        return 0\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_price_key","title":"<code>get_price_key(asset_key)</code>","text":"<p>Retorna a chave correta a ser usada para consultar o preco/rentabilidade do ativo.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_price_key(self, asset_key):\n    \"\"\" Retorna a chave correta a ser usada para consultar o preco/rentabilidade do ativo.\"\"\"\n\n    asset = self.get_table(\"assets\").query(\"Key == @asset_key\").squeeze()\n\n    if asset[\"Group\"] in [\"vc\", \"luxor\"]:\n        return asset[\"Ticker\"]\n\n    # Verificando se ha valor valido de ticker bbg\n    if type(asset[\"Ticker_BBG\"]) == type(\"\"):\n\n        return asset[\"Ticker_BBG\"]\n    # caso nao haja, sera usado o ticker\n    return asset[\"Ticker\"]\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_prices","title":"<code>get_prices(tickers=None, recent_date=dt.date.today(), previous_date=dt.date.today() - dt.timedelta(days=30), period=None, currency='local', usdbrl_ticker='bmfxclco curncy', force_continuous_date_range=True, holiday_location='all', get_intraday_prices=False, force_month_end=False)</code>","text":"<pre><code>Filtra o historico de pre\u00e7os pelos tickers e pelo periodo informado.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>recent_date</code> <code>date</code> <p>data de fim. Por padrao, data de hoje.</p> <code>today()</code> <code>previous_date</code> <code>date</code> <p>data de inicio. Por padrao, 30 dias antes de hoje.</p> <code>today() - timedelta(days=30)</code> <code>tickers</code> <code>list | set</code> <p>Lista de tickers que devem ser incluidos, quando existirem.</p> <code>None</code> <code>period(str)</code> <p>ytd, mtd ou 'xm' onde 'x' \u00e9 o numero de meses.</p> required <code>currency(str)</code> <p>codigo de 3 caracteres da moeda ou 'all' para considerar a moeda local de cada ativo.</p> required <code>period(str)</code> <p>ytd|mtd|'xm' onde 'x' \u00e9 o numero de meses. Usara como base o parametro 'recent_date'</p> required <code>holiday_location(str)</code> <p>all|any|us|bz ... ver metodo 'is_holiday'</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: Tabela de precos filtrada e convertida para moeda desejada</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_prices(self, tickers=None, recent_date=dt.date.today(), previous_date=dt.date.today()-dt.timedelta(days=30),\n                     period=None, currency=\"local\", usdbrl_ticker=\"bmfxclco curncy\", force_continuous_date_range=True,\n                     holiday_location=\"all\", get_intraday_prices=False, force_month_end=False):\n    \"\"\"\n        Filtra o historico de pre\u00e7os pelos tickers e pelo periodo informado.\n\n    Args:\n        recent_date (dt.date, optional): data de fim. Por padrao, data de hoje.\n        previous_date (dt.date, optional): data de inicio. Por padrao, 30 dias antes de hoje.\n        tickers (list|set, optional): Lista de tickers que devem ser incluidos, quando existirem.\n        period(str): ytd, mtd ou 'xm' onde 'x' \u00e9 o numero de meses.\n        currency(str): codigo de 3 caracteres da moeda ou 'all' para considerar a moeda local de cada ativo.\n        period(str): ytd|mtd|'xm' onde 'x' \u00e9 o numero de meses. Usara como base o parametro 'recent_date'\n        holiday_location(str): all|any|us|bz ... ver metodo 'is_holiday'\n\n    Returns:\n        pd.DataFrame: Tabela de precos filtrada e convertida para moeda desejada\n    \"\"\"\n    if period is not None:\n        previous_date = self.get_start_period_dt(recent_date, period, holiday_location=holiday_location, force_bday=True,\n                                                 force_month_end=force_month_end)\n\n    if recent_date &lt; previous_date:\n        logger.warning(\"Possivel inversao dos parametros de inicio e fim do periodo.\")\n        temp = recent_date\n        recent_date = previous_date\n        previous_date = temp\n\n    if not self.price_tables_loaded:\n        try:\n            self.__load_table_group([\"hist_px_last\", \"hist_non_bbg_px_last\", \"hist_vc_px_last\", \"all_funds_quotas\",\n                                 \"custom_funds_quotas\", \"custom_prices\", \"px_last\"])\n        except FileNotFoundError:\n            # Vamos tentar sem o custom_funds_quotas, pois pode nao existir ainda.\n            self.__load_table_group([\"hist_px_last\", \"hist_non_bbg_px_last\", \"hist_vc_px_last\",\n                                     \"all_funds_quotas\", \"custom_prices\", \"px_last\"])\n\n    if tickers is None:\n        #prices = self.hist_prices_table.query(\"Date &lt;= @recent_date and Date &gt;= @previous_date\")\n        # Nesse caso, tickers sera uma lista com todos os ativos da tabela de precos\n        tickers = self.hist_prices_table[\"Asset\"].unique()\n\n    if isinstance(tickers, str):\n            tickers = [tickers]\n\n    tickers = [t.lower() for t in tickers] # padronizando tickers para lowercase\n\n    prices = self.hist_prices_table.copy()\n\n\n    surrogate_previous_date = previous_date - dt.timedelta(days=30)\n    surrogate_recent_date = recent_date + dt.timedelta(days=30)\n    prices = prices.query(\"Date &lt;= @surrogate_recent_date and Date &gt;= @surrogate_previous_date\\\n                               and Asset.isin(@tickers)\")\n\n    if force_continuous_date_range:\n        # Vamos ajustar as datas logo aqui, caso flag esteja ativa\n        prices = prices.set_index(\"Date\").groupby(\"Asset\")\\\n                        .resample(\"D\").last().ffill()\\\n                        .reset_index(level=0, drop=True).reset_index()\n\n    if get_intraday_prices:\n        prices = self.__hist_prices_intraday_extensor(prices)\n\n    if currency != \"local\":\n\n        # TODO:  Resolver problema de consistencia com ticker e ticker_bbg \n\n        assets= self.get_table(\"assets\").copy().query(\"Type != 'a\u00e7\u00f5es_bdr' and Asset != 'spx eagle'\")\n\n        ticker_map = assets.query(\"~Ticker_BBG.isna() and Ticker_BBG != Ticker\")[[\"Ticker\", \"Ticker_BBG\"]].set_index(\"Ticker\").to_dict(\"index\")\n        assets[\"Ticker\"] = assets[\"Ticker\"].apply(lambda x: ticker_map[x][\"Ticker_BBG\"] if x in ticker_map.keys() else x)\n\n        prices = pd.merge(assets[[\"Ticker\", \"Location\"]], prices, left_on=\"Ticker\", right_on=\"Asset\")[[\"Date\", \"Asset\", \"Last_Price\", \"Location\"]]\n        currency_map = {\"bz\":\"brl\", \"us\":\"usd\", \"cn\":\"cad\", \"eur\":\"eur\"}\n        prices[\"Location\"] = prices[\"Location\"].apply(lambda x: currency_map[x])\n\n        prices_by_location = []\n        iter_prices = prices.groupby(\"Location\")\n\n        for p in iter_prices:\n            price_currency = p[0]\n            prices_by_location.append(self.convert_currency(p[1][list(set(p[1].columns) - {\"Location\"})], \n                                                            price_currency=price_currency, dest_currency=currency,\n                                                            usdbrl_ticker=usdbrl_ticker, force_continuous_date_range=force_continuous_date_range,\n                                                            holiday_location=holiday_location, get_intraday_prices=get_intraday_prices,\n                                                            force_month_end=force_month_end))\n\n\n        prices = pd.concat(prices_by_location)[[\"Date\", \"Asset\", \"Last_Price\"]].sort_values(by=[\"Asset\", \"Date\"])\n\n    # Finalmente, vamos seguir filtrando pelo periodo desejado.\n    prices = prices.query(\"Date &lt;= @recent_date and Date &gt;= @previous_date and Asset.isin(@tickers)\")\n\n    #if adjust_bmfxlcoc and ('bmfxclco curncy' in tickers) and (recent_date == dt.date.today()):\n    #    max_date = prices.query(\"Asset == 'bmfxclco curncy'\")[\"Date\"].max()\n    #    if max_date &lt; dt.date.today(): # vamos colocar mais um dia usando usdbrl curncy\n    #        var_usdbrl1d = self.get_pct_change('usdbrl curncy', \n    #                                           recent_date=dt.date.today(),\n    #                                           previous_date=max_date)\n    #        # vamos pegar o last_price em max_date\n    #        last_price = self.get_price('bmfxclco curncy', px_date=max_date)\n    #        # vamos ajustar com a variacao do usdbrl\n    #        last_price = last_price * (1 + var_usdbrl1d)\n    #        #vamos colocar na base na data de hoje\n    #        prices = pd.concat([prices, \n    #                    pd.DataFrame({\"Date\":[dt.date.today()],\n    #                                  \"Asset\":['bmfxclco curncy'], \"Last_Price\":[last_price]})])\n\n    return prices\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_px_update_time","title":"<code>get_px_update_time()</code>","text":"<p>Informa Hor\u00e1rio da ultima atualizacao da base de precos. Returns:     dt.datetime</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_px_update_time(self):\n    \"\"\" Informa Hor\u00e1rio da ultima atualizacao da base de precos.\n    Returns:\n        dt.datetime\n    \"\"\"\n    time = self.get_table(\"px_last\")[\"Query_Time\"].max()\n    return dt.time(time.hour, time.minute, time.second)\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_risk_metric","title":"<code>get_risk_metric(fund_name, metrics=None, date=dt.date.today())</code>","text":"<pre><code>Retorna as metricas de risco numa determinada data.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>fund_name</code> <code>str</code> <p>nome do fundo correspontente</p> required <code>metrics</code> <code>str, list(str</code> <p>str da metrica desejada ou uma lista de strings para mais de uma metrica. Defaults to None (all available).</p> <code>None</code> <code>date</code> <code>date</code> <p>Data da metrica. Defaults to datetime.date.today().</p> <code>today()</code> <p>Returns:</p> <pre><code>Pode retornar o valor de uma metrica especifica ou Dataframe quando \nmetrics passado for uma lista de metricas.\n</code></pre> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_risk_metric(self, fund_name,  metrics=None, date=dt.date.today()):\n    \"\"\"\n        Retorna as metricas de risco numa determinada data.\n\n    Args:\n        fund_name (str): nome do fundo correspontente\n        metrics (str, list(str)): str da metrica desejada ou uma lista de strings\n         para mais de uma metrica. Defaults to None (all available).\n        date (datetime.date, optional): Data da metrica. Defaults to datetime.date.today().\n\n    Returns:\n\n        Pode retornar o valor de uma metrica especifica ou Dataframe quando \n        metrics passado for uma lista de metricas.\n\n    \"\"\"\n    fund_name = fund_name.replace(\"_\", \" \")\n\n    hist_metrics = self.get_table(\"hist_risk_metrics\")\n    hist_metrics = hist_metrics.loc[((hist_metrics[\"Fund\"] == fund_name) &amp; (hist_metrics[\"Date\"].dt.date &lt;= date) )].tail(1).reset_index(drop=True)\n\n    if metrics is None:\n        metrics = list(hist_metrics.columns)\n        metrics.remove(\"Date\")\n        metrics.remove(\"Fund\")\n\n    try:\n        if type(metrics) == list:\n\n            return hist_metrics[metrics].astype(float)\n\n        return float(hist_metrics[metrics].squeeze())\n    except KeyError:\n        logger.error(f\"Metrica(s) {metrics} indisponivel(is)\")\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_start_period_dt","title":"<code>get_start_period_dt(ref_date, period, force_bday=False, holiday_location='all', force_month_end=False)</code>","text":"<p>A partir da data informada e do periodo, retorna a data de inicio do periodo.</p> <p>Parameters:</p> Name Type Description Default <code>ref_date</code> <code>date</code> <p>A data de referencia</p> required <code>period</code> <code>str</code> <p>O periodo em questao dado em meses, ou ytd ou mtd. Ex.: '12m', '6m', 'ytd', 'mtd', '24m'</p> required <code>force_bday</code> <code>bool</code> <p>Determina se a data retorna devera ser dia util</p> <code>False</code> <code>holiday_location</code> <code>str</code> <pre><code>                \"bz\",\"us\" -&gt; considerar feriados num local especifico\n\n                \"all\" -&gt; considerar feriados globais apenas\n\n                \"any\" -&gt; considerar feriado em qualquer localidade (dentre bz e us)\n</code></pre> <code>'all'</code> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_start_period_dt(self, ref_date, period, force_bday=False, holiday_location=\"all\",\n                        force_month_end=False):\n    \"\"\" A partir da data informada e do periodo, retorna a data de inicio do periodo.\n\n    Args:\n        ref_date (dt.date): A data de referencia\n        period (str): O periodo em questao dado em meses, ou ytd ou mtd. Ex.: '12m', '6m', 'ytd', 'mtd', '24m'\n        force_bday (bool): Determina se a data retorna devera ser dia util\\n\n        holiday_location (str): \\n\n                                \"bz\",\"us\" -&gt; considerar feriados num local especifico\\n\n                                \"all\" -&gt; considerar feriados globais apenas\\n\n                                \"any\" -&gt; considerar feriado em qualquer localidade (dentre bz e us)\n    \"\"\"\n    period = period.lower()\n    start_date = None\n    if period[-1] == \"m\": \n\n        n_months = int(period.split(\"m\")[0])\n\n        if force_month_end:\n            start_date = ref_date - pd.DateOffset(months=n_months)\n            start_date = start_date.date()\n            start_date = self.get_month_end(start_date)\n        else:\n\n            is_leap_date = ((ref_date.month == 2) and (ref_date.day == 29))\n\n            if is_leap_date:\n                ref_date = ref_date-dt.timedelta(days=1)\n\n\n            year_offset = n_months//12 # obtendo o numero de anos inteiros no periodo informado\n            month_offset = n_months%12 # obtendo o numero de anos parciais \n\n            start_date = dt.date(ref_date.year-year_offset, ref_date.month, ref_date.day)\n            start_date = start_date - month_offset * dt.timedelta(days=30)\n\n            if is_leap_date:\n                start_date = start_date + dt.timedelta(days=10) # forcando avanco ao mes seguinte\n                # Retornando ao ultimo dia do mes anterior\n                start_date = dt.date(start_date.year, start_date.month, 1)-dt.timedelta(days=1)\n\n\n    elif period == \"ytd\":\n        start_date = dt.date(ref_date.year-1, 12, 31)\n\n    elif period == \"mtd\":\n        start_date = dt.date(ref_date.year, ref_date.month , 1) - dt.timedelta(days=1)\n\n    # A partir de uma data pegar o trimestre anterior\n    elif period == 'qtr':\n        # Deve retornar sempre a ultima data do final do trimestre anterior\n        current_quarter = (ref_date.month - 1) // 3 + 1\n        start_of_current_quarter = dt.date(ref_date.year, (current_quarter - 1) * 3 + 1, 1)\n        start_date = start_of_current_quarter - dt.timedelta(days=1) # Last day of previous quarter\n\n        #start_date = ref_date - pd.DateOffset(months=3) \n        #start_date = start_date.date()\n        #start_date = self.get_month_end(start_date)\n\n    if force_bday:\n        start_date = self.get_bday_offset(start_date, offset=0, location=holiday_location)\n\n\n    return start_date\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.get_table","title":"<code>get_table(table_name, index=False, index_name='index', dtypes_override={}, force_reload=False)</code>","text":"<p>Retorna uma copia do DataFrame do 'table_name' correspondente. Se n\u00e3o estiver disponivel, retorna None.</p> table_name <p>'px_last'              - \u00daltimo preco dos ativos</p> <p>'trades'               - Historico de boletas dos trades da Luxor</p> <p>'assets'               - Tabela de ativos validos</p> <p>'hist_px_last'         - Pre\u00e7o hist\u00f3rico dos ativos desde 1990</p> <p>'hist_vc_px_last'      - Pre\u00e7o hist\u00f3rico dos VCs</p> <p>'hist_non_bbg_px_last' - Pre\u00e7o hist\u00f3rico de ativos sem preco no bbg (fidc trybe, spx hawker)</p> <p>'[NOME_FUNDO]_quotas   - Cotas historicas do fundo (fund_a, fund_b, hmx, ...</p> <p>'hist_us_cash'         - Historico de caixas dos fundos (us apenas)</p> <p>'bbg_tickers'          - Ticker bbg de todos os tickers cadastrados</p> <p>'bdr_sizes'            - Tabela com o ultimo peso de cada bdr</p> <p>'hist_swap'            - Tabela historica dos swaps</p> <p>'hist_positions'       - Historico de posicoes dos fundos</p> <p>'hist_positions_by_bank- Posicoes por banco</p> <p>'cash_movements'       - Historico de movimentacoes com data de liquidacao</p> <p>'holidays'             - Tabela de feriados das bolsas</p> <p>'daily_pnl'            - Tabela de PnL diario</p> <p>'custom_funds_quotas'  - Cotas dos fundos customizados(luxor equities, non-equities, etc)</p> dict : set - Dicionario com os tipos de dados das colunas devem ser sobrescritos. <p>Deve possuir as chaves 'float', 'date', 'bool' e 'str_nan_format'(troca 'nan' por pd.NA)     Para cada chave, colocar um Set com os nomes das colunas que receberao o cast.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def get_table(self, table_name, index=False, index_name=\"index\", dtypes_override={}, force_reload=False):\n    \"\"\"\n        Retorna uma copia do DataFrame do 'table_name' correspondente. Se n\u00e3o estiver disponivel,\n        retorna None.\n\n        table_name: \n            'px_last'              - \u00daltimo preco dos ativos\\n\n            'trades'               - Historico de boletas dos trades da Luxor\\n\n            'assets'               - Tabela de ativos validos\\n\n            'hist_px_last'         - Pre\u00e7o hist\u00f3rico dos ativos desde 1990\\n\n            'hist_vc_px_last'      - Pre\u00e7o hist\u00f3rico dos VCs\\n\n            'hist_non_bbg_px_last' - Pre\u00e7o hist\u00f3rico de ativos sem preco no bbg (fidc trybe, spx hawker)\\n\n            '[NOME_FUNDO]_quotas   - Cotas historicas do fundo (fund_a, fund_b, hmx, ...\\n\n            'hist_us_cash'         - Historico de caixas dos fundos (us apenas)\\n\n            'bbg_tickers'          - Ticker bbg de todos os tickers cadastrados\\n\n            'bdr_sizes'            - Tabela com o ultimo peso de cada bdr\\n\n            'hist_swap'            - Tabela historica dos swaps\\n\n            'hist_positions'       - Historico de posicoes dos fundos\\n\n            'hist_positions_by_bank- Posicoes por banco\\n\n            'cash_movements'       - Historico de movimentacoes com data de liquidacao\\n\n            'holidays'             - Tabela de feriados das bolsas\\n\n            'daily_pnl'            - Tabela de PnL diario\\n\n            'custom_funds_quotas'  - Cotas dos fundos customizados(luxor equities, non-equities, etc)\\n\n\n        dtypes_override: dict : set - Dicionario com os tipos de dados das colunas devem ser sobrescritos.\n            Deve possuir as chaves 'float', 'date', 'bool' e 'str_nan_format'(troca 'nan' por pd.NA)\n                Para cada chave, colocar um Set com os nomes das colunas que receberao o cast.\n    \"\"\"\n    table_name = table_name.lower().replace(\" \", \"_\")\n    if table_name == 'bbg_tickers': return self.__get_tickers_bbg() # DEPRECATED TODO: remover apos testes\n\n    if (table_name in self.tables_in_use) and not force_reload:\n        return self.tables_in_use[table_name][\"table_data\"]\n\n    return self.__load_table(table_name, index=index, index_name=index_name, dtypes_override=dtypes_override)\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.is_holiday","title":"<code>is_holiday(date, location)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> required <code>location</code> <code>str</code> <pre><code>'any'-&gt; se \u00e9 feriado em qualquer um dos lugares cadastrados\n\n'all' -&gt; se \u00e9 feriado em todas as localidades da tabela\n\n'us', 'bz', (...) -&gt; para local especifico, com codigo presente na tabela\n</code></pre> required <p>Returns:</p> Name Type Description <code>bool</code> <p>se \u00e9 ou nao feriado</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def is_holiday(self, date,  location):\n    \"\"\"\n    Args:\n        date (dt.date): \n        location (str): \n                'any'-&gt; se \u00e9 feriado em qualquer um dos lugares cadastrados\\n\n                'all' -&gt; se \u00e9 feriado em todas as localidades da tabela\\n\n                'us', 'bz', (...) -&gt; para local especifico, com codigo presente na tabela\\n\n    Returns:\n        bool: se \u00e9 ou nao feriado\n    \"\"\"\n    holidays = self.get_table(\"holidays\")\n\n    if location != \"any\" and location != \"all\":\n        holidays = holidays.loc[holidays[\"Location\"] == location]\n    if location == \"all\":\n        n_locations = len(holidays[\"Location\"].unique())\n        holidays = (holidays[\"Date\"]\n                            .value_counts().reset_index()\n                            .query(\"count == @n_locations\")\n                            )\n\n    return date in set(holidays[\"Date\"].dt.date)\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.list_blob_files","title":"<code>list_blob_files(container, sub_dir, ends_with=None)</code>","text":"<p>Lista todos os arquivos dentro de um diretorio no blob storage.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def list_blob_files(self, container, sub_dir, ends_with=None):\n    \"\"\"\n        Lista todos os arquivos dentro de um diretorio no blob storage.\n    \"\"\"\n\n    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n\n    # Vamos listar os arquivos do diretorio\n    blob_files = blob_service_client\\\n                .get_container_client(container)\\\n                .list_blobs(name_starts_with=sub_dir)\n\n    if ends_with is not None:\n        return [blob_file.name for blob_file in blob_files if blob_file.name.endswith(ends_with)]\n    return [blob_file.name for blob_file in blob_files]\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.normalize_price_key","title":"<code>normalize_price_key(df)</code>","text":"<pre><code>Adiciona a coluna 'Price_ID' no df informado representando uma chave normalizada para o preco do ativo desejado.\n</code></pre> <p>Args:     df (pandas.DataFrame): DataFrame que obrigatoriamente precisa conter 'Asset' e 'Ticker' como colunas.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def normalize_price_key(self, df):\n    \"\"\"\n        Adiciona a coluna 'Price_ID' no df informado representando uma chave normalizada para o preco do ativo desejado.\n    Args:\n        df (pandas.DataFrame): DataFrame que obrigatoriamente precisa conter 'Asset' e 'Ticker' como colunas. \n    \"\"\"\n\n    df_columns = list(df.columns)\n\n    assets = self.get_table(\"assets\")[[\"Key\", \"Asset\", \"Ticker\", \"Ticker_BBG\",]].rename(columns={\"Key\":\"Asset_ID\"})\n\n    assets[\"Price_Key\"] = assets[\"Asset_ID\"].apply(lambda x: self.get_price_key(x))\n    asset_price_key_map = {\"etf s&amp;p\" : 'voo us equity', \"spx hawker\": \"spx hawker usd\", \"berkshire\" : \"brk/b us equity\",\n                           \"localiza\" : \"rent3 bz equity\", \"suzano\":\"suzb3 bz equity\"}\n    assets[\"Price_Key\"] = assets.apply(lambda row: asset_price_key_map[row[\"Asset\"]] if row[\"Asset\"] in asset_price_key_map else row[\"Price_Key\"], axis=1)\n\n    df = df.rename(columns={\"Price_Key\": \"Price_Old_key\"})\n    df = pd.merge(df, assets[[\"Asset_ID\", \"Price_Key\"]], on=\"Asset_ID\")\n\n    # Casos padroes |-&gt; tratados na tabela asset, usando o get_price_key. Casos especificos adicionar no asset_price_key_map.\n    #df[\"Price_Key\"] = np.where((df[\"Ticker_BBG\"] is None) or (df[\"Ticker_BBG\"].isnull()), df[\"Ticker\"], df[\"Ticker_BBG\"])\n\n    if \"Price_Key\" not in df_columns: df_columns.append(\"Price_Key\")\n\n    return df[df_columns]\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.normalize_trades","title":"<code>normalize_trades(trades, currency, asset_id_columns='Asset_ID', usdbrl_ticker='bmfxclco curncy')</code>","text":"<p>Para um dado dataframe de trades, converte para moeda indicada.     Trades devem conter as colunas Asset, Ticker e Delta_Shares.     BDR: Usa o peso do BDR e a quantidade de BDR operada para chegar na quantidade do ativo original.     Financeiro: Corrigido pelo cambio de fechamento.</p> <p>Parameters:</p> Name Type Description Default <code>trades</code> <code>DataFrame</code> <p>dataframe de trades, mais especificamente o output             de get_positions_and_movements.</p> required <code>currency</code> <code>str</code> <p>brl; usd; all -&gt; todas as moedas, no caso usd e brl</p> required Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def normalize_trades(self, trades, currency, asset_id_columns=\"Asset_ID\", usdbrl_ticker=\"bmfxclco curncy\"):\n    \"\"\" Para um dado dataframe de trades, converte para moeda indicada.\n        Trades devem conter as colunas Asset, Ticker e Delta_Shares.\n        BDR: Usa o peso do BDR e a quantidade de BDR operada para chegar na quantidade do ativo original.\n        Financeiro: Corrigido pelo cambio de fechamento.\n\n    Args:\n        trades (pd.DataFrame): dataframe de trades, mais especificamente o output\n                        de get_positions_and_movements.\n        currency (str): brl; usd; all -&gt; todas as moedas, no caso usd e brl\n    \"\"\"\n    assert(currency in [\"usd\", \"brl\"])\n    previous_date=trades[\"Date\"].min()\n    recent_date=trades[\"Date\"].max()\n\n    assets = self.get_table(\"assets\")\n\n    # tratando inconsistencia da Location do spx hawker\n    spx_hawker_brl_tickers = [\n        \"SPX SEG HAWKER JAN22\", \"SPX SEG HAWKER FEB22\", \"SPX HAWKER CL AMAR22\",\n        \"SPX SEG HAWKER APR22\", \"SPX HAWKER CL ASET18\", \"SPX SEG HAWKER JUN23\", \"SPX SEG HAWKER SEP23\"]\n    spx_hawker_brl_tickers = list(map(str.lower, spx_hawker_brl_tickers)) # colocando tudo para minusculo\n    assets[\"Location\"] = np.where(assets[\"Ticker\"].isin(spx_hawker_brl_tickers), \"bz\", assets[\"Location\"])\n\n\n    # Salvando os nomes das colunas antes de editar a tabela.\n    trades_columns = list(trades.columns)        \n\n\n    # Adicionando algumas colunas que vamos precisar para conseguir distinguir os ativos\n    trades = pd.merge(trades, assets[[\"Key\", \"Asset\", \"Ticker\", \"Type\", \"Location\", \"Ticker_BBG\"]], left_on=\"Asset_ID\", right_on=\"Key\")\n\n    # Achando quantidade do trade no ativo original -&gt; usando peso do bdr \n    bdr_sizes = self.get_table(\"bdr_sizes\", index=True, index_name=\"Ticker\").reset_index()\n    trades = pd.merge(trades, bdr_sizes, on=\"Ticker\", how=\"left\").rename(columns={\"adr_adr_per_sh\":\"BDR_Size\"})\n    # Achando quantidade do trade no ativo original -&gt; usando peso do bdr \n    # -&gt; Partindo da premissa que sempre atualizamos o historico quando ha alteracao do peso (inplit/split do bdr)\n    trades[\"Delta_Shares\"] = np.where(trades[\"Type\"] == 'a\u00e7\u00f5es_bdr', trades[\"Delta_Shares\"] / trades[\"BDR_Size\"], trades[\"Delta_Shares\"])\n    # Ajustando quantidades dos trades de localiza proporcionalmente:\n    #lcam_rent_ratio = 0.4388444  #0.44 rent3 para cada lcam3\n    #lzrfy_rent_ratio = 1 # 1 rent para cada lzrfy\n    #trades[\"Delta_Shares\"] = np.where(trades[\"Ticker\"] == 'lcam3 bz equity', trades[\"Delta_Shares\"] * lcam_rent_ratio, trades[\"Delta_Shares\"])\n    # Ajuste da lzrfy eh desnecessario ja que eh 1:1\n    #trades[\"Delta_Shares\"] = np.where(trades[\"Ticker\"] == 'lzrfy us equity', trades[\"Delta_Shares\"] * lzrfy_rent_ratio, trades[\"Delta_Shares\"])\n\n    #brkA_brkB_ratio = 1500\n    #trades[\"Delta_Shares\"] = np.where(trades[\"Ticker\"] == 'brk/a us equity', trades[\"Delta_Shares\"] * brkA_brkB_ratio, trades[\"Delta_Shares\"])\n\n    # Adicionando coluna de usdbrl de fechamento em cada dia\n    hist_usdbrl = self.get_prices(usdbrl_ticker, previous_date=previous_date, recent_date=recent_date).rename(columns={\"Last_Price\": \"USDBRL\"})\n\n    trades = pd.merge(trades, hist_usdbrl[[\"Date\", \"USDBRL\"]], on=\"Date\")\n\n    # Criando colunas com financeiro normalizado.\n    if currency == 'usd':\n        trades[\"Trade_Amount\"] = np.where(trades[\"Location\"] == 'bz', trades[\"Trade_Amount\"]/trades[\"USDBRL\"], trades[\"Trade_Amount\"])\n\n    elif currency == 'brl':\n        trades[\"Trade_Amount\"] = np.where(trades[\"Location\"] == 'us', trades[\"Trade_Amount\"]*trades[\"USDBRL\"], trades[\"Trade_Amount\"])\n\n    return trades[trades_columns]\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.simulate_portfolio_performance","title":"<code>simulate_portfolio_performance(portfolio, portfolio_date, adm_fee, performance_fee=0)</code>","text":"<p>Simula o desempenho de um portf\u00f3lio com base em um dicion\u00e1rio de ativos e suas aloca\u00e7\u00f5es iniciais.</p> <p>Par\u00e2metros: - portfolio: dicion\u00e1rio com os ativos e suas aloca\u00e7\u00f5es iniciais. Dicion\u00e1rio deve seguir o formato:</p> <pre><code>{\n    \"ticker1\": peso1,\n    \"ticker2\": peso2,\n    ...\n    \"tickerN\": pesoN\n}, onde os pesos devem somar 1.\n</code></pre> <ul> <li>portfolio_date: data inicial do portf\u00f3lio.</li> <li>adm_fee: taxa de administra\u00e7\u00e3o %.a.a</li> <li>performance_fee: taxa de performance (opcional).</li> </ul> <p>Retorna: - DataFrame com o fator de corre\u00e7\u00e3o da cota para cada dia a partir da data inicial do portfolio.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def simulate_portfolio_performance(self, portfolio: dict, portfolio_date: dt.date, adm_fee: float, performance_fee: float = 0):\n        \"\"\"\n        Simula o desempenho de um portf\u00f3lio com base em um dicion\u00e1rio de ativos e suas aloca\u00e7\u00f5es iniciais.\n\n        Par\u00e2metros:\n        - portfolio: dicion\u00e1rio com os ativos e suas aloca\u00e7\u00f5es iniciais. Dicion\u00e1rio deve seguir o formato:\\n\n            {\n                \"ticker1\": peso1,\n                \"ticker2\": peso2,\n                ...\n                \"tickerN\": pesoN\n            }, onde os pesos devem somar 1.\n        - portfolio_date: data inicial do portf\u00f3lio.\n        - adm_fee: taxa de administra\u00e7\u00e3o %.a.a\n        - performance_fee: taxa de performance (opcional).\n\n        Retorna:\n        - DataFrame com o fator de corre\u00e7\u00e3o da cota para cada dia a partir da data inicial do portfolio.\n        \"\"\"\n\n        # Formatar os tickers para minusculas\n        portfolio = {k.lower(): v for k, v in portfolio.items()}\n\n        initial_portfolio = {\n            \"date\" : portfolio_date,\n            \"assets\" : portfolio,\n        }\n\n        # Criando dataframe com as colunas  Date|Ticker|Weight|\n        positions = pd.DataFrame(initial_portfolio[\"assets\"].items(), columns=[\"Ticker\", \"Weight\"])\n        positions[\"Date\"] = initial_portfolio[\"date\"]\n        positions[\"Returns\"] = 0\n        positions[\"Daily_Attribution\"] = 0.0\n        positions[\"Daily_Portfolio_Return\"] = 0.0\n\n        tickers = positions[\"Ticker\"].tolist()\n\n        max_date = portfolio_date\n\n        while max_date &lt; dt.date.today():\n            # Adicionando 1 dia\n            new_date = max_date + dt.timedelta(days=1)\n            daily_returns = self.get_pct_changes(tickers=tickers, previous_date=max_date, recent_date=new_date, currency=\"usd\")\n\n            daily_returns.index.name = \"Ticker\"\n\n            new_day = positions.query(\"Date == @max_date\").copy().set_index(\"Ticker\")\n            new_day[\"Returns\"] = daily_returns\n            new_day = new_day.reset_index()\n            # Ajusta pesos considerando a atribuicao de retorno do dia anterior\n            new_day[\"Weight\"] = new_day[\"Weight\"] + new_day[\"Daily_Attribution\"]\n            new_day[\"Date\"]  = new_date\n\n            new_day[\"Daily_Attribution\"] = new_day[\"Weight\"] * new_day[\"Returns\"]\n            new_day[\"Daily_Portfolio_Return\"] = new_day[\"Daily_Attribution\"].sum()\n\n            positions = pd.concat([positions, new_day])\n            max_date = new_date\n\n        positions[\"Date\"] = pd.to_datetime(positions[\"Date\"])\n        #positions.to_excel(\"positions_tci.xlsx\")\n        daily_returns = positions[[\"Date\", \"Daily_Portfolio_Return\"]].groupby(\"Date\").last()\n\n        daily_returns[\"Acc_Returns\"] = (1 + daily_returns[\"Daily_Portfolio_Return\"]).cumprod()\n        daily_returns[\"Acc_Returns_Adjusted_by_Taxes\"] = np.where(daily_returns[\"Acc_Returns\"] &gt; 1,\n                                                        ((daily_returns[\"Acc_Returns\"] -1 ) * (1-performance_fee) + 1) - adm_fee/12,\n                                                        daily_returns[\"Acc_Returns\"] - adm_fee/12\n                                                        )\n\n        return daily_returns.reset_index()[[\"Date\", \"Acc_Returns_Adjusted_by_Taxes\"]].rename(columns={\"Acc_Returns_Adjusted_by_Taxes\": \"Factor\"})\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.text_to_lowercase","title":"<code>text_to_lowercase(t)</code>","text":"<p>Converte todas as colunas de texto para lowercase Args:     t (dt.DataFrame): pandas DataFrame Returns:     dt.DataFrame</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def text_to_lowercase(self, t):\n    \"\"\"\n    Converte todas as colunas de texto para lowercase\n    Args:\n        t (dt.DataFrame): pandas DataFrame\n    Returns:\n        dt.DataFrame\n    \"\"\"\n    try:\n        return t.map(lambda x: x.lower().strip() if isinstance(x, str) else x)\n    except AttributeError:\n        logger.warning(\"Pendente de atualizacao para o python 3.12.2\")\n        return t.applymap(lambda x: x.lower().strip() if isinstance(x, str) else x)\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.update","title":"<code>update(update_attempts_limit=8)</code>","text":"<p>Atualiza todas as tabelas em uso.</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def update(self, update_attempts_limit=8):\n    \"\"\"\n        Atualiza todas as tabelas em uso.\n    \"\"\"\n\n    update_attempts = 0\n    update_success = False\n    self.price_cache = {}  # -&gt; reset da otimizacao da consulta de preco \n\n    while not update_success and (update_attempts &lt; update_attempts_limit):\n        try:\n            update_attempts += 1\n            for table_key in self.tables_in_use:\n                # Verificando se tabela foi criada ou modificada\n                if self.__is_table_modified(table_key):                        \n                    self.__load_table_group([table_key])\n\n\n            update_success = True\n\n        except PermissionError:\n            hist_prices_tables = [] # desconsidera appends feitos no loop nao concluido\n            logger.error(\"N\u00e3o foi poss\u00edvel carregar as tabelas pois tem algum arquivo aberto.\")\n            logger.info(f\"Tentativas de atualiza\u00e7\u00e3o: {update_attempts} de {update_attempts_limit}\")\n            time.sleep(30)\n\n        except:\n            logger.error(\"N\u00e3o foi possivel carregar as tabelas.\")\n            logger.info(f\"Tentativas de atualiza\u00e7\u00e3o: {update_attempts} de {update_attempts_limit}\")\n            time.sleep(5*update_attempts)\n\n    if not update_success:\n        logger.critical(\"Nao foi possivel atualizar os dados. Execu\u00e7\u00e3o finalizada.\")\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.usdbrl_clean_coupon_fix","title":"<code>usdbrl_clean_coupon_fix(usdbrl_df)</code>","text":"<p>Corrige o problema do ticker bmfxclco curncy nao ter dados intraday.     Usa a variacao intraday do usdbrl curncy Args:     usdbrl_df (pd.DataFrame): dataframe com precos historicos do bmfxclco curncy</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def usdbrl_clean_coupon_fix(self, usdbrl_df):\n    \"\"\" Corrige o problema do ticker bmfxclco curncy nao ter dados intraday.\n        Usa a variacao intraday do usdbrl curncy\n    Args:\n        usdbrl_df (pd.DataFrame): dataframe com precos historicos do bmfxclco curncy\n    \"\"\"\n    max_date = usdbrl_df[\"Date\"].max().date()\n    today = dt.date.today()\n    if max_date &lt; today:\n        # vamos pegar a variacao do usdbrl curncy\n        var_usdbrl = self.get_pct_change('usdbrl curncy', \n                                           recent_date=today,\n                                           previous_date=max_date)\n        # vamos pegar o last_price em max_date\n        last_price = usdbrl_df.query(\"Date == @max_date\")[\"Last_Price\"].squeeze()\n        # vamos ajustar com a variacao do usdbrl\n        last_price = last_price * (1 + var_usdbrl)\n        #vamos colocar na base na data de hoje\n        usdbrl_df = pd.concat([usdbrl_df, \n                    pd.DataFrame({\"Date\":[today],\n                                  \"Asset\":['bmfxclco curncy'], \"Last_Price\":[last_price]})])\n\n        usdbrl_df[\"Date\"] = pd.to_datetime(usdbrl_df[\"Date\"])\n\n    return usdbrl_df\n</code></pre>"},{"location":"modules/luxorDB_datareader/#luxorDB_datareader.LuxorQuery.xirr","title":"<code>xirr(cashflows, dates)</code>","text":"<p>Calcula o XIRR para uma serie de cash flows em datas especificas.</p> <p>:cashflows (numpy.array of float): cash flows (positive para entrada, negativo para saida) :dates (numpy.array of datetime64): datas correspondentes aos cash flows :return: XIRR</p> Source code in <code>LuxorASAP\\luxorDB_datareader.py</code> <pre><code>def xirr(self, cashflows, dates):\n    \"\"\"\n        Calcula o XIRR para uma serie de cash flows em datas especificas.\n\n        :cashflows (numpy.array of float): cash flows (positive para entrada, negativo para saida)\n        :dates (numpy.array of datetime64): datas correspondentes aos cash flows\n        :return: XIRR\n    \"\"\"\n    # Garantindo que dates eh um numpy array de datetime64\n    if not isinstance(dates, np.ndarray) or dates.dtype.type is not np.datetime64:\n        dates = np.array(pd.to_datetime(dates))\n\n    t0 = dates[0]  # Reference date\n    years = np.array([(date - t0).astype('timedelta64[D]').astype(int) / 365.25 for date in dates])\n\n    def xnpv(rate):\n        # Limiting the rate to avoid overflow and division by zero\n        rate = max(min(rate, 1), -0.9999)\n        return sum([cf / (1 + rate) ** yr for cf, yr in zip(cashflows, years)])\n\n    def xnpv_prime(rate):\n        rate = max(min(rate, 1), -0.9999)\n        return sum([-yr * cf / (1 + rate) ** (yr + 1) for cf, yr in zip(cashflows, years)])\n\n    try:\n        # Using a conservative initial guess\n        initial_guess = 0.1\n        return newton(xnpv, x0=initial_guess, fprime=xnpv_prime)\n    except (RuntimeError, OverflowError):\n        # Return NaN if the calculation fails\n        return np.nan\n</code></pre>"},{"location":"modules/market_data_extractor/","title":"<code>market_data_extractor.py</code>","text":"<p>Classe <code>DataExtractor</code> (abstrata) + implementa\u00e7\u00f5es BDP/BDH; grava raw Parquet.</p>"},{"location":"modules/market_data_extractor/#market_data_extractor.DataPointExtractor","title":"<code>DataPointExtractor</code>","text":"<p>               Bases: <code>DataExtractor</code></p> Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>class DataPointExtractor(DataExtractor):\n\n    def __init__(self, lq: LuxorQuery):\n        \"\"\"Extrai dados mais recentes de precos e fields para ativos.\"\"\"\n        super().__init__(lq)\n        self.prices_table_name = 'px_last_raw'\n        self.fields_table_name = 'last_all_flds_raw'\n\n    #@task\n    def extract_prices(self, freq: str) -&gt; pd.DataFrame:\n\n        self.update_unique_price_tickers()\n\n        assets = self.lq.get_table(\"assets\")\n        assets = assets.loc[((assets[\"Disable BDP\"] != True) &amp; (assets[\"Frequency\"] == freq))]\n        tickers_to_query = list(assets[\"Ticker_BBG\"].dropna().unique())\n\n        if len(tickers_to_query) == 0: return None\n\n        if len(tickers_to_query) &gt; 300 and freq == \"minute\":\n            tickers_to_query = tickers_to_query[:300]\n            logger.warning(\n                \"Excedeu limite de tickers para consulta de minuto. Limitando em 300 tickers.\")\n        #if self.is_develop_mode:\n        #    tickers_to_query = tickers_to_query[:5]\n        data = blp.bdp(tickers=tickers_to_query, flds=[\"px_last\", \"last_update_dt\"])\n        try:\n            data[\"last_update_dt\"] = pd.to_datetime(data[\"last_update_dt\"]).dt.date\n        except KeyError:\n            print(f\"Falha ao obter precos. Freq:{freq}\")\n            pass\n\n        now = dt.datetime.now()\n        data[\"Query_Time\"] = now\n        self.api_hits_counter += len(tickers_to_query) * 2\n        data.index.name = \"Key\"\n\n        return data\n\n    #@task\n    def extract_fields(self, freq: str = \"day\", additional_assets_flds: pd.DataFrame = None):\n\n        asset_field_map = self.lq.get_table(\"asset_field_map\")\n        if additional_assets_flds is not None:\n            asset_field_map = pd.concat([asset_field_map, additional_assets_flds])\n\n        self.update_flds_keys() # atualiza somente aquelas que vem do arquivo.\n\n        assets_to_query = (asset_field_map\n                           .query(\"Frequency == @freq and Last_Data\")[[\"Ticker\", \"Field\"]]\n                           .dropna())\n\n        tickers_to_query = list(assets_to_query[\"Ticker\"].unique())\n        if len(tickers_to_query) == 0: return None\n\n        fields = list(assets_to_query[\"Field\"].unique())\n        fields_info = self.lq.get_table(\"field_map\")\n        fields_with_date_values = set(fields_info.query(\"Value_Type == 'date'\")[\"Field\"])\n\n        data_pieces = []\n        if len(assets_to_query) &gt; 300 and freq == \"minute\":\n            assets_to_query = assets_to_query[:300]\n            logger.warning(\n                \"Excedeu limite de tickers para consulta de minuto. Limitando em 200 tickers.\")\n\n        for field in fields:\n            tickers = list(assets_to_query.query(\"Field == @field\" )[\"Ticker\"].unique())\n            bbg_query = (blp.bdp(tickers=tickers, flds=[field])\n                                .reset_index().rename(columns={\"index\" : \"Ticker\"})\n                                .melt(id_vars=[\"Ticker\"], var_name=\"Field\", value_name=\"Value\")\n                                .fillna(0))\n\n            if field in fields_with_date_values:\n                bbg_query[\"Value\"] = pd.to_datetime(bbg_query[\"Value\"])\n                bbg_query[\"Value\"] = bbg_query[\"Value\"].apply(lambda x: x.timestamp())\n\n            data_pieces.append(bbg_query)\n            self.api_hits_counter += len(tickers)\n\n        data = pd.concat(data_pieces)\n\n        last_updates = (blp.bdp(tickers=tickers_to_query, flds=[\"last_update_dt\"])\n                                .reset_index().rename(columns={\"index\" : \"Ticker\"}))\n        self.api_hits_counter += len(tickers_to_query)\n\n        data[\"last_update_dt\"] = data.merge(last_updates, on=\"Ticker\")[\"last_update_dt\"]\n\n        now = dt.datetime.now()\n        data[\"Query_Time\"] = now\n        data[\"Key\"] = data[\"Ticker\"] + \"_\" + data[\"Field\"]\n        data = data.set_index(\"Key\")\n        data.index.name = \"Key\"\n\n        return data[['Ticker', 'last_update_dt', 'Field', 'Value', 'Query_Time']]\n\n\n    def get_prices_table_name(self) -&gt; str:\n        return self.prices_table_name\n\n\n    def get_fields_table_name(self) -&gt; str:\n        return self.fields_table_name\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.DataPointExtractor.__init__","title":"<code>__init__(lq)</code>","text":"<p>Extrai dados mais recentes de precos e fields para ativos.</p> Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>def __init__(self, lq: LuxorQuery):\n    \"\"\"Extrai dados mais recentes de precos e fields para ativos.\"\"\"\n    super().__init__(lq)\n    self.prices_table_name = 'px_last_raw'\n    self.fields_table_name = 'last_all_flds_raw'\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.HistoricalDataExtractor","title":"<code>HistoricalDataExtractor</code>","text":"<p>               Bases: <code>DataExtractor</code></p> Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>class HistoricalDataExtractor(DataExtractor):\n\n    def __init__(self, lq: LuxorQuery):\n        \"\"\"Extrai dados historicos de precos e fields.\"\"\"\n        super().__init__(lq)\n        self.prices_table_name = 'hist_px_last_raw'\n        self.fields_table_name = 'hist_all_flds_raw'\n\n\n    def is_dpdf_on(self):\n            # Vamos checar se o historico do BBG esta correto, com ajustes do DPDF\n            dpdf_df_test = blp.bdh(\"ge us equity\", start_date=\"2020-01-01\", end_date=\"2024-12-31\",UseDPDF=\"Y\")\n            dpdf_df_test.columns = dpdf_df_test.columns.get_level_values(0)\n            test_ge_end_price = dpdf_df_test.tail(1)[\"ge us equity\"].squeeze()\n            test_ge_start_price = dpdf_df_test.head(1)[\"ge us equity\"].squeeze()\n            queried_ge_rentab = (test_ge_end_price/test_ge_start_price)-1\n            GE_EXPECTED_RENTAB = 1.8708103195\n\n            return abs(queried_ge_rentab - GE_EXPECTED_RENTAB) &lt; 0.0001\n\n    #@task\n    def extract_prices(self, freq: str = \"day\") -&gt; pd.DataFrame:\n\n\n        if freq != 'day': # Forcando execucao somente diaria\n            return None\n\n        # Vamos comecar checando se o DPDF est\u00e1 ligado\n        if not self.is_dpdf_on():\n            logger.critical(\"ATENCAO: DPDF desligado. Dados historicos n\u00e3o ser\u00e3o atualizados.\")\n            return None\n\n        assets = self.lq.get_table(\"assets\")\n        start_date = dt.date(1999,12,27) \n\n        self.update_unique_price_tickers()\n        unique_hist_tks = self.unique_price_tickers\n        tickers_to_exclude = set(assets.loc[assets[\"Hist_Price\"] == False, \"Ticker_BBG\"])\n        unique_hist_tks = unique_hist_tks - tickers_to_exclude\n\n        hist_prices = []\n\n        # TODO: Passar a guardar rentabilidade. Para preco, usar periodo mais curto.        \n\n\n        hist_prices = blp.bdh(tickers=unique_hist_tks, flds=\"px_last\",\n                              start_date=start_date, UseDPDF=\"Y\")\n        hist_prices.columns = hist_prices.columns.get_level_values(0)\n\n        hist_prices = hist_prices.reset_index().rename(columns={\"index\" : \"Date\"})\n\n        hist_prices = hist_prices.melt(id_vars=[\"Date\"], value_name=\"Last_Price\",\n                                       var_name=\"Asset\").dropna()\n        hist_prices[\"Key\"] = hist_prices[\"Date\"].astype(str)+\"_\"+hist_prices[\"Asset\"]\n        hist_prices = hist_prices.set_index(\"Key\")\n        return hist_prices\n\n    #@task\n    def extract_fields(self, freq: str = \"day\",\n            additional_assets_flds: pd.DataFrame = None) -&gt; pd.DataFrame:\n\n        if freq != 'day': # Forcando execucao somente diaria\n            return None\n\n        # Vamos comecar checando se o DPDF est\u00e1 ligado\n        if not self.is_dpdf_on():\n            logger.critical(\"ATENCAO: DPDF desligado. Dados historicos n\u00e3o ser\u00e3o atualizados.\")\n            return None\n\n        asset_field_map = self.lq.get_table(\"asset_field_map\")\n        if additional_assets_flds is not None:\n            asset_field_map = pd.concat([asset_field_map, additional_assets_flds])\n\n        self.update_flds_keys() # atualiza somente aquelas que vem do arquivo.\n\n        assets_to_query = asset_field_map.query(\"Historical_Data\")[[\"Ticker\", \"Field\", \"Start_Date\"]]\n        unique_start_dates = list(assets_to_query[\"Start_Date\"].unique())\n\n        data = []\n\n        for start_date in unique_start_dates:\n\n            fields_to_query = list(assets_to_query\n                                   .query(\"Start_Date == @start_date\")[\"Field\"]\n                                   .unique())\n            for field in fields_to_query:\n\n                tickers_to_query = list(assets_to_query\n                                        .query(\"Start_Date == @start_date and Field == @field\"\n                                            )[\"Ticker\"].unique())\n\n                df = blp.bdh(tickers=tickers_to_query, flds=field, start_date=start_date,\n                             timeout=2000, UseDPDF=\"Y\")\n\n                df.columns = df.columns.get_level_values(0)\n                df = df.reset_index().rename(columns={\"index\":\"Date\"})\n                df[\"Field\"] = field\n                df = (df.melt(id_vars=[\"Date\", \"Field\"], var_name=\"Ticker\", value_name=\"Value\")\n                        .ffill().dropna())\n                data.append(df.copy())\n\n        data = pd.concat(data)\n\n        data[\"Key\"] = data[\"Date\"].astype(str)+\"_\"+data[\"Ticker\"]+\"_\"+data[\"Field\"]\n        data = data.set_index(\"Key\")\n        return data\n\n\n    def get_prices_table_name(self) -&gt; str:\n        return self.prices_table_name\n\n\n    def get_fields_table_name(self) -&gt; str:\n        return self.fields_table_name\n\n    #@task\n    def extract_holidays(self, today: dt.date) -&gt; pd.DataFrame:\n\n        locations_to_map = [\"us\", \"bz\"]\n        holidays_tables = []\n\n        for l in locations_to_map:\n            start_date = str(dt.date(1999, 12, 1)).replace(\"-\",\"\") # parametro precisa ser texto\n            end_date = str(today+dt.timedelta(days=500)).replace(\"-\",\"\")\n            h_table = blp.bds(\"ma us equity\",\"calendar_non_settlement_dates\",\n                              SETTLEMENT_CALENDAR_CODE=l,CALENDAR_START_DATE=start_date,\n                              CALENDAR_END_DATE=end_date).reset_index(drop=True)\n            h_table[\"Location\"] = l\n            holidays_tables.append(h_table.rename(columns={\"holiday_date\":\"Date\"}) )\n\n        holidays_tables = pd.concat(holidays_tables).sort_values(by=\"Date\")\n\n        return holidays_tables\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.HistoricalDataExtractor.__init__","title":"<code>__init__(lq)</code>","text":"<p>Extrai dados historicos de precos e fields.</p> Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>def __init__(self, lq: LuxorQuery):\n    \"\"\"Extrai dados historicos de precos e fields.\"\"\"\n    super().__init__(lq)\n    self.prices_table_name = 'hist_px_last_raw'\n    self.fields_table_name = 'hist_all_flds_raw'\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.MarketDataExtractor","title":"<code>MarketDataExtractor</code>","text":"Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>class MarketDataExtractor:\n\n    def __init__(self, extractor_type: str, raw_data_path: Path = None):\n        \"\"\" Classe para extrair dados de mercado e salvar na area de dados raw.\n\n        Args:\n            extractor_type (str): 'datapoint' ou 'histdata' \n            raw_data_path (Path, optional): camingo para staing area.\n                Defaults to None.\n        \"\"\"\n\n        assert(extractor_type in [\"datapoint\", \"histdata\"])\n\n        if raw_data_path is None:\n            raw_data_path = Path().absolute()/\"LuxorDB\"/\"raw\"\n        onelake_path = Path().absolute().parents[2]/\"OneLake - Microsoft\"/\"Fabric Lakehouse Tutorial\"\\\n                            /\"luxorLH_bronze.Lakehouse\"/\"Files\"/\"bloomberg\"\n        self.dl = DataLoader(raw_data_path)\n        #self.onelake_dl = DataLoader(onelake_path)\n        # caminho para consultar tabelas salvas em raw\n        self.lq_raw_data = LuxorQuery(tables_path=raw_data_path) \n        # caminho padrao para consultar base Luxor\n        self.lq = LuxorQuery()\n\n        self.data_extractor = None\n        if extractor_type == \"datapoint\":\n            self.data_extractor = DataPointExtractor(lq=self.lq)\n        elif extractor_type == \"histdata\":\n            self.data_extractor = HistoricalDataExtractor(lq=self.lq)\n\n        self.extractor_type = extractor_type\n\n        self.today = dt.date.today()\n        now = dt.datetime.now() - dt.timedelta(days=1)\n        self.px_freqs_last_update = {\n            \"day\": {\"last\" : now, \n                    \"seconds_til_update\" : int(5 * 60 * 60)\n                    },\n            \"hour\": {\"last\" : now, \n                    \"seconds_til_update\" : 30 * 60\n                    },\n            \"minute\": {\"last\" : now, \n                    \"seconds_til_update\" : 50\n                    },\n            }\n        self.flds_freqs_last_update = {\n            \"day\": {\"last\" : now, \n                    \"seconds_til_update\" : 10 * 60 * 60\n                    },\n            \"hour\": {\"last\" : now, \n                    \"seconds_til_update\" : 60 * 60\n                    },\n            \"minute\": {\"last\" : now, \n                    \"seconds_til_update\" : 50\n                    },\n            }\n        self.other_freqs_last_update = {\n            \"day\": {\"last\" : now,\n                    \"seconds_til_update\": 10 * 60 * 60\n                    },\n            }\n        self.is_first_run = True\n        self.prices_table_name = self.data_extractor.get_prices_table_name()\n        self.fields_table_name = self.data_extractor.get_fields_table_name()\n        self.prices_table = self.lq_raw_data.get_table(self.prices_table_name,\n                                              index=True, index_name=\"Key\")\n        self.fields_table = self.lq_raw_data.get_table(self.fields_table_name,\n                                              index=True, index_name=\"Key\")\n\n\n    def __add_flds(self):\n        \"\"\"Adiciona dinamicamente alguns campos de dados a serem extraidos.\"\"\"\n\n        assets = self.lq.get_table(\"assets\")\n        # Adicionando flds para consulta bdh de VOL e outros\n        cut_date = self.today - dt.timedelta(days=90)\n        all_fund_assets = self.lq.get_positions(\"lipizzaner\", date=self.today)\n        all_fund_assets.update(self.lq.get_positions(\"fund a\", date=self.today))\n        all_fund_assets = pd.DataFrame(all_fund_assets.items(), columns=[\"Key\", \"#\"])[[\"Key\"]]\n\n        last_movs = self.lq.get_position_variation(\"lipizzaner\", previous_date=cut_date, recent_date=self.today).query(\"Variation != 0\")[[\"Key\"]]\n        # Com isso, temos um df com a coluna 'Key' com todos os ativos que estiveram na carteira nos ultimos 90 dias \n        all_fund_assets = pd.DataFrame(pd.concat([all_fund_assets, last_movs])[\"Key\"].unique(), columns=[\"Key\"])\n        # Vamos filtrar ainda para obter apenas as acoes\n        all_fund_assets = pd.merge(all_fund_assets, assets[[\"Key\", \"Group\", \"Type\"]], on=\"Key\").query(\"Group == 'a\u00e7\u00f5es' and Type != 'a\u00e7\u00f5es_bdr'\")[[\"Key\"]]\n\n        # Listando flds que serao adicionados para esses tickers\n        flds_configs = {\"volatility_30d\"  : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                         \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n                    #\"volatility_60d\"  : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                    #                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n                    \"volatility_180d\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                         \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n                    \"volatility_360d\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                         \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n\n                    #\"russell_sector_name\"  : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":False,\n                    #                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n                    #\"gics_sector_name\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":False,\n                    #                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n\n                    \"cur_mkt_cap\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                         \"Start_Date\": dt.date(1999,12,27), \"Override\" : \"nan\"},\n                    \"best_pe_ratio\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                         \"Start_Date\": dt.date(1999,12,27), \"Override\" : \"nan\"},\n                    }\n        dfs = []\n        for flds, configs in flds_configs.items():\n            df = all_fund_assets.copy()\n            df[\"Asset\"] = df[\"Key\"].str.split(\"_\").apply(lambda x: x[0])\n            df[\"Ticker\"] = df[\"Key\"].str.split(\"_\").apply(lambda x: x[1])\n            df [[\"Field\", \"Last_Data\", \"Frequency\", \n                \"Historical_Data\", \"Start_Date\", \"Override\"]] = [flds, configs[\"Last_Data\"],\n                                                                 configs[\"Frequency\"],configs[\"Historical_Data\"],\n                                                                 configs[\"Start_Date\"], configs[\"Override\"]\n                                                                 ]\n            dfs.append(df.copy())\n\n        additional_assets_flds = pd.concat(dfs)\n\n        return additional_assets_flds\n\n\n    def is_time_to_update(self, freq, freq_type):\n        \"\"\"\n        Args:\n            freq (str): [\"day\", \"hour\", \"minute\"]\n            freq_type (str): [\"flds\",\"px\"]\n            return (bool)\n        \"\"\"\n\n        last_updates = self.flds_freqs_last_update.copy() if freq_type == \"flds\" else self.px_freqs_last_update.copy()\n\n        last_update = last_updates[freq][\"last\"]\n\n        secs_til_next_update = last_updates[freq][\"seconds_til_update\"]\n        now = dt.datetime.now()\n\n        seconds_since_last_update = (now - last_update).total_seconds()\n\n        if seconds_since_last_update &gt;= secs_til_next_update:\n            last_updates[freq][\"last\"] = now\n            if freq_type == \"flds\":\n                self.flds_freqs_last_update = last_updates\n            else:\n                self.px_freqs_last_update = last_updates\n\n            return True\n\n        return False\n\n\n    def update_table_data(self, table, subtable):\n        \"\"\"\n            Atualiza ou adiciona linhas da subtable na table.\n            \u00c9 usado o index das tabelas, para definir quando sobrescrever.\n        \"\"\"\n\n        if (subtable is None) or len(subtable) == 0 :\n            return table, False\n        if table is None or len(table) == 0:\n            return subtable, True\n        # obtendo indices que serao modificados\n        modified_idx = set(subtable.index)\n        # obtendo tickers que nao serao modificados\n        unmodified_idx = set(table.index) - modified_idx\n        updated_table = subtable.copy()\n        if len(unmodified_idx) != 0:\n            updated_table = pd.concat([table.loc[list(unmodified_idx)], subtable])\n\n\n        return updated_table, True\n\n\n    def extract(self) -&gt; bool:\n        \"\"\"Checa se \u00e9 hora de atualizar os dados e executa a extra\u00e7\u00e3o.\"\"\"\n\n        if dt.date.today() != self.today:\n            # Checando se o dia de hoje ter\u00e1 mercado BZ ou US\n            tday = dt.date.today()\n            if tday.weekday() &gt; 5 :\n                return\n            if self.lq.is_holiday(tday, location=\"all\"):\n                return \n\n            # Atualizar data de hoje e triggar atualizacao de dados historicos.\n            now = dt.datetime.now()\n            if now.hour*100+now.minute &gt;= 809: # vai atualizar as 8:09h\n                self.today = dt.date.today()\n\n        is_program_running = True\n\n        self.lq.update()\n        now = dt.datetime.now()\n\n        price_tickers_changed = self.data_extractor.check_unique_price_tickers_modified()\n        # Extraindo e atualizando precos para os ativos cadastrados\n        prices_modified = False\n        for freq in self.px_freqs_last_update.keys():\n            if self.is_time_to_update(freq, \"px\") or self.is_first_run or price_tickers_changed:\n                extracted_prices = self.data_extractor.extract_prices(freq)\n                self.prices_table, modified =  self.update_table_data(\n                                                    self.prices_table, extracted_prices)\n                prices_modified = prices_modified | modified\n\n                if modified:\n                    logger.info(f\"\"\"atualizando pre\u00e7os de freq = '{freq}'. Hits consumidos {self.data_extractor.get_api_hits_counter()}/500k.\"\"\")\n\n        if prices_modified:\n            #last_prices, modifed = self.__get_all_last_prices(assets, last_prices)\n            logger.info(f\"Salvando tabela {self.prices_table_name}...\")\n            self.dl.load_table_if_modified(self.prices_table_name, self.prices_table,\n                                            now.timestamp(), index=True, index_name=\"Key\",\n                                            export_to_blob=True, blob_directory='raw/parquet')\n            #self.onelake_dl.load_table_if_modified(self.prices_table_name, self.prices_table,\n            #                                now.timestamp(), index=True, index_name=\"Key\",\n            #                                do_not_load_excel=True)\n            logger.info(f\"Tabela {self.prices_table_name} salva.\")\n\n        # Extraindo dados para outros ativos e fields cadastrados\n        flds_data_modified = False\n        additional_assets_flds = self.__add_flds()\n        flds_keys_changed = self.data_extractor.check_unique_flds_modified()\n\n        for freq in self.flds_freqs_last_update.keys():\n            if self.is_time_to_update(freq, \"flds\") or self.is_first_run or flds_keys_changed:\n                extracted_field_data = self.data_extractor.extract_fields(freq,\n                                                                additional_assets_flds)\n                self.fields_table, modified = self.update_table_data(\n                                                self.fields_table,extracted_field_data)\n                flds_data_modified = flds_data_modified | modified\n\n                if modified :\n                    logger.info(f\"\"\"atualizando fields de freq = '{freq}'. Hits consumidos {self.data_extractor.get_api_hits_counter()}/500k.\"\"\")\n        if flds_data_modified:\n            logger.info(f\"Salvando tabela {self.fields_table_name}...\")\n            self.dl.load_table_if_modified(self.fields_table_name, self.fields_table,\n                                            now.timestamp(), index=True, index_name=\"Key\",\n                                            export_to_blob=True, blob_directory='raw/parquet')\n            #self.onelake_dl.load_table_if_modified(self.fields_table_name, self.fields_table,\n            #                                now.timestamp(), index=True, index_name=\"Key\",\n            #                                do_not_load_excel=True)\n            logger.info(f\"Tabela {self.fields_table_name} salva.\")\n\n        # Espaco para extrair e salvar outras tabelas que venham do BBG.\n        for freq in self.other_freqs_last_update.keys():\n            if self.is_time_to_update(freq, \"flds\") or self.is_first_run:\n                if self.extractor_type == \"histdata\":\n                    if freq == \"day\":\n                        holidays = self.data_extractor.extract_holidays(self.today)\n                        logger.info(f\"Salvando tabela holidays_raw...\")\n                        self.dl.load_table_if_modified(\"holidays_raw\", holidays, now.timestamp(),\n                                                       export_to_blob=True, blob_directory='raw/parquet')\n                        #self.onelake_dl.load_table_if_modified(\"holidays_raw\", holidays,\n                        #                               now.timestamp(), do_not_load_excel=True)\n                        logger.info(\"Tabela holidays_raw salva.\")\n\n        self.is_first_run = False\n\n        current_time = now.hour*100+now.minute\n\n        # Ira executar entre 8:30 e 19:30\n        if (current_time &lt; 630) or (current_time &gt;= 1930):\n            is_program_running = False\n\n        return is_program_running\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.MarketDataExtractor.__add_flds","title":"<code>__add_flds()</code>","text":"<p>Adiciona dinamicamente alguns campos de dados a serem extraidos.</p> Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>def __add_flds(self):\n    \"\"\"Adiciona dinamicamente alguns campos de dados a serem extraidos.\"\"\"\n\n    assets = self.lq.get_table(\"assets\")\n    # Adicionando flds para consulta bdh de VOL e outros\n    cut_date = self.today - dt.timedelta(days=90)\n    all_fund_assets = self.lq.get_positions(\"lipizzaner\", date=self.today)\n    all_fund_assets.update(self.lq.get_positions(\"fund a\", date=self.today))\n    all_fund_assets = pd.DataFrame(all_fund_assets.items(), columns=[\"Key\", \"#\"])[[\"Key\"]]\n\n    last_movs = self.lq.get_position_variation(\"lipizzaner\", previous_date=cut_date, recent_date=self.today).query(\"Variation != 0\")[[\"Key\"]]\n    # Com isso, temos um df com a coluna 'Key' com todos os ativos que estiveram na carteira nos ultimos 90 dias \n    all_fund_assets = pd.DataFrame(pd.concat([all_fund_assets, last_movs])[\"Key\"].unique(), columns=[\"Key\"])\n    # Vamos filtrar ainda para obter apenas as acoes\n    all_fund_assets = pd.merge(all_fund_assets, assets[[\"Key\", \"Group\", \"Type\"]], on=\"Key\").query(\"Group == 'a\u00e7\u00f5es' and Type != 'a\u00e7\u00f5es_bdr'\")[[\"Key\"]]\n\n    # Listando flds que serao adicionados para esses tickers\n    flds_configs = {\"volatility_30d\"  : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n                #\"volatility_60d\"  : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                #                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n                \"volatility_180d\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n                \"volatility_360d\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n\n                #\"russell_sector_name\"  : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":False,\n                #                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n                #\"gics_sector_name\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":False,\n                #                     \"Start_Date\": dt.date(2019,12,31), \"Override\" : \"nan\"},\n\n                \"cur_mkt_cap\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                     \"Start_Date\": dt.date(1999,12,27), \"Override\" : \"nan\"},\n                \"best_pe_ratio\" : {\"Last_Data\":True, \"Frequency\": \"day\", \"Historical_Data\":True,\n                                     \"Start_Date\": dt.date(1999,12,27), \"Override\" : \"nan\"},\n                }\n    dfs = []\n    for flds, configs in flds_configs.items():\n        df = all_fund_assets.copy()\n        df[\"Asset\"] = df[\"Key\"].str.split(\"_\").apply(lambda x: x[0])\n        df[\"Ticker\"] = df[\"Key\"].str.split(\"_\").apply(lambda x: x[1])\n        df [[\"Field\", \"Last_Data\", \"Frequency\", \n            \"Historical_Data\", \"Start_Date\", \"Override\"]] = [flds, configs[\"Last_Data\"],\n                                                             configs[\"Frequency\"],configs[\"Historical_Data\"],\n                                                             configs[\"Start_Date\"], configs[\"Override\"]\n                                                             ]\n        dfs.append(df.copy())\n\n    additional_assets_flds = pd.concat(dfs)\n\n    return additional_assets_flds\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.MarketDataExtractor.__init__","title":"<code>__init__(extractor_type, raw_data_path=None)</code>","text":"<p>Classe para extrair dados de mercado e salvar na area de dados raw.</p> <p>Parameters:</p> Name Type Description Default <code>extractor_type</code> <code>str</code> <p>'datapoint' ou 'histdata' </p> required <code>raw_data_path</code> <code>Path</code> <p>camingo para staing area. Defaults to None.</p> <code>None</code> Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>def __init__(self, extractor_type: str, raw_data_path: Path = None):\n    \"\"\" Classe para extrair dados de mercado e salvar na area de dados raw.\n\n    Args:\n        extractor_type (str): 'datapoint' ou 'histdata' \n        raw_data_path (Path, optional): camingo para staing area.\n            Defaults to None.\n    \"\"\"\n\n    assert(extractor_type in [\"datapoint\", \"histdata\"])\n\n    if raw_data_path is None:\n        raw_data_path = Path().absolute()/\"LuxorDB\"/\"raw\"\n    onelake_path = Path().absolute().parents[2]/\"OneLake - Microsoft\"/\"Fabric Lakehouse Tutorial\"\\\n                        /\"luxorLH_bronze.Lakehouse\"/\"Files\"/\"bloomberg\"\n    self.dl = DataLoader(raw_data_path)\n    #self.onelake_dl = DataLoader(onelake_path)\n    # caminho para consultar tabelas salvas em raw\n    self.lq_raw_data = LuxorQuery(tables_path=raw_data_path) \n    # caminho padrao para consultar base Luxor\n    self.lq = LuxorQuery()\n\n    self.data_extractor = None\n    if extractor_type == \"datapoint\":\n        self.data_extractor = DataPointExtractor(lq=self.lq)\n    elif extractor_type == \"histdata\":\n        self.data_extractor = HistoricalDataExtractor(lq=self.lq)\n\n    self.extractor_type = extractor_type\n\n    self.today = dt.date.today()\n    now = dt.datetime.now() - dt.timedelta(days=1)\n    self.px_freqs_last_update = {\n        \"day\": {\"last\" : now, \n                \"seconds_til_update\" : int(5 * 60 * 60)\n                },\n        \"hour\": {\"last\" : now, \n                \"seconds_til_update\" : 30 * 60\n                },\n        \"minute\": {\"last\" : now, \n                \"seconds_til_update\" : 50\n                },\n        }\n    self.flds_freqs_last_update = {\n        \"day\": {\"last\" : now, \n                \"seconds_til_update\" : 10 * 60 * 60\n                },\n        \"hour\": {\"last\" : now, \n                \"seconds_til_update\" : 60 * 60\n                },\n        \"minute\": {\"last\" : now, \n                \"seconds_til_update\" : 50\n                },\n        }\n    self.other_freqs_last_update = {\n        \"day\": {\"last\" : now,\n                \"seconds_til_update\": 10 * 60 * 60\n                },\n        }\n    self.is_first_run = True\n    self.prices_table_name = self.data_extractor.get_prices_table_name()\n    self.fields_table_name = self.data_extractor.get_fields_table_name()\n    self.prices_table = self.lq_raw_data.get_table(self.prices_table_name,\n                                          index=True, index_name=\"Key\")\n    self.fields_table = self.lq_raw_data.get_table(self.fields_table_name,\n                                          index=True, index_name=\"Key\")\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.MarketDataExtractor.extract","title":"<code>extract()</code>","text":"<p>Checa se \u00e9 hora de atualizar os dados e executa a extra\u00e7\u00e3o.</p> Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>def extract(self) -&gt; bool:\n    \"\"\"Checa se \u00e9 hora de atualizar os dados e executa a extra\u00e7\u00e3o.\"\"\"\n\n    if dt.date.today() != self.today:\n        # Checando se o dia de hoje ter\u00e1 mercado BZ ou US\n        tday = dt.date.today()\n        if tday.weekday() &gt; 5 :\n            return\n        if self.lq.is_holiday(tday, location=\"all\"):\n            return \n\n        # Atualizar data de hoje e triggar atualizacao de dados historicos.\n        now = dt.datetime.now()\n        if now.hour*100+now.minute &gt;= 809: # vai atualizar as 8:09h\n            self.today = dt.date.today()\n\n    is_program_running = True\n\n    self.lq.update()\n    now = dt.datetime.now()\n\n    price_tickers_changed = self.data_extractor.check_unique_price_tickers_modified()\n    # Extraindo e atualizando precos para os ativos cadastrados\n    prices_modified = False\n    for freq in self.px_freqs_last_update.keys():\n        if self.is_time_to_update(freq, \"px\") or self.is_first_run or price_tickers_changed:\n            extracted_prices = self.data_extractor.extract_prices(freq)\n            self.prices_table, modified =  self.update_table_data(\n                                                self.prices_table, extracted_prices)\n            prices_modified = prices_modified | modified\n\n            if modified:\n                logger.info(f\"\"\"atualizando pre\u00e7os de freq = '{freq}'. Hits consumidos {self.data_extractor.get_api_hits_counter()}/500k.\"\"\")\n\n    if prices_modified:\n        #last_prices, modifed = self.__get_all_last_prices(assets, last_prices)\n        logger.info(f\"Salvando tabela {self.prices_table_name}...\")\n        self.dl.load_table_if_modified(self.prices_table_name, self.prices_table,\n                                        now.timestamp(), index=True, index_name=\"Key\",\n                                        export_to_blob=True, blob_directory='raw/parquet')\n        #self.onelake_dl.load_table_if_modified(self.prices_table_name, self.prices_table,\n        #                                now.timestamp(), index=True, index_name=\"Key\",\n        #                                do_not_load_excel=True)\n        logger.info(f\"Tabela {self.prices_table_name} salva.\")\n\n    # Extraindo dados para outros ativos e fields cadastrados\n    flds_data_modified = False\n    additional_assets_flds = self.__add_flds()\n    flds_keys_changed = self.data_extractor.check_unique_flds_modified()\n\n    for freq in self.flds_freqs_last_update.keys():\n        if self.is_time_to_update(freq, \"flds\") or self.is_first_run or flds_keys_changed:\n            extracted_field_data = self.data_extractor.extract_fields(freq,\n                                                            additional_assets_flds)\n            self.fields_table, modified = self.update_table_data(\n                                            self.fields_table,extracted_field_data)\n            flds_data_modified = flds_data_modified | modified\n\n            if modified :\n                logger.info(f\"\"\"atualizando fields de freq = '{freq}'. Hits consumidos {self.data_extractor.get_api_hits_counter()}/500k.\"\"\")\n    if flds_data_modified:\n        logger.info(f\"Salvando tabela {self.fields_table_name}...\")\n        self.dl.load_table_if_modified(self.fields_table_name, self.fields_table,\n                                        now.timestamp(), index=True, index_name=\"Key\",\n                                        export_to_blob=True, blob_directory='raw/parquet')\n        #self.onelake_dl.load_table_if_modified(self.fields_table_name, self.fields_table,\n        #                                now.timestamp(), index=True, index_name=\"Key\",\n        #                                do_not_load_excel=True)\n        logger.info(f\"Tabela {self.fields_table_name} salva.\")\n\n    # Espaco para extrair e salvar outras tabelas que venham do BBG.\n    for freq in self.other_freqs_last_update.keys():\n        if self.is_time_to_update(freq, \"flds\") or self.is_first_run:\n            if self.extractor_type == \"histdata\":\n                if freq == \"day\":\n                    holidays = self.data_extractor.extract_holidays(self.today)\n                    logger.info(f\"Salvando tabela holidays_raw...\")\n                    self.dl.load_table_if_modified(\"holidays_raw\", holidays, now.timestamp(),\n                                                   export_to_blob=True, blob_directory='raw/parquet')\n                    #self.onelake_dl.load_table_if_modified(\"holidays_raw\", holidays,\n                    #                               now.timestamp(), do_not_load_excel=True)\n                    logger.info(\"Tabela holidays_raw salva.\")\n\n    self.is_first_run = False\n\n    current_time = now.hour*100+now.minute\n\n    # Ira executar entre 8:30 e 19:30\n    if (current_time &lt; 630) or (current_time &gt;= 1930):\n        is_program_running = False\n\n    return is_program_running\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.MarketDataExtractor.is_time_to_update","title":"<code>is_time_to_update(freq, freq_type)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str</code> <p>[\"day\", \"hour\", \"minute\"]</p> required <code>freq_type</code> <code>str</code> <p>[\"flds\",\"px\"]</p> required Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>def is_time_to_update(self, freq, freq_type):\n    \"\"\"\n    Args:\n        freq (str): [\"day\", \"hour\", \"minute\"]\n        freq_type (str): [\"flds\",\"px\"]\n        return (bool)\n    \"\"\"\n\n    last_updates = self.flds_freqs_last_update.copy() if freq_type == \"flds\" else self.px_freqs_last_update.copy()\n\n    last_update = last_updates[freq][\"last\"]\n\n    secs_til_next_update = last_updates[freq][\"seconds_til_update\"]\n    now = dt.datetime.now()\n\n    seconds_since_last_update = (now - last_update).total_seconds()\n\n    if seconds_since_last_update &gt;= secs_til_next_update:\n        last_updates[freq][\"last\"] = now\n        if freq_type == \"flds\":\n            self.flds_freqs_last_update = last_updates\n        else:\n            self.px_freqs_last_update = last_updates\n\n        return True\n\n    return False\n</code></pre>"},{"location":"modules/market_data_extractor/#market_data_extractor.MarketDataExtractor.update_table_data","title":"<code>update_table_data(table, subtable)</code>","text":"<p>Atualiza ou adiciona linhas da subtable na table. \u00c9 usado o index das tabelas, para definir quando sobrescrever.</p> Source code in <code>LuxorASAP\\market_data_extractor.py</code> <pre><code>def update_table_data(self, table, subtable):\n    \"\"\"\n        Atualiza ou adiciona linhas da subtable na table.\n        \u00c9 usado o index das tabelas, para definir quando sobrescrever.\n    \"\"\"\n\n    if (subtable is None) or len(subtable) == 0 :\n        return table, False\n    if table is None or len(table) == 0:\n        return subtable, True\n    # obtendo indices que serao modificados\n    modified_idx = set(subtable.index)\n    # obtendo tickers que nao serao modificados\n    unmodified_idx = set(table.index) - modified_idx\n    updated_table = subtable.copy()\n    if len(unmodified_idx) != 0:\n        updated_table = pd.concat([table.loc[list(unmodified_idx)], subtable])\n\n\n    return updated_table, True\n</code></pre>"},{"location":"modules/market_data_loader/","title":"<code>market_data_loader.py</code>","text":"<p>Daemon que unifica pre\u00e7os e estimativas, salvando tabelas em LuxorDB/tables.</p>"},{"location":"modules/market_data_loader/#market_data_loader.TableDataLoader","title":"<code>TableDataLoader</code>","text":"Source code in <code>LuxorASAP\\market_data_loader.py</code> <pre><code>class TableDataLoader:\n\n    def __init__(self, update_func: callable, kwargs: dict = {}, luxordb_path: Path = None):\n        \"\"\"\n        Args:\n            update_func (callable): Funcao que sera chamada para atualizar a tabela.\n            kwargs (dict): Dicionario com os argumentos para a funcao de update.\n            luxordb_path (Path, optional): Caminho para o diretorio raiz do luxorDB.\n                Defaults to None.\n        \"\"\"\n\n        if luxordb_path is None:\n            luxordb_path = Path().absolute()/'luxor_db'/'tables'\n\n        self.dl = DataLoader(luxordb_path)\n        self.update_func = update_func\n        self.kwargs = kwargs\n\n\n    def add_load_trigger(self, trigger_path: Path):\n        \"\"\" Adiciona tabela na lista para controle de alteracao.\n            O load sera disparado quando essa tabela for atualizada.\n        Args:\n            table_path (Path): Path para a tabela que ira disparar a atualizacao.\n        \"\"\"\n        self.dl.add_file_tracker(trigger_path)\n\n\n    def table_load_triggered(self) -&gt; bool:\n        \"\"\"Verifica se alguma das trigger tables foi atualizada.\"\"\"\n\n        load_triggered = False\n        # Verificando se alguma das tabelas foi atualizada\n        for trigger_path in self.dl.tracked_files:\n\n            file_updated, last_update_time = self.dl.is_file_modified(trigger_path)\n            if file_updated:\n                load_triggered = True #load_triggered | True\n                self.dl.set_file_modified_time(trigger_path, last_update_time)\n\n        return load_triggered\n\n\n    def load_table(self):\n        \"\"\"Carrega a tabela no luxorDB.\n        Args:\n            kwargs (dict): Dicionario com os argumentos para a funcao de update.\n        \"\"\"\n        self.update_func(**self.kwargs)\n</code></pre>"},{"location":"modules/market_data_loader/#market_data_loader.TableDataLoader.__init__","title":"<code>__init__(update_func, kwargs={}, luxordb_path=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>update_func</code> <code>callable</code> <p>Funcao que sera chamada para atualizar a tabela.</p> required <code>kwargs</code> <code>dict</code> <p>Dicionario com os argumentos para a funcao de update.</p> <code>{}</code> <code>luxordb_path</code> <code>Path</code> <p>Caminho para o diretorio raiz do luxorDB. Defaults to None.</p> <code>None</code> Source code in <code>LuxorASAP\\market_data_loader.py</code> <pre><code>def __init__(self, update_func: callable, kwargs: dict = {}, luxordb_path: Path = None):\n    \"\"\"\n    Args:\n        update_func (callable): Funcao que sera chamada para atualizar a tabela.\n        kwargs (dict): Dicionario com os argumentos para a funcao de update.\n        luxordb_path (Path, optional): Caminho para o diretorio raiz do luxorDB.\n            Defaults to None.\n    \"\"\"\n\n    if luxordb_path is None:\n        luxordb_path = Path().absolute()/'luxor_db'/'tables'\n\n    self.dl = DataLoader(luxordb_path)\n    self.update_func = update_func\n    self.kwargs = kwargs\n</code></pre>"},{"location":"modules/market_data_loader/#market_data_loader.TableDataLoader.add_load_trigger","title":"<code>add_load_trigger(trigger_path)</code>","text":"<p>Adiciona tabela na lista para controle de alteracao.     O load sera disparado quando essa tabela for atualizada. Args:     table_path (Path): Path para a tabela que ira disparar a atualizacao.</p> Source code in <code>LuxorASAP\\market_data_loader.py</code> <pre><code>def add_load_trigger(self, trigger_path: Path):\n    \"\"\" Adiciona tabela na lista para controle de alteracao.\n        O load sera disparado quando essa tabela for atualizada.\n    Args:\n        table_path (Path): Path para a tabela que ira disparar a atualizacao.\n    \"\"\"\n    self.dl.add_file_tracker(trigger_path)\n</code></pre>"},{"location":"modules/market_data_loader/#market_data_loader.TableDataLoader.load_table","title":"<code>load_table()</code>","text":"<p>Carrega a tabela no luxorDB. Args:     kwargs (dict): Dicionario com os argumentos para a funcao de update.</p> Source code in <code>LuxorASAP\\market_data_loader.py</code> <pre><code>def load_table(self):\n    \"\"\"Carrega a tabela no luxorDB.\n    Args:\n        kwargs (dict): Dicionario com os argumentos para a funcao de update.\n    \"\"\"\n    self.update_func(**self.kwargs)\n</code></pre>"},{"location":"modules/market_data_loader/#market_data_loader.TableDataLoader.table_load_triggered","title":"<code>table_load_triggered()</code>","text":"<p>Verifica se alguma das trigger tables foi atualizada.</p> Source code in <code>LuxorASAP\\market_data_loader.py</code> <pre><code>def table_load_triggered(self) -&gt; bool:\n    \"\"\"Verifica se alguma das trigger tables foi atualizada.\"\"\"\n\n    load_triggered = False\n    # Verificando se alguma das tabelas foi atualizada\n    for trigger_path in self.dl.tracked_files:\n\n        file_updated, last_update_time = self.dl.is_file_modified(trigger_path)\n        if file_updated:\n            load_triggered = True #load_triggered | True\n            self.dl.set_file_modified_time(trigger_path, last_update_time)\n\n    return load_triggered\n</code></pre>"},{"location":"modules/market_data_loader/#market_data_loader.__update_table_data","title":"<code>__update_table_data(table, subtable)</code>","text":"<p>Atualiza ou adiciona linhas da subtable na table. \u00c9 usado o index das tabelas, para definir quando sobrescrever. Logo, precisa ter index definido para ambas!</p> Source code in <code>LuxorASAP\\market_data_loader.py</code> <pre><code>def __update_table_data(table, subtable):\n        \"\"\"\n            Atualiza ou adiciona linhas da subtable na table.\n            \u00c9 usado o index das tabelas, para definir quando sobrescrever.\n            Logo, precisa ter index definido para ambas!\n        \"\"\"\n\n        if (subtable is None) or len(subtable) == 0 :\n            return table, False\n        if table is None or len(table) == 0:\n            return subtable, True\n        # obtendo indices que serao modificados\n        modified_idx = set(subtable.index)\n        # obtendo tickers que nao serao modificados\n        unmodified_idx = set(table.index) - modified_idx\n        updated_table = subtable.copy()\n        if len(unmodified_idx) != 0:\n            subtable = subtable.astype(table.dtypes)\n            updated_table = pd.concat([table.loc[list(unmodified_idx)], subtable])\n\n\n        return updated_table, True\n</code></pre>"},{"location":"modules/run_luxorASAP/","title":"<code>run_luxorASAP.py</code>","text":"<p>Daemon que monitora a planilha historico_trades_ativos.xlsx, gera as tabelas Parquet e atualiza posi\u00e7\u00f5es &amp; caixa em tempo real.</p> <ul> <li>Inicializa\u00e7\u00e3o e carregamento de dados do LuxorDB.</li> <li>Processamento de trades e atualiza\u00e7\u00e3o de posi\u00e7\u00f5es de fundos.</li> <li>Exporta\u00e7\u00e3o de dados processados de volta para o LuxorDB.</li> <li>Gerenciamento de logs e controle de execu\u00e7\u00e3o.</li> </ul> <p>O programa opera continuamente durante o hor\u00e1rio de mercado, verificando periodicamente por novas transa\u00e7\u00f5es e atualizando as posi\u00e7\u00f5es dos fundos.</p>"},{"location":"modules/run_luxorASAP/#run_luxorASAP.LuxorASAPManager","title":"<code>LuxorASAPManager</code>","text":"Source code in <code>LuxorASAP\\run_luxorASAP.py</code> <pre><code>class LuxorASAPManager:\n\n    @logger.catch\n    def __init__(self, reset=False, is_develop_mode=False):\n\n        self.today = dt.date.today()\n\n        self.__initialize(reset, is_develop_mode)\n\n        self.source_bases_exec_count = 0\n\n\n    def __reset(self):\n        self._register_assets()\n        self.save_funds()\n\n\n    def __initialize(self, reset, is_develop_mode):\n\n        self.is_develop_mode = is_develop_mode\n        # Criando db manager, inicia carregando todos os dados\n        self.lq = LuxorQuery(is_develop_mode=self.is_develop_mode)\n        self.dl = DataLoader(Path().absolute()/\"LuxorDB\"/\"tables\")\n        logger.info(\"LuxorDB: Dados Inicializados.\")\n\n        self.dl.add_file_tracker(Path().absolute()/\"source_bases\"/\"historico_trades_ativos.xlsx\", \n                                sheet_names={\"Boletas\": \"trades\", \"Ativos\": \"assets\", \"VCs_prices\":\"hist_vc_px_last\",\n                                             \"asset_field_map\": \"asset_field_map\", \"field_map\" : \"field_map\"}, index=False,\n                                             normalize_columns=True)\n        self.dl.load_file_if_modified(Path().absolute()/\"source_bases\"/\"historico_trades_ativos.xlsx\", export_to_blob=True)\n\n        # Carregando/criando fundos\n        self.funds = {}\n\n        if reset:\n            self.__reset()\n        else:\n            self.funds = self._load_funds()\n\n\n    def get_unique_funds(self):\n        \"\"\" Retorna uma lista de ocorrencia unicas de fundos na base de boletas \"\"\"\n        unq_funds = list(self.lq.get_table(\"trades\")[\"Fund\"].unique()) \n        unq_funds.remove(\"lipizzaner(eduardo)\")\n\n\n        return unq_funds\n\n\n    def get_unique_assets(self, fund_filter=None):\n        \"\"\" Retorna lista de ocorrencias unicas de ativos. Se 'fund_filter' for informado, retorna as ocorrencias unicas nesse fundo.\"\"\"\n        trades = self.lq.get_table(\"trades\").sort_values(by=\"Date\", kind=\"stable\")\n\n        trades = self.lq.text_to_lowercase(trades)\n        trades[\"FRGN_KEY\"] = trades[\"Asset\"] + \"_\" + trades[\"Ticker\"]\n\n        if fund_filter is None:\n            return trades[\"FRGN_KEY\"].unique()\n\n        return trades.loc[ trades[\"Fund\"] == fund_filter.lower(), \"FRGN_KEY\"].unique()\n\n\n    def get_asset_properties(self, asset_key):\n        \"\"\" Encontra as propriedades do ativo dado por 'asset_key'. Se nao for um ativo valido, retorna 'None'.\"\"\"\n\n        valid_assets = self.lq.get_table(\"assets\") \n        properties = valid_assets.loc[ valid_assets[\"Key\"] == asset_key ]\n\n        if properties.empty:\n            return properties\n\n        return properties.iloc[0]\n\n\n    def _load_funds(self):\n        # Seleciona o arquivo mais recente e carrega os dados dos fundos\n\n        files = os.listdir(\"funds_data\")\n        files.sort(reverse=True)\n        savefile = os.path.join(\"funds_data\", files[0])\n\n        with open(savefile, \"rb\") as f:\n            funds = pickle.load(f)\n\n        return funds\n\n\n    def save_funds(self):\n        # salva os dados de cada fundo num pickle\n        attempts = 5\n        file_name = os.path.join(\"funds_data\", \"FundsData_\"+str(self.today)+\".pickle\")\n        while attempts &gt; 0:\n            try:\n                with open(file_name, \"wb\") as f:\n                    pickle.dump(self.funds, f, pickle.HIGHEST_PROTOCOL)\n                    attempts = 0\n            except PermissionError:\n                attempts -=1\n                logger.error(f\"Erro ao tentar salvar FundsData_{str(self.today)}.pickle\\nSer\u00e3o feitas 5 tentativas...\")\n                time.sleep(5)\n\n    def redirect_trades(self):\n        # Filtra o df de trades e redireciona para processamento individual em cada fundo.\n        trades = self.lq.get_table(\"trades\")  \n        for fund_name, fund in self.funds.items():\n            trades_df = trades.loc[ trades[\"Fund\"] == fund_name ]\n\n            fund.set_trades_to_process(trades_df)\n\n\n    # TODO: Nao precisa usar o db_manager. Pode simplesmente salvar dentro da luxorDB.\n    def export_data_to_db(self, date_reference=None):\n        \"\"\"\n        Insere as tabelas processadas (posicoes, posicoes_dia) para luxor_db.\n        \"\"\"\n        if date_reference is None: date_reference=self.today\n        last_positions = []\n        hist_positions = []\n        cash_movements = [] # vamos agrupar as movimentacoes de caixa aqui para exportar\n        positions_by_bank = [] # vamos agrupar as posicoes por banco aqui para exportar\n        last_positions_by_bank = [] # vamos agrupar as posicoes por banco aqui para exportar\n        # Recuperamos a base de posicoes.. Mais recentes e historicas\n        for fund_name, fund_data in self.funds.items():\n            last_positions.append(fund_data.get_positions(all=False, date_reference=date_reference))\n            hist_positions.append(fund_data.get_positions(all=True))\n            cash_movements.append(fund_data.get_cash_movements())\n            positions_by_bank.append(fund_data.get_positions_by_bank())\n            last_positions_by_bank.append(fund_data.get_positions_by_bank(all=False, date_reference=date_reference))\n\n        last_update = dt.datetime.now().timestamp() # forcar atualizacao\n\n        # Concatenamos tudo numa tabela fato so\n        last_positions = pd.concat(last_positions)\n        # Exportamos as tabelas para luxorDB\n        # sobrescrevendo as movimentacoes na base (modificacoes ou nao, vamos sobrescrever pois base pode estar desatualizada)\n        logger.info(f\"Exportando dados para a base de dados.\")\n        self.dl.load_table_if_modified(\"last_positions\", last_positions, last_update=last_update, normalize_columns=True,\n                                       export_to_blob=True)\n\n        #O mesmo eh feito com o restante\n        hist_positions = pd.concat(hist_positions)\n\n        self.dl.load_table_if_modified(\"hist_positions\", hist_positions, last_update=last_update, normalize_columns=True,\n                                       export_to_blob=True)\n        # Exportamos as tabelas para luxorDB\n        cash_movements = pd.concat(cash_movements)\n        self.dl.load_table_if_modified(\"cash_movements\", cash_movements, last_update=last_update, normalize_columns=True,\n                                       export_to_blob=True)\n\n        positions_by_bank = pd.concat(positions_by_bank)\n        self.dl.load_table_if_modified(\"hist_positions_by_bank\", positions_by_bank, last_update=last_update, normalize_columns=True,\n                                       export_to_blob=True)\n\n        last_positions_by_bank = pd.concat(last_positions_by_bank)\n        self.dl.load_table_if_modified(\"last_positions_by_bank\", last_positions_by_bank, last_update=last_update, normalize_columns=True,\n                                       export_to_blob=True)\n\n\n    def _register_assets(self):\n        # TODO PRECISAMOS SEPARAR POR CONTA BANC\u00c1RIA!\n        # Restagando nomes unicos dos fundos\n        fund_names = self.get_unique_funds()\n        for name in fund_names:     \n\n            self.funds[name] = Fund(name)\n            # Resgatando ativos unicos de cada fundo\n            fund_assets = self.get_unique_assets(fund_filter=name)\n\n            # Inicializamos cada ativo valido com suas respectivas props\n            for asset_key in fund_assets:\n                asset_props = self.get_asset_properties(asset_key)\n\n                if asset_props.empty:\n                    continue    # Ativo nao eh valido\n\n                props = {\n                \"asset_type\" : asset_props[\"Type\"], \"group\" : asset_props[\"Group\"], \"ticker_bbg\" : asset_props[\"Ticker_BBG\"],\n                \"curncy_exp\" : asset_props[\"Currency Exposure\"], \"location\" : asset_props[\"Location\"]\n                }\n\n                self.funds[name].register_asset(asset_props[\"Asset\"], asset_props[\"Ticker\"], props, self.lq)\n\n\n    def _register_new_assets(self, fund_name):\n\n        assets_list = self.get_unique_assets(fund_filter=fund_name)\n        assets_already_registered = self.funds[fund_name].get_assets()\n\n        for asset_key in assets_list:\n            if asset_key not in assets_already_registered:\n                # Entao eh um ativo novo que precisamos registrar no fundo\n                asset_props = self.get_asset_properties(asset_key)\n\n                if asset_props.empty:\n                    continue    # Ativo nao eh valido\n\n                props = {\n                \"asset_type\" : asset_props[\"Type\"], \"group\" : asset_props[\"Group\"], \"ticker_bbg\" : asset_props[\"Ticker_BBG\"],\n                \"curncy_exp\" : asset_props[\"Currency Exposure\"], \"location\" : asset_props[\"Location\"]\n                }\n\n                self.funds[fund_name].register_asset(asset_props[\"Asset\"], asset_props[\"Ticker\"], props, self.lq)\n\n\n    def process_trades(self, fund_names=None, date_reference=None):\n\n        if date_reference is None: date_reference = self.today\n\n        flag_modified = False\n        flag_reset = False\n\n        if fund_names is None: #Faz para todos os fundos cadastrados\n            fund_names = list(self.funds.keys())\n            # Nesse caso processa todos os fundos\n\n        for f in fund_names:\n            # Processa apenas para os fundos informados, se existirem\n            if f in self.funds:\n\n                # Primeiro criamos ativos novos (caso onde foram feitos trades com ativos novos)\n                self._register_new_assets(fund_name=f)\n\n                # Agora podemos processas os trades#\n                _flag_modified, _flag_reset = self.funds[f].process_trades(self.lq)\n                flag_modified = flag_modified | _flag_modified\n                flag_reset = flag_reset | _flag_reset\n\n        return flag_modified, flag_reset\n\n\n    def stop(self):\n        self.save_funds()\n        #self.export_data_to_db()\n\n\n    def update(self):\n        # Faz diferenca olhar se eh fim de semana ou nao? Acho que nao mais...\n        #if self.today != dt.date.today():\n        #    tday= dt.date.today()\n        #    if tday.weekday() &lt;= 6:\n        #        if not self.lq.is_holiday(tday, location=\"any\"):  \n        #            self.source_bases_exec_count = 0\n        #            self.today = tday\n        #        else: return\n        #    else: return #final de semana, nao vamos rodar\n\n        # Foi criado um script separado para atualizacao das source_bases\n        #if self.source_bases_exec_count == 0:\n        #    now = dt.datetime.now()\n        #    if now.hour*100 + now.minute &gt;= 50:\n        #        source_bases_updater.update(is_develop_mode=self.is_develop_mode)\n        #        self.source_bases_exec_count += 1\n\n        # Processamos todos os trades\n        self.redirect_trades()\n        positions_changed, flag_reset = self.process_trades()\n\n        if not flag_reset:\n            if positions_changed:\n                # Enviamos as tabelas resultantes para a base de dados \n                self.export_data_to_db()\n\n            # Salvar pickle com informacao de processamento dos fundos\n            self.save_funds()\n\n            # Buscando alteracoes na historico_trades_ativos. Carregando caso haja.\n            self.dl.scan_files(export_to_blob=True)\n\n            # Atualizando tabelas carregadas.\n            self.lq.update()\n\n        else:\n            logger.warning(\"RESET autom\u00e1tico agendado para pr\u00f3xima exec.\")\n            self.__initialize(reset=flag_reset, is_develop_mode=self.is_develop_mode)\n</code></pre>"},{"location":"modules/run_luxorASAP/#run_luxorASAP.LuxorASAPManager.export_data_to_db","title":"<code>export_data_to_db(date_reference=None)</code>","text":"<p>Insere as tabelas processadas (posicoes, posicoes_dia) para luxor_db.</p> Source code in <code>LuxorASAP\\run_luxorASAP.py</code> <pre><code>def export_data_to_db(self, date_reference=None):\n    \"\"\"\n    Insere as tabelas processadas (posicoes, posicoes_dia) para luxor_db.\n    \"\"\"\n    if date_reference is None: date_reference=self.today\n    last_positions = []\n    hist_positions = []\n    cash_movements = [] # vamos agrupar as movimentacoes de caixa aqui para exportar\n    positions_by_bank = [] # vamos agrupar as posicoes por banco aqui para exportar\n    last_positions_by_bank = [] # vamos agrupar as posicoes por banco aqui para exportar\n    # Recuperamos a base de posicoes.. Mais recentes e historicas\n    for fund_name, fund_data in self.funds.items():\n        last_positions.append(fund_data.get_positions(all=False, date_reference=date_reference))\n        hist_positions.append(fund_data.get_positions(all=True))\n        cash_movements.append(fund_data.get_cash_movements())\n        positions_by_bank.append(fund_data.get_positions_by_bank())\n        last_positions_by_bank.append(fund_data.get_positions_by_bank(all=False, date_reference=date_reference))\n\n    last_update = dt.datetime.now().timestamp() # forcar atualizacao\n\n    # Concatenamos tudo numa tabela fato so\n    last_positions = pd.concat(last_positions)\n    # Exportamos as tabelas para luxorDB\n    # sobrescrevendo as movimentacoes na base (modificacoes ou nao, vamos sobrescrever pois base pode estar desatualizada)\n    logger.info(f\"Exportando dados para a base de dados.\")\n    self.dl.load_table_if_modified(\"last_positions\", last_positions, last_update=last_update, normalize_columns=True,\n                                   export_to_blob=True)\n\n    #O mesmo eh feito com o restante\n    hist_positions = pd.concat(hist_positions)\n\n    self.dl.load_table_if_modified(\"hist_positions\", hist_positions, last_update=last_update, normalize_columns=True,\n                                   export_to_blob=True)\n    # Exportamos as tabelas para luxorDB\n    cash_movements = pd.concat(cash_movements)\n    self.dl.load_table_if_modified(\"cash_movements\", cash_movements, last_update=last_update, normalize_columns=True,\n                                   export_to_blob=True)\n\n    positions_by_bank = pd.concat(positions_by_bank)\n    self.dl.load_table_if_modified(\"hist_positions_by_bank\", positions_by_bank, last_update=last_update, normalize_columns=True,\n                                   export_to_blob=True)\n\n    last_positions_by_bank = pd.concat(last_positions_by_bank)\n    self.dl.load_table_if_modified(\"last_positions_by_bank\", last_positions_by_bank, last_update=last_update, normalize_columns=True,\n                                   export_to_blob=True)\n</code></pre>"},{"location":"modules/run_luxorASAP/#run_luxorASAP.LuxorASAPManager.get_asset_properties","title":"<code>get_asset_properties(asset_key)</code>","text":"<p>Encontra as propriedades do ativo dado por 'asset_key'. Se nao for um ativo valido, retorna 'None'.</p> Source code in <code>LuxorASAP\\run_luxorASAP.py</code> <pre><code>def get_asset_properties(self, asset_key):\n    \"\"\" Encontra as propriedades do ativo dado por 'asset_key'. Se nao for um ativo valido, retorna 'None'.\"\"\"\n\n    valid_assets = self.lq.get_table(\"assets\") \n    properties = valid_assets.loc[ valid_assets[\"Key\"] == asset_key ]\n\n    if properties.empty:\n        return properties\n\n    return properties.iloc[0]\n</code></pre>"},{"location":"modules/run_luxorASAP/#run_luxorASAP.LuxorASAPManager.get_unique_assets","title":"<code>get_unique_assets(fund_filter=None)</code>","text":"<p>Retorna lista de ocorrencias unicas de ativos. Se 'fund_filter' for informado, retorna as ocorrencias unicas nesse fundo.</p> Source code in <code>LuxorASAP\\run_luxorASAP.py</code> <pre><code>def get_unique_assets(self, fund_filter=None):\n    \"\"\" Retorna lista de ocorrencias unicas de ativos. Se 'fund_filter' for informado, retorna as ocorrencias unicas nesse fundo.\"\"\"\n    trades = self.lq.get_table(\"trades\").sort_values(by=\"Date\", kind=\"stable\")\n\n    trades = self.lq.text_to_lowercase(trades)\n    trades[\"FRGN_KEY\"] = trades[\"Asset\"] + \"_\" + trades[\"Ticker\"]\n\n    if fund_filter is None:\n        return trades[\"FRGN_KEY\"].unique()\n\n    return trades.loc[ trades[\"Fund\"] == fund_filter.lower(), \"FRGN_KEY\"].unique()\n</code></pre>"},{"location":"modules/run_luxorASAP/#run_luxorASAP.LuxorASAPManager.get_unique_funds","title":"<code>get_unique_funds()</code>","text":"<p>Retorna uma lista de ocorrencia unicas de fundos na base de boletas</p> Source code in <code>LuxorASAP\\run_luxorASAP.py</code> <pre><code>def get_unique_funds(self):\n    \"\"\" Retorna uma lista de ocorrencia unicas de fundos na base de boletas \"\"\"\n    unq_funds = list(self.lq.get_table(\"trades\")[\"Fund\"].unique()) \n    unq_funds.remove(\"lipizzaner(eduardo)\")\n\n\n    return unq_funds\n</code></pre>"},{"location":"modules/source_bases_updater/","title":"<code>source_bases_updater.py</code>","text":"<p>Orquestra os scripts em <code>source_bases/scripts/</code>, exportando as tabelas para LuxorDB e Azure Data Lake.</p>"},{"location":"modules/carteira_online/production/asset_data_extraction/","title":"<code>asset_data_extraction.py</code> (LEGACY)","text":"<p>M\u00f3dulo historicamente usado para swap e dados de fundos Luxor. Hoje resta por compatibilidade de import apenas.</p>"},{"location":"modules/carteira_online/production/asset_data_extraction/#carteira_online.production.asset_data_extraction.AssetDataExtractor","title":"<code>AssetDataExtractor</code>","text":"Source code in <code>carteira_online\\production\\asset_data_extraction.py</code> <pre><code>class AssetDataExtractor:\n\n    def __init__(self, query_mngr, onedrive_path=\"\", today=dt.date.today()):\n        self.today = today\n        self.btg_extractor = btg_data_extraction.BTGDataExtractor(onedrive_path=onedrive_path)\n        self._print_btg_dates()\n        self.hidex = historical_data_extraction.HistoricalDataExtractor(today=self.today)\n\n        #swaps\n        self.swaps = {}     # consolidado dia anterior \n        self.swaps_pnl = {} # p&amp;l atualizado diario\n        self.swaps_exp = {} # ponta passiva diaria\n\n        # caixas\n        self.fut_adj_data = {}  # lista com nome fundo : [qtd, ticker]\n        self.term_adj = {}      # lista com nome fundo : total termos\n\n        self.query_mngr = query_mngr\n\n\n    def _print_btg_dates(self):\n        dates = self.btg_extractor.get_all_dates()\n        print(\"Carteiras BTG em uso:\")\n        for tpl in dates:\n            print(tpl[0], \": \", tpl[1])\n\n\n    def __get_date_offset(self, ref_date, offset: int): # -&gt; datetime\n        \"\"\"\n        Retorna a data informada em 'ref_date'(datetime) somada  \n        de 'offset' dias(que pode ser negativo).\n        \"\"\"        \n        delta_day = dt.timedelta(days=1)\n        return ref_date + delta_day * offset\n\n\n    #def get_usdbrl(self, usd_ticker=\"usdbrl bgn curncy\", flds=\"px_last\"):\n    #    return blp.bdp(usd_ticker, flds).iloc[0,0]\n\n\n    def _update_swaps(self, fund_name):\n        \"\"\"Calcula o p&amp;l swap do fundo solicitado.\n            Caso ainda nao tenha o valor do swap,\n            carrega o dia anterior da base historica/btg\n        \"\"\"\n        # Hardcoded para ndf feito nos fundos us\n        if fund_name in [\"Fund b\", \"Fund a\", \"Hmx\"]:\n            exps = {\"Fund b\" : 0, \"Fund a\" : 0, \"Hmx\" : 0}\n\n            pnl_fund_b = exps[\"Fund b\"] * (-1) * (self.query_mngr.get_price(\"usdbrl curncy\")/4.6991-1)\n            pnl_hmx = exps[\"Hmx\"] * (-1) * (self.query_mngr.get_price(\"usdbrl curncy\")/4.7004-1)\n\n            pnls = {\"Fund b\": pnl_fund_b, \"Fund a\" : 0, \"Hmx\" : pnl_hmx}\n            self.swaps_pnl[fund_name] = pnls[fund_name]\n            self.swaps_exp[fund_name] = exps[fund_name]\n\n        else:\n            if fund_name not in self.swaps:\n                # precisa resgatar/atualizar base historica\n                self.swaps[fund_name] = self.hidex.update_swap(fund_name, self.btg_extractor, self.query_mngr)\n\n\n            chg_pct_1d = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.query_mngr.get_bday_offset(self.today, -1))\n            print(f\"Varia\u00e7\u00e3o d\u00f3lar: {chg_pct_1d*100}%\")\n            new_exp = self.swaps[fund_name][\"Exposi\u00e7\u00e3o\"] * (1 + chg_pct_1d)\n            self.swaps_pnl[fund_name] = self.swaps[fund_name][\"Acumulado\"] + (self.swaps[fund_name][\"Exposi\u00e7\u00e3o\"]-new_exp)\n            self.swaps_pnl[fund_name] = self.swaps_pnl[fund_name].iat[-1]\n            self.swaps_exp[fund_name] = new_exp.iat[-1]\n\n\n    def get_swap_pnl(self, fund_name):\n\n        self._update_swaps(fund_name)\n        return self.swaps_pnl[fund_name]\n\n    def get_swap_exp(self, fund_name):\n\n        if fund_name not in self.swaps_exp:\n            self._update_swaps(self, fund_name)\n        return self.swaps_exp[fund_name]\n\n\n    # ---- VERSAO CAIXAS  ----- \n    def set_fut_data(self, fund_name, data):\n        # Para o correto funcionamento, precisa ser setado o quanto antes\n        self.fut_adj_data[fund_name] = data\n\n\n    def set_term_adj(self, fund_name, term_amnt):\n        fund_name = fund_name.title()\n        # Para o correto funcionamento, precisa ser setado o quanto antes\n        self.term_adj[fund_name] = term_amnt\n\n\n    def calculate_loan_bond(self, name):\n        loans = {\n                \"jp loan\" : {\n                    \"base\" : -1015000,\n                    \"taxa\" : 0.0131,\n                    \"inicio\" : dt.date(2020, 11, 30),\n                    \"fim\" : dt.date(2023, 10, 31)\n                },\n\n                \"loan al\" : {\n                    \"base\" : -500000,\n                    \"taxa\" : 0.0244,\n                    \"inicio\" : dt.date(2021, 1, 21),\n                    \"fim\" : dt.date(2022, 1, 21)\n\n                },\n\n                \"al bond\" : {\n                    \"base\" : 500000,\n                    \"taxa\" : 0.08,\n                    \"inicio\" : dt.date(2021, 2, 13),\n                    \"fim\" : dt.date(2022, 2, 13)\n\n                }\n                }\n\n        loan_data = loans[name]\n        days = (self.today - loan_data[\"inicio\"]).days\n        pnl = loan_data[\"base\"] * loan_data[\"taxa\"] * days/365\n\n        return loan_data[\"base\"] + pnl\n\n\n    # ----- VERSAO RETORNOS -----\n\n    def get_ptf_quota_data(self, fund_name):\n        try:\n            data =  {\n                \"Data\" : self.btg_extractor.get_date(fund_name.title()),\n                \"Fundo\": fund_name,\n                \"PL\" : self.btg_extractor.get_pl(fund_name.title()),\n                \"#_cota\" : self.btg_extractor.get_qtd_cotas(fund_name.title()),\n                \"Cota\" : self.btg_extractor.get_cota(fund_name.title())\n            }\n            return data\n\n        except IndexError:\n            return {} \n</code></pre>"},{"location":"modules/carteira_online/production/asset_data_extraction/#carteira_online.production.asset_data_extraction.AssetDataExtractor.__get_date_offset","title":"<code>__get_date_offset(ref_date, offset)</code>","text":"<p>Retorna a data informada em 'ref_date'(datetime) somada de 'offset' dias(que pode ser negativo).</p> Source code in <code>carteira_online\\production\\asset_data_extraction.py</code> <pre><code>def __get_date_offset(self, ref_date, offset: int): # -&gt; datetime\n    \"\"\"\n    Retorna a data informada em 'ref_date'(datetime) somada  \n    de 'offset' dias(que pode ser negativo).\n    \"\"\"        \n    delta_day = dt.timedelta(days=1)\n    return ref_date + delta_day * offset\n</code></pre>"},{"location":"modules/carteira_online/production/btg_data_extraction/","title":"<code>btg_data_extraction.py</code>","text":"<p>Fun\u00e7\u00f5es utilit\u00e1rias que consomem carteiras_btg.xlsx para obter quebras de caixa e cotas de terceiros.</p>"},{"location":"modules/carteira_online/production/btg_data_extraction/#carteira_online.production.btg_data_extraction.BTGDataExtractor","title":"<code>BTGDataExtractor</code>","text":"Source code in <code>carteira_online\\production\\btg_data_extraction.py</code> <pre><code>class BTGDataExtractor:\n\n    def __init__(self, path_to_excel=\"Carteiras_BTG.xlsx\", path_to_pdfs=\"carteiras_btg\", debug=True, onedrive_path=\"\"):\n        #path_to_pdfs = os.path.join(\"carteira_online\",\"production\", \"carteiras_btg\")\n        self.excel_path = os.path.join(path_to_pdfs, path_to_excel)\n\n        self.coluna_ativos = \"B\"\n        self.coluna_cotas_fundos = \"F\"\n\n\n        # configura\u00e7\u00f5es caixa\n        self.coluna_var_cx_tot = \"D\"\n        self.var_cx_tot_id = \"Total_outros\"\n        self.despesa_id = \"DESPESA\"\n        self.tx_adm_id = \"TAXA ADMINISTRACAO\"\n        self.a_pagar_id = \"A PAGAR BOLSA\"\n        self.a_receber_id = \"A RECEBER BOLSA\"\n        self.a_rec_dividendos = \"A REC. DIVIDENDOS\"\n        self.cx_offshore = \"CAIXA EXTERNO OFFSHORE\"\n        self.acoes_offshore = \"A\u00c7\u00d5ES OFFSHORE\"\n        self.ntnb = \"NTNB IPCA\"\n\n\n\n        # configuracoes data\n        self.data_id = \"Data_carteira\"\n        self.coluna_data = \"C\"\n\n        # configuracoes swaps\n        self.coluna_passivo_swap = \"L\"\n        self.coluna_vec_swap = \"C\"\n        self.coluna_result_swap = \"O\"\n        self.coluna_diario_swap = \"N\"\n        self.swap_start_id = \"Swap \u00cdndices\"\n        self.swap_end_id = \"Total_swaps\"\n\n        #configuracoes BMF\n        self.dol_fut_id = \"DOLF\"\n        self.coluna_dol_fut = \"G\"\n\n        #configuracoes tesouro selic\n        self.tesouro_id = \"TESOURO SELIC FI RF\"\n        self.coluna_tes_financeiro = \"D\"\n\n        # configuracoes cotas\n        self.coluna_cotas = \"D\"\n        self.cotas_id = \"Cota\"\n\n        #configuracoes patrimonio\n        self.coluna_patrimonio = \"C\"\n        self.patrimonio_id = \"Patrim\u00f4nio\"\n\n        # configuracoes Termos\n        self.coluna_term_values = \"L\"\n        self.coluna_term_tot = \"K\"\n        self.term_tot_id = \"Total_termos\"\n\n        # Configuracoes acoes\n        self.acoes_id = \"Acoes\"\n        self.coluna_ativos = \"B\"\n        self.coluna_fin_ativo = \"D\"\n        self.coluna_qtd_ativo = \"E\"\n        self.coluna_preco_ativo = \"F\"\n\n        self.debug = debug\n        self.onedrive_path = onedrive_path\n        #try:\n            # tenta transferir novos pdf's do onedrive\n        onedrive_file_redirect.replace_btg_pdfs(is_first_run=True)\n        #except:\n            #logger.warning(\"Nao foi poss\u00edvel copiar carteiras_btg.xlsx para diret\u00f3rio de trading\")\n            #pass\n        self.path_to_pdfs = os.path.join(os.getcwd(), path_to_pdfs)\n        self.nmr_pdfs = len(os.listdir(self.path_to_pdfs))\n        leitor_carteira_excel.run(output_path=self.excel_path, input_path=self.path_to_pdfs)\n\n\n        self.btg_data = self.__read_excel_data()\n\n\n    def get_date(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n        # TODO remover o if quando tivermos a carteira do maratona pelo btg\n        #if carteira.lower() == \"maratona\":\n        #    carteira = \"Lipizzaner\"\n        try:\n            data = self.btg_data[carteira].loc[self.btg_data[carteira][self.coluna_ativos] == self.data_id , self.coluna_data].iloc[0]\n            return pd.to_datetime(data, dayfirst=True).date()\n        except (IndexError, KeyError) : \n\n            logger.warning(f\"Data n\u00e3o encontrada para carteira: {carteira}\")\n            return \"\"\n\n\n    def get_all_dates(self):\n        dates = []\n        for idx in self.btg_data.keys():\n            dates.append((idx, self.get_date(idx)))\n        return dates\n\n\n    def get_swap(self, carteira):\n\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        swap_found = False\n        ponta_passiva = 0\n        vencimento = \"\"\n        resultado = 0\n        result_diario = 0\n\n        # TODO apagar esse if quando maratona tiver com swap e carteira btg. (deixar apenas o else como padrao)\n        #if carteira.lower() == \"maratona\":\n        #    data = self.get_date(\"Lipizzaner\")\n        #else:\n        data = self.get_date(carteira)\n\n        status=\"btg\"\n        if carteira.lower() != \"maratona\":# TODO apagar esse if quando maratona tiver com swap e carteira btg.\n\n            for idx, row in self.btg_data[carteira].iterrows():\n                if swap_found and \"/\" in row[self.coluna_ativos]:\n                    vencimento = pd.to_datetime(row[self.coluna_vec_swap], dayfirst=True).date()\n                    if row[\"F\"] == \"REAL\":\n                        ponta_passiva += float(row[self.coluna_passivo_swap])\n                    else:\n                        ponta_passiva -= float(row[self.coluna_passivo_swap])\n                    result_diario += float(row[self.coluna_diario_swap])\n\n                elif not swap_found and row[self.coluna_ativos] == self.swap_start_id:\n                    swap_found = True    \n\n                if swap_found and row[self.coluna_ativos] == self.swap_end_id:\n                    resultado = float(row[self.coluna_result_swap])\n                    break\n\n        #if carteira == \"Global\":\n        #    carteira = \"Mangalarga Master\"\n\n        return [data, carteira, ponta_passiva, numpy.nan, resultado, result_diario, vencimento, status]\n\n\n    def __read_excel_data(self):\n\n        try:\n            data = pd.read_excel(self.excel_path, sheet_name=None, engine=\"openpyxl\")\n            # renomeando as colunas para valores de A-Z\n            for carteira in data:\n                letras = string.ascii_uppercase\n                # pode causar IndexErrorException\n                colunas = [letras[i] for i in range(len(data[carteira].columns))]\n                data[carteira].columns = colunas\n\n            return data\n\n        except FileNotFoundError:\n            logger.critical(f'Arquivo n\u00e3o encontrado em: {self.excel_path}')\n            sys.exit()\n        except IndexError:\n            logger.critical(\"O n\u00famero de colunas \u00e9 provavelmente maior que o de letras.\")\n            sys.exit()\n\n\n    def get_fund_quota(self, cnpj_key: str) : \n        \"\"\" Busca pelo fundo desejado em todas as carteiras carregadas\n            retornando o pre\u00e7o da cota mais recente e a respectiva data\n        \"\"\"\n\n        if self.nmr_pdfs != len(os.listdir(self.path_to_pdfs)):\n            #processar carteiras novamente\n            logger.trace(\"Atualizando carteiras BTG\")\n            self.nmr_pdfs = len(os.listdir(self.path_to_pdfs))\n            leitor_carteira_excel.run(output_path=self.excel_path, input_path=self.path_to_pdfs)\n            self.btg_data = self.__read_excel_data()\n\n        # SE O NOME DO FUNDO MUDAR, NAO SERA ENCONTRADO MAIS\n        funds = {\n                \"35828684000107 bz equity\"  : \"GENOA CR FIC FIM\",\n                \"35726908000161 bz equity\"  : \"CAPSTONE MAC FIC FIM\",\n                \"34581406000127 bz equity\"  : \"ACS ABS PAR II FCFIA\",\n                \"34270988000120 bz equity\"  : \"ABS PART P R FIC FIA\",\n                \"26218389000130 bz equity\"  : \"RAPTOR L FIC FIM CP\",\n                \"08833200000137 bz equity\"  : \"CSHG MULTIPLO MC FIC\",\n                \"36326771000110 bz equity\"  : \"ATMR II FC FIA\",\n                \"05936530000160 bz equity\"  : \"IP VALUE HEDGE FIC\",\n                \"23381392000181 bz equity\"  : \"PORAQUE FIC FIP\",\n                \"73232530000139 bz equity\"  : \"DYNAMO COUGAR FIA\",\n                \"32990098000168 bz equity\"  : \"THUNDERBOLT FIC FIM\",\n                \"11590051000137 bz equity\"  : \"BIJUPIRA FIP MULT\",\n                \"11589904000110 bz equity\"  : \"SALMON FIP MULTIESTR\",\n                \"19906540000167 bz equity\"  : \"TAMBAQUI FIP MULTIES\",\n                \"18489909000110 bz equity\"  : \"SPXR FIQ FIM CP IE\",\n                \"22187998000118 bz equity\"  : \"VERDE AM VII FICFIM\",\n                \"99999999999995 bz equity\"  : \"SPX SEG HAWKER JUN23\",\n                \"398838740001700 bz equity\" : \"FIDC TRYBE VQV SR1\", #\"TRYBE VQV FIDC NP\"\n                \"398838740001701 bz equity\" : \"FIDC TRYBE VQV SR2\", #FIDC TRYBE VQV SEN 2\n                \"36492461000176 bz equity\"  : \"ACS CAPSTONE FIC FIM\",\n                \"atmosac bz equity\"         : \"atmosac bz equity\",\n                \"absoppr bz equity\"         : \"absoppr bz equity\",\n                \"dyncodo bz equity\"         : \"dyncodo bz equity\",\n                \"captmac bz equity\"         : \"captmac bz equity\",\n                \"51152458000105 bz equity\"  : \"3 ILHAS FC FIA\",\n                }\n        # primeiro encontramos o nome do fundo a buscar\n        fund_name = None\n        try:\n            fund_name = funds[cnpj_key]\n        except KeyError:\n            return [0,\"0001-01-01\"]\n\n        quota_date = [0,\"0001-01-01\"]\n\n        # depois iteramos por cada dataframe\n        for carteira in self.btg_data:\n            if carteira == \"Termos\":\n                continue\n            # extraimos a data\n            date = str(self.btg_data[carteira].loc[self.btg_data[carteira][self.coluna_ativos] == self.data_id, \n                    self.coluna_data].iloc[0]).split(\"/\")\n            date.reverse()\n            date = \"-\".join(date)\n            # extraimos a cota, apenas se a data da carteira for mais recente\n            if date &gt; quota_date[1]:\n                try:\n                    quota = self.btg_data[carteira].loc[self.btg_data[carteira][self.coluna_ativos] == fund_name.upper(),\n                        self.coluna_cotas_fundos].iloc[0]                    \n                    quota = float(quota)\n                    quota_date = [quota, date]\n\n                except (IndexError, ValueError):\n                    # carteira nao possui esse fundo, vai ocorrer...\n                    pass\n\n        #if fund_name == \"3 ILHAS FIC FIA\":\n        #    quota_date = [1,\"2023-06-30\"]\n\n        if self.debug and quota_date[0] == 0:\n            logger.debug(f\"BTGDataExtractor: Nenhuma cota encontrada para {fund_name}\")\n\n        return quota_date\n\n\n    # ----- CAIXAS -----\n\n    def get_cx_outros(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.var_cx_tot_id, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var\n\n\n    def get_cx_a_pagar(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.a_pagar_id, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var    \n\n\n    def get_cx_a_receber(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.a_receber_id, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var  \n\n\n    def get_cx_a_rec_divid(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.a_rec_dividendos, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var  \n\n\n    def get_cx_despesa(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.despesa_id, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var\n\n\n    def get_cx_offshore(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.cx_offshore, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var  \n\n\n    def get_acoes_offshore(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.acoes_offshore, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var  \n\n\n    def get_ntnb(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.ntnb, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var  \n\n\n    def get_cx_taxa_adm(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_var = 0\n        try:\n            cx_var = data.loc[data[self.coluna_ativos] == self.tx_adm_id, self.coluna_var_cx_tot].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_var  \n\n\n    def get_cx_tesouro(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        cx_tesouro = 0\n        try:\n            cx_tesouro = data.loc[data[self.coluna_ativos] == self.tesouro_id, self.coluna_tes_financeiro].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return cx_tesouro\n\n\n    def get_ajuste_dol_fut(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        data = self.btg_data[carteira]\n        ajuste_fut = 0\n        try:\n            ajuste_fut = data.loc[data[self.coluna_ativos] == self.dol_fut_id, self.coluna_dol_fut].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return ajuste_fut\n\n\n    # ----- RETORNOS -----\n\n    def get_cota(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        cota = 0\n        flag = False\n        for idx, row in self.btg_data[carteira].iterrows():\n\n            if flag:\n                cota = row[self.coluna_cotas]\n                return cota\n\n            if row[self.coluna_cotas] == self.cotas_id:\n                flag = True\n\n    def get_pl(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        pl = 0\n        flag = False\n        for idx, row in self.btg_data[carteira].iterrows():\n\n            if flag:\n                pl = row[self.coluna_patrimonio]\n                return pl\n\n            if row[self.coluna_patrimonio] == self.patrimonio_id:\n                flag = True\n\n\n    def get_acoes(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n\n        flag = False\n        skip = True\n        df = pd.DataFrame({\"Date\" : [], \"Ticker\" : [], \"Pre\u00e7o\": []})\n        for idx, row in self.btg_data[carteira].iterrows():\n\n            if row[self.coluna_ativos] == \"Total_acoes\":\n                return df\n\n            if flag:\n                if skip:\n                    skip = False #pular linha\n                    continue\n                df.loc[len(df)] ={\n                                 \"Ticker\" : row[self.coluna_ativos],\n                                 \"Pre\u00e7o\" : row[self.coluna_preco_ativo],\n                                 \"Date\" : self.get_date(carteira)\n                                 }                \n\n            if row[self.coluna_ativos] == self.acoes_id:\n                flag = True\n\n\n    def get_qtd_cotas(self, carteira):\n        #if carteira.lower() == \"mangalarga master\":\n        #    carteira = \"Global\"\n\n        # TODO Remover ajuste apos fluxo de carteira do maratona\n        #if carteira.lower() == \"maratona\":\n        #    return 0\n\n        return self.get_pl(carteira)/self.get_cota(carteira)\n\n\n    def get_tot_termos(self):\n\n        data = self.btg_data[\"Termos\"]\n        tot_termos = 0\n        try:\n            tot_termos = data.loc[data[self.coluna_term_tot] == self.term_tot_id, self.coluna_term_values].iat[0]\n        except (IndexError, KeyError):\n            pass\n        return tot_termos\n</code></pre>"},{"location":"modules/carteira_online/production/btg_data_extraction/#carteira_online.production.btg_data_extraction.BTGDataExtractor.get_fund_quota","title":"<code>get_fund_quota(cnpj_key)</code>","text":"<p>Busca pelo fundo desejado em todas as carteiras carregadas retornando o pre\u00e7o da cota mais recente e a respectiva data</p> Source code in <code>carteira_online\\production\\btg_data_extraction.py</code> <pre><code>def get_fund_quota(self, cnpj_key: str) : \n    \"\"\" Busca pelo fundo desejado em todas as carteiras carregadas\n        retornando o pre\u00e7o da cota mais recente e a respectiva data\n    \"\"\"\n\n    if self.nmr_pdfs != len(os.listdir(self.path_to_pdfs)):\n        #processar carteiras novamente\n        logger.trace(\"Atualizando carteiras BTG\")\n        self.nmr_pdfs = len(os.listdir(self.path_to_pdfs))\n        leitor_carteira_excel.run(output_path=self.excel_path, input_path=self.path_to_pdfs)\n        self.btg_data = self.__read_excel_data()\n\n    # SE O NOME DO FUNDO MUDAR, NAO SERA ENCONTRADO MAIS\n    funds = {\n            \"35828684000107 bz equity\"  : \"GENOA CR FIC FIM\",\n            \"35726908000161 bz equity\"  : \"CAPSTONE MAC FIC FIM\",\n            \"34581406000127 bz equity\"  : \"ACS ABS PAR II FCFIA\",\n            \"34270988000120 bz equity\"  : \"ABS PART P R FIC FIA\",\n            \"26218389000130 bz equity\"  : \"RAPTOR L FIC FIM CP\",\n            \"08833200000137 bz equity\"  : \"CSHG MULTIPLO MC FIC\",\n            \"36326771000110 bz equity\"  : \"ATMR II FC FIA\",\n            \"05936530000160 bz equity\"  : \"IP VALUE HEDGE FIC\",\n            \"23381392000181 bz equity\"  : \"PORAQUE FIC FIP\",\n            \"73232530000139 bz equity\"  : \"DYNAMO COUGAR FIA\",\n            \"32990098000168 bz equity\"  : \"THUNDERBOLT FIC FIM\",\n            \"11590051000137 bz equity\"  : \"BIJUPIRA FIP MULT\",\n            \"11589904000110 bz equity\"  : \"SALMON FIP MULTIESTR\",\n            \"19906540000167 bz equity\"  : \"TAMBAQUI FIP MULTIES\",\n            \"18489909000110 bz equity\"  : \"SPXR FIQ FIM CP IE\",\n            \"22187998000118 bz equity\"  : \"VERDE AM VII FICFIM\",\n            \"99999999999995 bz equity\"  : \"SPX SEG HAWKER JUN23\",\n            \"398838740001700 bz equity\" : \"FIDC TRYBE VQV SR1\", #\"TRYBE VQV FIDC NP\"\n            \"398838740001701 bz equity\" : \"FIDC TRYBE VQV SR2\", #FIDC TRYBE VQV SEN 2\n            \"36492461000176 bz equity\"  : \"ACS CAPSTONE FIC FIM\",\n            \"atmosac bz equity\"         : \"atmosac bz equity\",\n            \"absoppr bz equity\"         : \"absoppr bz equity\",\n            \"dyncodo bz equity\"         : \"dyncodo bz equity\",\n            \"captmac bz equity\"         : \"captmac bz equity\",\n            \"51152458000105 bz equity\"  : \"3 ILHAS FC FIA\",\n            }\n    # primeiro encontramos o nome do fundo a buscar\n    fund_name = None\n    try:\n        fund_name = funds[cnpj_key]\n    except KeyError:\n        return [0,\"0001-01-01\"]\n\n    quota_date = [0,\"0001-01-01\"]\n\n    # depois iteramos por cada dataframe\n    for carteira in self.btg_data:\n        if carteira == \"Termos\":\n            continue\n        # extraimos a data\n        date = str(self.btg_data[carteira].loc[self.btg_data[carteira][self.coluna_ativos] == self.data_id, \n                self.coluna_data].iloc[0]).split(\"/\")\n        date.reverse()\n        date = \"-\".join(date)\n        # extraimos a cota, apenas se a data da carteira for mais recente\n        if date &gt; quota_date[1]:\n            try:\n                quota = self.btg_data[carteira].loc[self.btg_data[carteira][self.coluna_ativos] == fund_name.upper(),\n                    self.coluna_cotas_fundos].iloc[0]                    \n                quota = float(quota)\n                quota_date = [quota, date]\n\n            except (IndexError, ValueError):\n                # carteira nao possui esse fundo, vai ocorrer...\n                pass\n\n    #if fund_name == \"3 ILHAS FIC FIA\":\n    #    quota_date = [1,\"2023-06-30\"]\n\n    if self.debug and quota_date[0] == 0:\n        logger.debug(f\"BTGDataExtractor: Nenhuma cota encontrada para {fund_name}\")\n\n    return quota_date\n</code></pre>"},{"location":"modules/carteira_online/production/cash_calculator/","title":"<code>cash_calculator.py</code>","text":"<p>Realiza a \u201cquebra\u201d do caixa (Tesouro Selic, Trade RV, taxas, etc.) e calcula provis\u00f5es de trades a liquidar.</p>"},{"location":"modules/carteira_online/production/cash_calculator/#carteira_online.production.cash_calculator.CashCalculator","title":"<code>CashCalculator</code>","text":"Source code in <code>carteira_online\\production\\cash_calculator.py</code> <pre><code>class CashCalculator:\n\n    def __init__(self, input_path, today=dt.date.today()):\n\n        self.today=today\n        self.base_path = input_path\n        self.cash_base = self.load_base()\n\n\n    def load_base(self):\n        # por enquanto trata apenas os caixas offshore\n        data = pd.read_excel(self.base_path, sheet_name=\"Caixa Offshore\")\n\n        return data\n\n\n    def _calculate_trades_rv(self, fund_name, trades, start_date, source=\"offshore\"):\n        hoje = self.today\n        n_days = (hoje - start_date).days\n\n        trades_rv = [\"trade rv d0\", \"trade rv d1\", \"trade rv d2\", \"trade rv d3\",\n                    \"trade rv d4\", \"trade rv d5\", \"trade rv d6\"]\n        if n_days &gt; 6:\n            print(f\"Poss\u00edvel erro no caixa do {fund_name}. Verificar!\")\n\n        # Acumulando trades rv\n        trades_rv = trades_rv[:n_days]\n\n        trade_rv_tot = 0\n        for idx, row in trades.iterrows():\n            if row[\"Asset\"] in trades_rv:\n                trade_rv_tot += row[\"Quantidade\"]\n        return trade_rv_tot\n\n\n    def get_net_cash(self, fund_name, trades, data_extractor, query_mngr, source=\"offshore\"):\n        \"\"\"\n        Calcula o caixa l\u00edquido de um fundo, considerando trades e ajustes.\n\n        Args:\n            fund_name (str): Nome do fundo.\n            trades (pd.DataFrame): DataFrame contendo os trades do fundo.\n            data_extractor: Objeto para extrair dados espec\u00edficos (e.g., BTG).\n            query_mngr: Objeto para consultas ao banco de dados.\n            source (str, optional): Fonte do caixa ('offshore' ou 'onshore'). Defaults to \"offshore\".\n\n        Returns:\n            dict: Dicion\u00e1rio contendo o caixa e outros componentes relevantes.\n        \"\"\"\n\n\n        if source == \"offshore\":\n            # TODO pegar caixa da offshore do book gerencial\n\n            #df = self.cash_base.loc[ self.cash_base[\"Fundo\"] == fund_name ].sort_values(by=\"Data\")\n            #last_date = df[\"Data\"].iat[-1]\n            #df = df.loc[ df[\"Data\"] == last_date]\n            #cash = df[\"Caixa\"].sum() + df[\"Ajuste\"].sum()\n            #start_date = self.cash_base.loc[ self.cash_base[\"Fundo\"] == fund_name, \"Data\"].iloc[0].date()\n            #trade_rv = self._calculate_trades_rv(fund_name, trades, start_date, source=source)\n            #cash += trade_rv\n            # Obtendo a tabela de caixas para o fundo em questao\n            df = self.cash_base.query(\"Fundo == @fund_name\")[[\"Data\", \"Tipo_Conta\", \"Caixa\"]].copy()\n            print(\"-- - -- -- -- -- -- -- -- -- -- \")\n            print(df)\n            print(\"-- - -- -- -- -- -- -- -- -- -- \")\n\n            # Agrupando o caixa por tipo de conta\n            df = df.groupby(\"Tipo_Conta\").agg({\"Data\":\"last\", \"Caixa\":\"sum\"}).reset_index()\n            # Na data mais recente dos caixas informados, vamos calcular os trades rv\n            last_date = df[\"Data\"].iat[-1].date()\n\n            trade_rv = self._calculate_trades_rv(fund_name, trades, last_date, source=source)\n            # O trade rv ser\u00e1 impactado no caixa na conta asset\n            df[\"Caixa\"] = np.where(df[\"Tipo_Conta\"] == 'us asset account', df[\"Caixa\"] + trade_rv, df[\"Caixa\"])\n\n            # Retornando finalmente um dicionario quebrando o caixa por tipo de conta\n            cash = df.set_index(\"Tipo_Conta\")[\"Caixa\"].to_dict()\n\n            return {\"Caixa\" : cash, \"Trade RV\" : trade_rv}\n\n            #df = self.cash_base.loc[ self.cash_base[\"Fundo\"] == fund_name ].sort_values(by=\"Data\")\n            #last_date = df[\"Data\"].iat[-1]\n            #df = df.loc[ df[\"Data\"] == last_date]\n            #cash = df[\"Caixa\"].sum() + df[\"Ajuste\"].sum()\n            #start_date = self.cash_base.loc[ self.cash_base[\"Fundo\"] == fund_name, \"Data\"].iloc[0].date()\n            #trade_rv = self._calculate_trades_rv(fund_name, trades, start_date, source=source)\n            #cash += trade_rv\n            #\n            #return {\"Caixa\" : cash, \"Trade RV\" : trade_rv}\n            #\n\n        elif source == \"onshore\":\n            btg_extractor = data_extractor.btg_extractor\n            fund_name = fund_name.title()\n            # primeiro salva o caixa baseado na carteira do btg disponivel\n            # (valido para manga, lipi e global)\n            selic = btg_extractor.get_cx_tesouro(fund_name)\n            outros = btg_extractor.get_cx_outros(fund_name)\n            # destrinchando outros:\n            a_pagar = btg_extractor.get_cx_a_pagar(fund_name)\n            a_receber = btg_extractor.get_cx_a_receber(fund_name)\n            despesa = btg_extractor.get_cx_despesa(fund_name)\n            taxa_adm = btg_extractor.get_cx_taxa_adm(fund_name)\n\n            cx_offshore = btg_extractor.get_cx_offshore(fund_name)\n            acoes_offshore = btg_extractor.get_acoes_offshore(fund_name)\n            ntnb = btg_extractor.get_ntnb(fund_name)\n\n\n            termo = 0 if fund_name not in data_extractor.term_adj else data_extractor.term_adj[fund_name] # TODO Fazer pegar esse total direito btg_extractor.get_tot_termos()\n            a_pagar = a_pagar - termo    # Tirando termos do a pagar\n                            # PROBLEMA 1: acabando o termo, devera continuar sendo abatido ate carteira atualizar\n                            # PROBLEMA 2: novos termos s\u00f3 deverao ser abatidos depois da carteira atualizar\n            #SOLUCAO:\n            #if fund_name == \"Lipizzaner\":\n            #    ajuste_tecnico_termos = 0.0 #AJUSTAR DIRETO AQUI!!!\n            #    a_pagar += ajuste_tecnico_termos\n\n            outros = outros - a_pagar - termo - a_receber - despesa - taxa_adm - cx_offshore - acoes_offshore - ntnb\n\n\n            fut_adj = 0 #TODO REIMPLEMENTAR data_extractor.get_fut_adj(fund_name)\n\n            # corrigir caixa pelos trades realizados\n            # verficar numero de dias a serem corrigidos\n            data_btg = btg_extractor.get_date(fund_name)\n            trade_rv = self._calculate_trades_rv(fund_name, trades, data_btg, source=source)\n\n            caixa = selic + outros + a_pagar + a_receber + despesa + taxa_adm + fut_adj + trade_rv + cx_offshore\n\n            cx_liquido = caixa - despesa\n            return {\"Caixa\" : caixa, \"Caixa L\u00edquido\" : caixa,\n                    \"Tesouro\" : selic, \"Trade RV\" : trade_rv,\n                    \"A Pagar\" : a_pagar, \"A Receber\" : a_receber,\n                    \"Despesa\" : despesa, \"Taxa Adm\" : taxa_adm,\n                    \"Ajuste Futuro\" : fut_adj, \"Outros\" : outros, \"Cx Offshore\" : cx_offshore}\n</code></pre>"},{"location":"modules/carteira_online/production/cash_calculator/#carteira_online.production.cash_calculator.CashCalculator.get_net_cash","title":"<code>get_net_cash(fund_name, trades, data_extractor, query_mngr, source='offshore')</code>","text":"<p>Calcula o caixa l\u00edquido de um fundo, considerando trades e ajustes.</p> <p>Parameters:</p> Name Type Description Default <code>fund_name</code> <code>str</code> <p>Nome do fundo.</p> required <code>trades</code> <code>DataFrame</code> <p>DataFrame contendo os trades do fundo.</p> required <code>data_extractor</code> <p>Objeto para extrair dados espec\u00edficos (e.g., BTG).</p> required <code>query_mngr</code> <p>Objeto para consultas ao banco de dados.</p> required <code>source</code> <code>str</code> <p>Fonte do caixa ('offshore' ou 'onshore'). Defaults to \"offshore\".</p> <code>'offshore'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dicion\u00e1rio contendo o caixa e outros componentes relevantes.</p> Source code in <code>carteira_online\\production\\cash_calculator.py</code> <pre><code>def get_net_cash(self, fund_name, trades, data_extractor, query_mngr, source=\"offshore\"):\n    \"\"\"\n    Calcula o caixa l\u00edquido de um fundo, considerando trades e ajustes.\n\n    Args:\n        fund_name (str): Nome do fundo.\n        trades (pd.DataFrame): DataFrame contendo os trades do fundo.\n        data_extractor: Objeto para extrair dados espec\u00edficos (e.g., BTG).\n        query_mngr: Objeto para consultas ao banco de dados.\n        source (str, optional): Fonte do caixa ('offshore' ou 'onshore'). Defaults to \"offshore\".\n\n    Returns:\n        dict: Dicion\u00e1rio contendo o caixa e outros componentes relevantes.\n    \"\"\"\n\n\n    if source == \"offshore\":\n        # TODO pegar caixa da offshore do book gerencial\n\n        #df = self.cash_base.loc[ self.cash_base[\"Fundo\"] == fund_name ].sort_values(by=\"Data\")\n        #last_date = df[\"Data\"].iat[-1]\n        #df = df.loc[ df[\"Data\"] == last_date]\n        #cash = df[\"Caixa\"].sum() + df[\"Ajuste\"].sum()\n        #start_date = self.cash_base.loc[ self.cash_base[\"Fundo\"] == fund_name, \"Data\"].iloc[0].date()\n        #trade_rv = self._calculate_trades_rv(fund_name, trades, start_date, source=source)\n        #cash += trade_rv\n        # Obtendo a tabela de caixas para o fundo em questao\n        df = self.cash_base.query(\"Fundo == @fund_name\")[[\"Data\", \"Tipo_Conta\", \"Caixa\"]].copy()\n        print(\"-- - -- -- -- -- -- -- -- -- -- \")\n        print(df)\n        print(\"-- - -- -- -- -- -- -- -- -- -- \")\n\n        # Agrupando o caixa por tipo de conta\n        df = df.groupby(\"Tipo_Conta\").agg({\"Data\":\"last\", \"Caixa\":\"sum\"}).reset_index()\n        # Na data mais recente dos caixas informados, vamos calcular os trades rv\n        last_date = df[\"Data\"].iat[-1].date()\n\n        trade_rv = self._calculate_trades_rv(fund_name, trades, last_date, source=source)\n        # O trade rv ser\u00e1 impactado no caixa na conta asset\n        df[\"Caixa\"] = np.where(df[\"Tipo_Conta\"] == 'us asset account', df[\"Caixa\"] + trade_rv, df[\"Caixa\"])\n\n        # Retornando finalmente um dicionario quebrando o caixa por tipo de conta\n        cash = df.set_index(\"Tipo_Conta\")[\"Caixa\"].to_dict()\n\n        return {\"Caixa\" : cash, \"Trade RV\" : trade_rv}\n\n        #df = self.cash_base.loc[ self.cash_base[\"Fundo\"] == fund_name ].sort_values(by=\"Data\")\n        #last_date = df[\"Data\"].iat[-1]\n        #df = df.loc[ df[\"Data\"] == last_date]\n        #cash = df[\"Caixa\"].sum() + df[\"Ajuste\"].sum()\n        #start_date = self.cash_base.loc[ self.cash_base[\"Fundo\"] == fund_name, \"Data\"].iloc[0].date()\n        #trade_rv = self._calculate_trades_rv(fund_name, trades, start_date, source=source)\n        #cash += trade_rv\n        #\n        #return {\"Caixa\" : cash, \"Trade RV\" : trade_rv}\n        #\n\n    elif source == \"onshore\":\n        btg_extractor = data_extractor.btg_extractor\n        fund_name = fund_name.title()\n        # primeiro salva o caixa baseado na carteira do btg disponivel\n        # (valido para manga, lipi e global)\n        selic = btg_extractor.get_cx_tesouro(fund_name)\n        outros = btg_extractor.get_cx_outros(fund_name)\n        # destrinchando outros:\n        a_pagar = btg_extractor.get_cx_a_pagar(fund_name)\n        a_receber = btg_extractor.get_cx_a_receber(fund_name)\n        despesa = btg_extractor.get_cx_despesa(fund_name)\n        taxa_adm = btg_extractor.get_cx_taxa_adm(fund_name)\n\n        cx_offshore = btg_extractor.get_cx_offshore(fund_name)\n        acoes_offshore = btg_extractor.get_acoes_offshore(fund_name)\n        ntnb = btg_extractor.get_ntnb(fund_name)\n\n\n        termo = 0 if fund_name not in data_extractor.term_adj else data_extractor.term_adj[fund_name] # TODO Fazer pegar esse total direito btg_extractor.get_tot_termos()\n        a_pagar = a_pagar - termo    # Tirando termos do a pagar\n                        # PROBLEMA 1: acabando o termo, devera continuar sendo abatido ate carteira atualizar\n                        # PROBLEMA 2: novos termos s\u00f3 deverao ser abatidos depois da carteira atualizar\n        #SOLUCAO:\n        #if fund_name == \"Lipizzaner\":\n        #    ajuste_tecnico_termos = 0.0 #AJUSTAR DIRETO AQUI!!!\n        #    a_pagar += ajuste_tecnico_termos\n\n        outros = outros - a_pagar - termo - a_receber - despesa - taxa_adm - cx_offshore - acoes_offshore - ntnb\n\n\n        fut_adj = 0 #TODO REIMPLEMENTAR data_extractor.get_fut_adj(fund_name)\n\n        # corrigir caixa pelos trades realizados\n        # verficar numero de dias a serem corrigidos\n        data_btg = btg_extractor.get_date(fund_name)\n        trade_rv = self._calculate_trades_rv(fund_name, trades, data_btg, source=source)\n\n        caixa = selic + outros + a_pagar + a_receber + despesa + taxa_adm + fut_adj + trade_rv + cx_offshore\n\n        cx_liquido = caixa - despesa\n        return {\"Caixa\" : caixa, \"Caixa L\u00edquido\" : caixa,\n                \"Tesouro\" : selic, \"Trade RV\" : trade_rv,\n                \"A Pagar\" : a_pagar, \"A Receber\" : a_receber,\n                \"Despesa\" : despesa, \"Taxa Adm\" : taxa_adm,\n                \"Ajuste Futuro\" : fut_adj, \"Outros\" : outros, \"Cx Offshore\" : cx_offshore}\n</code></pre>"},{"location":"modules/carteira_online/production/historical_data_extraction/","title":"<code>historical_data_extraction.py</code> (LEGACY)","text":"<p>Calculava NDF swap di\u00e1rio at\u00e9 2023.</p>"},{"location":"modules/carteira_online/production/historical_data_extraction/#carteira_online.production.historical_data_extraction.HistoricalDataExtractor","title":"<code>HistoricalDataExtractor</code>","text":"Source code in <code>carteira_online\\production\\historical_data_extraction.py</code> <pre><code>class HistoricalDataExtractor:\n\n    def __init__(self, bases_path=\"bases_historicas\", bases_filename=\"historical_data.xlsx\", today=dt.date.today()):\n        self.today = today\n        self.bases_path = bases_path\n        self.bases_filename = bases_filename\n        self.bases_cash_filename = \"historical_cash.xlsx\"\n\n    def update_swap(self, fund_name, btg_extractor, query_mngr):\n        \"\"\"Carrega a base de swaps, pega ponta passiva e ajuste do btg, \n        atualiza ate data do dia anterior usando variacao do d\u00f3lar.\n\n        Return : dados do ultimo swap na base\n        \"\"\"\n\n        modified = False\n\n        #Primeiro carregamos a base historica de swaps\n        swap_base = pd.read_excel(os.path.join(os.getcwd(),self.bases_path, self.bases_filename), engine=\"openpyxl\", sheet_name=\"swap\")\n\n        swap_base[\"Data\"] = swap_base[\"Data\"].dt.date\n        swap_base_unchanged = swap_base.loc[ swap_base[\"Fundo\"] != fund_name ]\n        swap_base = swap_base.loc[ swap_base[\"Fundo\"] == fund_name ].sort_values(by=\"Data\")\n        # pegar swap do btg\n        swap_btg_last = btg_extractor.get_swap(fund_name)\n\n        # agora recuperamos a cotacao do dolar de todos os dias at\u00e9 o mais recente na carteira\n        start_date = swap_base[\"Data\"].iat[-1]  # ultimo registro da planilha\n        end_date = swap_btg_last[0]             # data da carteira btg\n        print(f\" ----------------&gt; &lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt; {swap_btg_last} &lt;&gt;&lt;&gt; {start_date} &lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\")\n\n        # Precisamos desconsiderar o resultado ja liquidado\n        if fund_name == \"Lipizzaner\":\n\n            swap_btg_last[4] -= 0#-4714777.60#-4017674#-202252.0 #-1052406.4\n\n\n        if start_date &lt; end_date:   #Base desatualizada. Vamos atualizar...\n            print(\"colocando datas ate btg\")\n            modified = True\n            #-&gt; precisamos adicionar rows ate igualar com btg\n            last_exp = swap_base[\"Exposi\u00e7\u00e3o\"].iat[-1]\n            last_acc = swap_base[\"Acumulado\"].iat[-1]\n            venc = swap_base[\"Vencimento\"].iat[-1]\n            #dol_hist = self.get_hist_dol(start_date, end_date, flds=\"chg_pct_1d\").apply(lambda x: x/100, axis=1)\n            for i in range(int( (end_date-start_date).days ) + 1):\n                # ordem \u00e9: Data, Fundo, Exposi\u00e7\u00e3o, Varia\u00e7\u00e3o Dolar, Acumulado, Di\u00e1rio, Vencimento, Status\n                new_row = [start_date + dt.timedelta(days=i), fund_name] #idx eh a data\n                st_dt = start_date + (i-1)*dt.timedelta(days=1)\n                ed_dt = start_date + dt.timedelta(days=i)\n                new_exp = last_exp * (1 + query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=st_dt, recent_date=ed_dt))\n                new_acc = last_acc + (last_exp-new_exp)\n                daily_res = new_acc - last_acc\n                new_row += [new_exp, query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=st_dt, recent_date=ed_dt), new_acc, daily_res, venc, \"ok\"]\n                #adicionando novo row na base\n                swap_base.loc[swap_base.index[-1]+1] = new_row\n\n                last_exp = new_exp\n                last_acc = new_acc\n\n            swap_base.at[swap_base.index[-1]+1] = swap_btg_last\n            swap_base[\"Varia\u00e7\u00e3o D\u00f3lar\"].at[swap_base.index[-1]] = query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=end_date-dt.timedelta(days=1), recent_date=end_date)\n\n        elif start_date &gt;= end_date:\n            # Nesse caso a base est\u00e1 atualizada e pode possuir valores para serem ajustados pelo btg\n\n            if swap_base.loc[swap_base[\"Data\"] == end_date, \"Status\"].iloc[0] != \"btg\":\n                # nesse caso dados na data do btg precisam ser ajustados\n\n\n\n                swap_base.loc[ swap_base[\"Data\"] == end_date, swap_base.columns ] = swap_btg_last\n                modified = True\n            else:\n                # precisamos alterar o start_date, pois os dados ja estao atualizados\n                end_date = swap_base[\"Data\"].iat[-1]\n\n        # setado ultimo valor do btg, atualizamos tudo ate a data de hoje\n        start_date = end_date\n        yesterday = self.today - dt.timedelta(days=1) \n        end_date = yesterday\n        if start_date &lt; end_date:\n\n            modified = True\n            #precisamos adicionar rows ate ontem\n            # precisamos cortar a base na data do ultimo btg\n            swap_base = swap_base.loc[swap_base[\"Data\"] &lt;= start_date]\n            # pegamos os dados da ultima linha na base\n            last_exp = swap_base[\"Exposi\u00e7\u00e3o\"].iat[-1]\n            last_acc = swap_base[\"Acumulado\"].iat[-1]\n            venc = swap_base[\"Vencimento\"].iat[-1]\n            #dol_hist = self.get_hist_dol(start_date, end_date, flds=\"chg_pct_1d\").apply(lambda x: x/100, axis=1)\n\n            for i in range(1, int( (end_date-start_date).days ) + 1):\n                # ordem \u00e9: Data, Fundo, Exposi\u00e7\u00e3o, Varia\u00e7\u00e3o Dolar, Acumulado, Di\u00e1rio, Vencimento, Status\n                new_row = [start_date + dt.timedelta(days=i), fund_name] #idx eh a data\n                st_dt = start_date + (i-1)*dt.timedelta(days=1)\n                ed_dt = start_date + dt.timedelta(days=i)\n                new_exp = last_exp * (1 + query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=st_dt, recent_date=ed_dt))\n                new_acc = last_acc + (last_exp-new_exp)\n                daily_res = new_acc - last_acc\n                new_row += [new_exp, query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=st_dt, recent_date=ed_dt), new_acc, daily_res, venc, \"ok\"]\n                #adicionando novo row na base\n                swap_base.loc[swap_base.index[-1]+1] = new_row\n\n                last_exp = new_exp\n                last_acc = new_acc\n\n        swap_data = swap_base.tail(1)\n        # Para finalizar, concatenamos a parte modificada ao restante da base\n        # e salvamos sobrescrevendo o arquivo anterior\n        swap_base = pd.concat([swap_base_unchanged,swap_base])\n\n        if modified:\n            # somente reescreve a base caso alguma modificacao tenha sido feita\n            self.overwrite_excel_sheet(os.path.join(self.bases_path, self.bases_filename), sheetname=\"swap\", dataframe=swap_base)\n\n        return swap_data\n\n\n    #def get_hist_dol(self, start, end, flds=\"px_last\", dol_ticker=\"usdbrl bgn curncy\"):\n    #    return blp.bdh(dol_ticker, start_date=start, end_date=end, flds=flds)\n\n\n    def overwrite_excel_sheet(self, filename, sheetname, dataframe):\n\n        #try:\n        dataframe.to_excel(filename, sheet_name=sheetname, index=False)\n        #except:\n        #    print(\"Falha ao salvar base de swaps\")\n        \"\"\"print(\"AQUUIIIIIIIII&lt;&gt;&lt;&gt;&lt;&gt;&lt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;\")\n        with pd.ExcelWriter(filename, engine='openpyxl', mode='a') as writer: \n            workBook = writer.book\n            try:\n                workBook.remove(workBook[sheetname])\n            except:\n                print(f\"Worksheet {sheetname} n\u00e3o existe.\")\n            finally:\n                dataframe.to_excel(writer, sheet_name=sheetname,index=False)\n                writer.save()\n        print(\"&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;AQUUIIIIIIIII&lt;&gt;&lt;&gt;&lt;&gt;&lt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;\")\"\"\"\n</code></pre>"},{"location":"modules/carteira_online/production/historical_data_extraction/#carteira_online.production.historical_data_extraction.HistoricalDataExtractor.update_swap","title":"<code>update_swap(fund_name, btg_extractor, query_mngr)</code>","text":"<p>Carrega a base de swaps, pega ponta passiva e ajuste do btg,  atualiza ate data do dia anterior usando variacao do d\u00f3lar.</p> <p>Return : dados do ultimo swap na base</p> Source code in <code>carteira_online\\production\\historical_data_extraction.py</code> <pre><code>def update_swap(self, fund_name, btg_extractor, query_mngr):\n    \"\"\"Carrega a base de swaps, pega ponta passiva e ajuste do btg, \n    atualiza ate data do dia anterior usando variacao do d\u00f3lar.\n\n    Return : dados do ultimo swap na base\n    \"\"\"\n\n    modified = False\n\n    #Primeiro carregamos a base historica de swaps\n    swap_base = pd.read_excel(os.path.join(os.getcwd(),self.bases_path, self.bases_filename), engine=\"openpyxl\", sheet_name=\"swap\")\n\n    swap_base[\"Data\"] = swap_base[\"Data\"].dt.date\n    swap_base_unchanged = swap_base.loc[ swap_base[\"Fundo\"] != fund_name ]\n    swap_base = swap_base.loc[ swap_base[\"Fundo\"] == fund_name ].sort_values(by=\"Data\")\n    # pegar swap do btg\n    swap_btg_last = btg_extractor.get_swap(fund_name)\n\n    # agora recuperamos a cotacao do dolar de todos os dias at\u00e9 o mais recente na carteira\n    start_date = swap_base[\"Data\"].iat[-1]  # ultimo registro da planilha\n    end_date = swap_btg_last[0]             # data da carteira btg\n    print(f\" ----------------&gt; &lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt; {swap_btg_last} &lt;&gt;&lt;&gt; {start_date} &lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\")\n\n    # Precisamos desconsiderar o resultado ja liquidado\n    if fund_name == \"Lipizzaner\":\n\n        swap_btg_last[4] -= 0#-4714777.60#-4017674#-202252.0 #-1052406.4\n\n\n    if start_date &lt; end_date:   #Base desatualizada. Vamos atualizar...\n        print(\"colocando datas ate btg\")\n        modified = True\n        #-&gt; precisamos adicionar rows ate igualar com btg\n        last_exp = swap_base[\"Exposi\u00e7\u00e3o\"].iat[-1]\n        last_acc = swap_base[\"Acumulado\"].iat[-1]\n        venc = swap_base[\"Vencimento\"].iat[-1]\n        #dol_hist = self.get_hist_dol(start_date, end_date, flds=\"chg_pct_1d\").apply(lambda x: x/100, axis=1)\n        for i in range(int( (end_date-start_date).days ) + 1):\n            # ordem \u00e9: Data, Fundo, Exposi\u00e7\u00e3o, Varia\u00e7\u00e3o Dolar, Acumulado, Di\u00e1rio, Vencimento, Status\n            new_row = [start_date + dt.timedelta(days=i), fund_name] #idx eh a data\n            st_dt = start_date + (i-1)*dt.timedelta(days=1)\n            ed_dt = start_date + dt.timedelta(days=i)\n            new_exp = last_exp * (1 + query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=st_dt, recent_date=ed_dt))\n            new_acc = last_acc + (last_exp-new_exp)\n            daily_res = new_acc - last_acc\n            new_row += [new_exp, query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=st_dt, recent_date=ed_dt), new_acc, daily_res, venc, \"ok\"]\n            #adicionando novo row na base\n            swap_base.loc[swap_base.index[-1]+1] = new_row\n\n            last_exp = new_exp\n            last_acc = new_acc\n\n        swap_base.at[swap_base.index[-1]+1] = swap_btg_last\n        swap_base[\"Varia\u00e7\u00e3o D\u00f3lar\"].at[swap_base.index[-1]] = query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=end_date-dt.timedelta(days=1), recent_date=end_date)\n\n    elif start_date &gt;= end_date:\n        # Nesse caso a base est\u00e1 atualizada e pode possuir valores para serem ajustados pelo btg\n\n        if swap_base.loc[swap_base[\"Data\"] == end_date, \"Status\"].iloc[0] != \"btg\":\n            # nesse caso dados na data do btg precisam ser ajustados\n\n\n\n            swap_base.loc[ swap_base[\"Data\"] == end_date, swap_base.columns ] = swap_btg_last\n            modified = True\n        else:\n            # precisamos alterar o start_date, pois os dados ja estao atualizados\n            end_date = swap_base[\"Data\"].iat[-1]\n\n    # setado ultimo valor do btg, atualizamos tudo ate a data de hoje\n    start_date = end_date\n    yesterday = self.today - dt.timedelta(days=1) \n    end_date = yesterday\n    if start_date &lt; end_date:\n\n        modified = True\n        #precisamos adicionar rows ate ontem\n        # precisamos cortar a base na data do ultimo btg\n        swap_base = swap_base.loc[swap_base[\"Data\"] &lt;= start_date]\n        # pegamos os dados da ultima linha na base\n        last_exp = swap_base[\"Exposi\u00e7\u00e3o\"].iat[-1]\n        last_acc = swap_base[\"Acumulado\"].iat[-1]\n        venc = swap_base[\"Vencimento\"].iat[-1]\n        #dol_hist = self.get_hist_dol(start_date, end_date, flds=\"chg_pct_1d\").apply(lambda x: x/100, axis=1)\n\n        for i in range(1, int( (end_date-start_date).days ) + 1):\n            # ordem \u00e9: Data, Fundo, Exposi\u00e7\u00e3o, Varia\u00e7\u00e3o Dolar, Acumulado, Di\u00e1rio, Vencimento, Status\n            new_row = [start_date + dt.timedelta(days=i), fund_name] #idx eh a data\n            st_dt = start_date + (i-1)*dt.timedelta(days=1)\n            ed_dt = start_date + dt.timedelta(days=i)\n            new_exp = last_exp * (1 + query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=st_dt, recent_date=ed_dt))\n            new_acc = last_acc + (last_exp-new_exp)\n            daily_res = new_acc - last_acc\n            new_row += [new_exp, query_mngr.get_pct_change(\"usdbrl curncy\", previous_date=st_dt, recent_date=ed_dt), new_acc, daily_res, venc, \"ok\"]\n            #adicionando novo row na base\n            swap_base.loc[swap_base.index[-1]+1] = new_row\n\n            last_exp = new_exp\n            last_acc = new_acc\n\n    swap_data = swap_base.tail(1)\n    # Para finalizar, concatenamos a parte modificada ao restante da base\n    # e salvamos sobrescrevendo o arquivo anterior\n    swap_base = pd.concat([swap_base_unchanged,swap_base])\n\n    if modified:\n        # somente reescreve a base caso alguma modificacao tenha sido feita\n        self.overwrite_excel_sheet(os.path.join(self.bases_path, self.bases_filename), sheetname=\"swap\", dataframe=swap_base)\n\n    return swap_data\n</code></pre>"},{"location":"modules/carteira_online/production/leitor_carteira_excel/","title":"<code>leitor_carteira_excel.py</code>","text":"<p>L\u00ea os arquivos .xlsx de carteira BTG mais recentes e compila num formato padronizado, gerando uma aba por fundo.</p>"},{"location":"modules/carteira_online/production/onedrive_file_redirect/","title":"<code>onedrive_file_redirect.py</code>","text":"<p>Camada de abstra\u00e7\u00e3o para resolver caminhos do OneDrive </p>"},{"location":"modules/carteira_online/production/portfolio_builder/","title":"<code>portfolio_builder.py</code>","text":"<p>Script-chave que monta a base base_portfolios.xlsx, atualiza <code>base_carteira_online_v12.xlsx</code> e <code>hist_risk_metrics.xlsx</code>.</p>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder","title":"<code>PortfolioBuilder</code>","text":"Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>class PortfolioBuilder:\n\n    def __init__(self, today=dt.date.today()):\n\n        self.is_develop_branch = os.path.basename(os.getcwd()).split(\"_\")[-1] == \"develop\"\n        self.today = today#dt.date(2023,2,22)#today\n\n\n        # padronizado para cd .. root folder -&gt; os.pardir\n        self.onedrive_path = os.pardir\n\n        # hexadecimal das cores disponiveis\n        self.colors = {\n            \"azul_escuro\" : \"#0f00ff\",\n            \"azul_claro\" : \"#0091ff\",\n            \"marrom\" : \"#7B3214\",\n            \"verde\" : \"#04a200\",\n            \"amarelo\" : \"#e5b900\",\n            \"cinza\" : \"#a6b5c1\",\n            \"vermelho\" : \"#ff0014\",\n            \"vinho\" : \"#904451\",\n        }\n\n        # Define a cor que cada grupo de ativos ter\u00e1\n        self.color_map = {\n            \"a\u00e7\u00f5es\"      : self.colors[\"azul_escuro\"],\n            \"termo\"      : self.colors[\"vermelho\"],\n            \"caixa\"      : self.colors[\"cinza\"],\n            \"caixa_us\"   : self.colors[\"cinza\"],\n            \"swap\"       : self.colors[\"azul_escuro\"],\n            \"futuro\"     : self.colors[\"azul_escuro\"],\n            \"fut_dol\"    : self.colors[\"azul_escuro\"],\n            \"fundos\"     : self.colors[\"verde\"],\n            \"fundos_fia\" : self.colors[\"azul_claro\"],\n            \"rf_privada\" : self.colors[\"vinho\"],\n            \"loan\"       : self.colors[\"azul_escuro\"],\n            \"vc\"         : self.colors[\"amarelo\"],\n            \"vc agro\"    : self.colors[\"marrom\"],\n            \"cambio\"     : self.colors[\"azul_escuro\"],\n            \"metricas\"   : self.colors[\"azul_escuro\"],\n            \"fundo_vc\"   : self.colors[\"azul_escuro\"],\n            \"spac\"       : self.colors[\"azul_claro\"],\n            \"trade rv\"   : self.colors[\"azul_escuro\"],\n            \"margin\"     : self.colors[\"azul_escuro\"],\n            \"swap_eurusd\": self.colors[\"azul_escuro\"],\n            \"forward\"    : self.colors[\"azul_escuro\"],\n            \"interest\"           : self.colors[\"azul_escuro\"],\n            \"currency coupon\"    : self.colors[\"azul_escuro\"],\n            \"luxor\"      : self.colors[\"azul_escuro\"],\n            \"ntnb\"       : self.colors[\"cinza\"],\n            \"bmf\"        : self.colors[\"azul_escuro\"],\n            \"curncy\"        : self.colors[\"azul_escuro\"],\n            \"ndf\"          : self.colors[\"cinza\"],\n            \"fx\"          : self.colors[\"cinza\"],\n            \"private equity\" : self.colors[\"amarelo\"],\n            \"benchmark\" : self.colors[\"cinza\"],\n            \"margin cash\": self.colors[\"vermelho\"],\n            \"equity_fund\" : self.colors[\"azul_claro\"],\n            \"taxas e custos\"   : self.colors[\"cinza\"],\n            \"cripto\"   : self.colors[\"cinza\"],\n\n        }\n\n        # Define a ordem das divisoes do\n        # grafico (com base na coluna grupo)\n        # 1-a\u00e7\u00f5es, 2-spacs, 3-fundos_3os, 4-vc,\n        # 5-loan,caixa e swap\n        # -&gt; Quanto mais negativo, mais no\n        # topo aparecer\u00e1\n        self.order_modifiers = {\n            \"a\u00e7\u00f5es\"      : -9_000000000,\n            \"termo\"      : -6_000000000,\n            \"caixa\"      : -2_000000000,\n            \"caixa_us\"   : -2_000000000,\n            \"swap\"       : -2_000000000,\n            \"futuro\"     : -2_000000000,\n            \"fut_dol\"    : -2_000000000,\n            \"fundos\"     : -7_000000000,\n            \"fundos_fia\" : -8_000000000,\n            \"rf_privada\" : -4_000000000,\n            \"loan\"       : -3_000000000,\n            \"vc\"         : -5_000000000,\n            \"vc agro\"    : -1_000000000,\n            \"cambio\"     : -3_000000000,\n            \"metricas\"   : 0,\n            \"fundo_vc\"   : -9_000000000,\n            \"spac\"       : -8_000000000,\n            \"spac_w\"     : -8_000000000,\n            \"trade rv\"   : 0,\n            \"margin\"     : -2_000000000,\n            \"swap_eurusd\": -2_000000000,\n            \"forward\"    : -2_000000000,\n            \"interest\"           : -1_000000000,\n            \"currency coupon\"    : -1_000000000,\n            \"luxor\"      : -9_000000000,\n            \"ntnb\"       : -2_000000000,\n            \"bmf\"       : -3_000000000,\n            \"curncy\"       : -3_000000000,\n            \"ndf\"       : -3_000000000,\n            \"fx\"       : -3_000000000,\n            \"private equity\"       : -5_000000000,\n            \"benchmark\" : -2_000000000,\n            \"margin cash\" : -3_000000000,\n            \"equity_fund\" : -8_000000000,\n            \"taxas e custos\"   : -2_000000000,\n            \"cripto\"   : -1_000000000,\n        }\n\n        # alguns agrupamentos de nomes para facilitar edicoes futuras\n        # -&gt; Todo novo fundo do portfolio devera ser inserido nos grupos certos abaixo\n        self.fund_names = {\n            \"luxor_ptf\" : [\"fund_a\", \"all_eqt\", \"all_eqt_acoes\", \"lipizzaner\", \"mangalarga_fic_fia\", \"maratona\"],\n\n            \"all\" : [\"fund_a\", \"all_eqt\", \"all_eqt_acoes\", \"lipizzaner\", \"lipizzaner_acoes\", \"mangalarga_fic_fia\",\n                     \"fund_a_ex_vc\", \"fund_b\", \"fund_b_ex_vc\", \"hmx\",\n                     \"hmx_ex_vc\", \"edel_lipi\", \"edel_lipi_acoes\", \"maratona\", \"maratona_acoes\", \"mrtn_hmx\"],\n\n            \"partial\" : [\"fund_a_ex_vc\", \"lipizzaner_acoes\", \"fund_b_ex_vc\", \"hmx_ex_vc\",\n                         \"all_eqt_acoes\", \"edel_lipi_acoes\", \"maratona_acoes\"],\n\n            \"vc_funds\": [\"fund_a\", \"fund_b\",\"hmx\"],\n\n            \"vc_funds_all\" : [\"fund_a\", \"all_eqt\", \"all_eqt_acoes\", \"lipizzaner\", \"hmx\", \"fund_b\", \"edel_lipi\", \"edel_lipi_acoes\", \"mangalarga_fic_fia\", \"mrtn_hmx\"],\n\n            \"trading\" : [\"lipizzaner_acoes\", \"fund_a_ex_vc\", \"fund_b_ex_vc\", \"hmx_ex_vc\", \"maratona_acoes\"],\n\n            \"dol_hedge\" : [\"lipizzaner\", \"lipizzaner_acoes\", \"edel_lipi\", \"edel_lipi_acoes\", \"maratona_acoes\", \"mrtn_hmx\"],\n\n            \"onshore\" : [\"lipizzaner\", \"mangalarga fic fia\", \"maratona\"],\n\n            \"offshore\" : [\"fund a\", \"fund b\", \"hmx\"],\n\n            \"simulated\" : [\"all_eqt\", \"all_eqt_acoes\", \"edel_lipi_acoes\", \"edel_lipi\", \"mrtn_hmx\"],\n\n            \"iliquids_funds\" : [\"fund_a\", \"all_eqt\", \"all_eqt_acoes\", \"lipizzaner\", \"mangalarga_fic_fia\", \"maratona\", \n                                \"fund_b\", \"hmx\", \"edel_lipi\", \"mrtn_hmx\"],\n\n        }\n\n        self.partial_to_complete_name_map = {\n            \"fund_a_ex_vc\" : 'fund_a', \"lipizzaner_acoes\" : \"lipizzaner\", \"fund_b_ex_vc\" : 'fund_b', \"hmx_ex_vc\" : 'hmx',\n                         \"all_eqt_acoes\" : 'lipizzaner', \"edel_lipi_acoes\" : 'edel_lipi', \"maratona_acoes\": 'maratona'\n        }\n\n        # criando modulos para extracao de dados do mercado e do btg\n\n        self.query_mngr = LuxorQuery(is_develop_mode=False)\n\n        self.data_extractor = adex.AssetDataExtractor(onedrive_path=self.onedrive_path, query_mngr=self.query_mngr,today=self.today)\n\n        self.yesterday = self.query_mngr.get_bday_offset(self.today, offset=-1, location=\"all\")\n\n        # Fixando nomes de arquivos e diretorios usado\n        # s\n        self.input_bases_dir = Path(\"bases_input\")\n        self.input_base_file = \"input_data.xls\"\n\n        #self.assets_base_name = \"assets_script_input.xlsx\"\n        #self.funds_base_name = \"fundos_script_input.xlsx\"\n\n        self.program_running = True\n\n\n    def build(self):\n\n        # Cotacao do dolar atual\n        self.USD_BRL = self.query_mngr.get_price(\"usdbrl curncy\", px_date=self.today)\n\n        # Carregando base de ativos\n        self.assets_db = self.__load_assets_base(self.input_base_file)\n\n        # Definindo as cores de cada ativo\n        self.assets_db = self.__set_color_hex(self.assets_db)\n\n        # -&gt; A forma de pegar os precos pode ser a mesma\n        self.funds_db = self.__load_funds_base(self.input_base_file)\n\n        # Juntando as colunas dos dados carregados em uma tabela apenas\n        merged_data = self.merge_data(self.assets_db, self.funds_db)\n\n        logger.info(f\"D\u00f3lar: {self.USD_BRL}\")\n        # Preencher coluna de precos\n        merged_data = self.populate_prices_BBG(merged_data)\n\n        # Pegar os caixas. Sera gerado um df com caixa pra cada fundo \n        # Precisa ser feito depois de pegar os precos, para nao sobrescrever\n        merged_data = self.get_caixas(merged_data)\n\n        # caixas calculados, removemos trades_rv da base\n        merged_data = merged_data.drop(merged_data[merged_data[\"Grupo\"] == \"trade rv\"].index)\n\n        # Preencher com localizacao do investimento: onshore ou offshore\n        merged_data = self.populate_location(merged_data)\n\n        # Calcular valor de mercado de cada ativo\n        merged_data = self.calculate_MktVal(merged_data)\n\n        # Ajustar cor dos negativos(short) \n        merged_data = self.__update_colors(merged_data)\n\n        # Separar bases dos fundos nos fundos como queremos olhar\n        self.funds = self.segregate_funds(merged_data)\n        print(self.funds[\"lipizzaner\"].columns)\n\n        # Computar os pls de cada base segregada\n        self.pls = self.get_pls(self.funds)\n        # DEBUG:\n        #print()\n        #logger.info(self.pls)\n\n        # Calcular percentuais, dividindo Mkt_val pelo pl respectivo\n        self.funds = self.calculate_percentages(self.funds, self.pls)\n\n\n        #self.calculate_day_attr(self.funds)\n        self.funds = self.set_date_and_usdbrl(self.funds)\n\n        # Gera output da base total com todos os ativos discriminados\n        self.save_portfolio_full_database(self.funds)\n\n        #self.non_groupable_vcs = self.get_non_groupable_vcs(self.funds)\n        self.non_groupable_iliquids = self.get_non_groupable_iliquids(self.funds)\n\n        self.non_groupable_iliquids_managers = self.get_non_groupable_iliquids_managers(self.funds)\n\n        # -&gt; Agrupar os vcs (regra &lt; 1%)\n        #self.funds = self.group_vcs(self.funds, self.non_groupable_vcs)\n        self.funds = self.group_iliquids(self.funds, self.non_groupable_iliquids_managers)\n\n        self.risk_df = self.compute_risk_metrics(self.funds)\n\n        # Agrupa os ativos com mesmo nome, alterando o nome mostrado\n        self.process_assets_concat(self.funds)\n\n        # Ajustando as cores mais uma vez, pois pode virar positivo/negativo ap\u00f3s agregacao\n        for fund_name, fund_df in self.funds.copy().items():\n            self.funds[fund_name] = self.__update_colors(fund_df)\n\n        self.funds = self.define_assets_order(self.funds)\n\n        # Salvando dados tratados para geracao do relatorio\n        self.dfs_to_save = self.funds\n        self.dfs_to_save[\"Metricas de Risco\"] = self.risk_df\n        self.dfs_to_save[\"Info Execu\u00e7\u00f5es\"] = self.__create_exec_info()\n        self.dfs_to_save[\"Lipi Investors\"] = self.__create_lipi_investors_table()\n        fund_info_df = pd.DataFrame({\"Fundo\" : list(self.pls.keys()), \"PL\" : list(self.pls.values())})\n        self.dfs_to_save[\"Info Fundos\"] = fund_info_df\n        self.dfs_to_save[\"Changes\"] = self.compute_positions_variation()\n        self.dfs_to_save[\"Metrics_changes\"] = self.compute_metrics_variation()\n\n        self.save_portfolio(self.dfs_to_save)\n        self.save_portfolio_risk_metrics()\n\n\n    def __load_assets_base(self, filename):\n        try:\n\n            data = pd.read_excel(self.input_bases_dir/filename, sheet_name=\"Ativos\")\n            data = data[ data.columns[:8] ]     # Selecionando 8 primeiras colunas\n            data = data[data[\"Asset\"].notna()]  # Ignora rows na (ativos nao lancados)\n\n            # garantindo remocao da coluna de pre\u00e7os e location\n            data = data.iloc[:, 0:6]\n\n            # converte todo o texto para lower case\n            data = data.apply(lambda x: x.astype(str).str.lower())\n\n            # removendo espacos em branco no inicio e no fim de cada texto\n            data = data.apply(lambda row: row.str.strip())\n\n            # cria uma coluna de precos sem dados ainda\n            data[\"Price\"] = np.nan\n\n            return data\n\n        except FileNotFoundError:\n            logger.error(f\"Arquivo {filename} n\u00e3o encontrado em {self.input_bases_dir}\")\n            logger.error(\"Finalizando...\")\n            sys.exit()\n\n    def __set_color_hex(self, data):\n        colors = []\n\n        for idx, row in data.iterrows():\n            color_key = row[\"Grupo\"] if row[\"Tipo\"] not in [\"spac\", \"spac_w\"] else row[\"Tipo\"][:4]\n            colors.append(self.color_map[color_key])\n\n        data[\"Cor\"] = colors\n\n        return data\n\n\n    def __update_colors(self, data):\n        \"\"\"Atualiza a cor com base em algum criterio.\n        Por enquanto, apenas coloca como vermelho se o\n        market value for negativo.\n        \"\"\"\n        for idx, row in data.iterrows():\n            if row[\"Mkt_Value\"] &lt; 0:\n                data.loc[idx, \"Cor\"] = self.colors[\"vermelho\"]\n            else:\n                data.loc[idx, \"Cor\"] = self.color_map[row[\"Grupo\"]]\n\n        return data\n\n\n    def __load_funds_base(self, filename):\n        \"\"\"\n            Carrega base de dados com as carteiras dos fundos.\n            Concatena todas numa tabela s\u00f3.\n            Args:\n                file_name (str): caminho para arquivo .xlsx\n\n            Returns:\n                Dataframe de pandas\n        \"\"\"\n        try:\n            data = pd.read_excel(self.input_bases_dir/filename, sheet_name=[\"Lipizzaner\", \"Fund A\", \"Fund B\", \"HMX\", \"Mangalarga FIC\", \"Maratona\"])        \n\n            # tratando dados:\n            for k, v in data.items():\n                data[k] = v[ v.columns[:4] ].dropna(how=\"any\")\n                data[k] = data[k].loc[ ((data[k][\"Quantidade\"] != 0) | (data[k][\"Ticker\"] == \"Caixa (a receber)\")), :]\n\n            # caixa a receber\n            f_data = data[\"Fund A\"]\n            idx_mod = f_data.loc[((f_data[\"Asset\"] == \"Caixa (a receber)\") &amp; (f_data[\"Ticker\"] == \"Caixa (a receber)\"))].index[0]\n\n            f_data.at[idx_mod, \"Quantidade\"] = 0/self.USD_BRL\n            data[\"Fund A\"] = f_data.copy()\n\n            f_data = data[\"Lipizzaner\"]\n            idx_mod = f_data.loc[((f_data[\"Asset\"] == \"Caixa (a receber)\") &amp; (f_data[\"Ticker\"] == \"Caixa (a receber)\"))].index[0]\n            f_data.at[idx_mod, \"Quantidade\"] = 0/self.USD_BRL # global vai multiplicar pelo mesmo dolar depois e netar\n            data[\"Lipizzaner\"] = f_data.copy()\n\n            # Seguiremos com os dados concatenados para facilitar por enquanto\n            data = pd.concat(data.values(), ignore_index=True)\n\n            # Antes de converter para lower case, copiar coluna Assets\n            assets_copy = data[\"Asset\"]\n            # converte todo o texto para lower case\n            data = data.apply(lambda row: row.astype(str).str.lower())\n\n            # remove espacos em branco do inicio e do fim de cada entrada\n            data = data.apply(lambda row: row.str.strip())\n\n            # coluna quantidade precisa ser reconvertida para numerico\n            try:\n                data[\"Quantidade\"] = pd.to_numeric(data[\"Quantidade\"]).round(8)\n            except:\n                logger.error(\"Voce errou, humano.\")\n                logger.error(\"Tem texto onde era pra ter quantidade.\")\n                logger.error(data[\"Quantidade\"])\n                data.to_excel(\"error.xlsx\")\n                sys.exit()\n\n            # Inserindo coluna com nomes personalizavel\n            data[\"Nome\"] = assets_copy\n            data[\"Nome\"] = data[\"Nome\"].apply(lambda x: x.replace(\" |\", \"|\").replace(\"| \", \"|\"))\n\n            cols = [\"Nome\"] + [col for col in data.columns if col != \"Nome\"]\n\n            data = data[cols]\n\n            return data\n\n        except FileNotFoundError:\n            logger.error(f\"Arquivo {filename} n\u00e3o encontrado em {self.input_bases_dir}\")\n            logger.error(\"Finalizando...\")\n            sys.exit()\n\n\n    def merge_data(self, assets_data, funds_data):\n        \"\"\"Realiza jun\u00e7\u00e3o interna sobre as colunas 'Asset' e 'Ticker'\n        Args:\n            assets_data (Pandas Dataframe): Base com dados dos ativos\n            funds_data (Pandas Dataframe): Base com dados dos fundos\n\n        Returns:\n            Pandas Dataframe: Base com os dados unificados\n        \"\"\"\n        data = pd.merge(funds_data, assets_data,\n                         how=\"inner\", on=[\"Asset\",\"Ticker\"])\n        return data\n\n\n    def populate_prices_BBG(self, data):\n        \"\"\"Popula a coluna 'Prices' com o preco atual de cada ativo.\n            Se nao for aplicavel, preenche com valor 1.\n\n        Args:\n            data: Dataframe de pandas com dados da carteira e ativos\n                  concatenados\n        Returns:\n            Dataframe de pandas com coluna de pre\u00e7os preenchida\n        \"\"\"\n\n        def __get_price(row):\n            # Fun\u00e7\u00e3o auxiliar para preencher condicionalmente\n            swaps_zerados = True\n            # se nao tem ticker bbg, preco valera 1\n            if row[\"Ticker_BBG\"] == \"nan\": \n                # TODO calculos das metricas internamente\n                if row[\"Grupo\"] == \"metricas\":\n                    return 0\n\n                if row[\"Grupo\"] == \"swap\":\n                    if row[\"Fundo\"].lower() == \"maratona\":\n                        return 1\n                    # Se for swap, retorna o p&amp;l\n                    if row[\"Tipo\"] == \"swap_p&amp;l\":\n                        if \"liq\" in row[\"Ticker\"]:\n                            return 1\n                        if swaps_zerados : return 0\n                        #else:\n                            # \n                        return self.data_extractor.get_swap_pnl(row[\"Fundo\"].title())\n\n                    elif row[\"Tipo\"] == \"swap_exp\":\n                        if swaps_zerados : return 0 # ALTERAR PARA PEGAR METRICA DE EXPOSICAO\n\n                        return self.data_extractor.get_swap_exp(row[\"Fundo\"].title())\n\n                #ip_atlas= \"Outrigger Fund Ltd - IP Atlas USD Equity Class - Subclass B\".lower()\n                #if row[\"Ticker\"] == ip_atlas:\n                #    return 1000\n\n                if row[\"Grupo\"] == 'termo':\n                    # Vamos pegar o preco medio da execucao, que eh o preco do termo fixo\n                    try:\n                        df = self.query_mngr.get_table(\"last_positions_by_bank\")\n                        ticker = row[\"Ticker\"].lower()\n\n                        price = df.query(\"Ticker == @ticker\").tail(1)[\"Avg_price\"].squeeze()\n                        return -price\n                    except:\n                        logger.error(f\"Erro ao buscar preco do termo {ticker}\")\n                        pass\n\n                price = self.query_mngr.get_price(row[\"Ticker\"], px_date=self.today)\n                if price == 0 or price is None: #Nao encontrado\n                    price = 1\n\n                #if row[\"Grupo\"] == \"termo\":\n                #    price = -price\n\n                return price\n\n            else:\n\n                price = self.query_mngr.get_price(row[\"Ticker_BBG\"])\n                #if row[\"Ticker_BBG\"] == 'lzrfy us equity':\n                #    price = self.query_mngr.get_price(\"rent3 bz equity\")\n                #    price = price/self.query_mngr.get_price(\"usdbrl curncy\")\n\n                # Preco do contrato de dolar vai ser a variacao de um dia pro outro\n                if row[\"Tipo\"] in [\"fut_dol\", \"fut_dol_min\"]:\n                    px_today = self.query_mngr.get_price(row[\"Ticker_BBG\"])\n                    prev_date = self.query_mngr.get_bday_offset(self.today, -1, location=\"bz\")\n                    px_prev = self.query_mngr.get_price(row[\"Ticker_BBG\"], px_date=prev_date)\n                    size = 50 if row[\"Tipo\"] == \"fut_dol\" else 10\n\n                    price = (px_today - px_prev) * size\n\n                # Se for bdr, precisaremos ajustar pelo peso\n                if row[\"Tipo\"] == \"a\u00e7\u00f5es_bdr\":\n                    bdr_ticker = row[\"Ticker\"]\n                    peso = self.query_mngr.get_bdr_size(bdr_ticker)\n\n\n                    #if bdr_ticker == \"amzo34 bz equity\":\n                    #    print(\"-------&gt; PESO DE AMZO34 ALTERADO NA MAO!!!!!! &lt;-----------\")\n                    #    peso = peso*20\n                    price = price * self.USD_BRL / peso\n\n                # AJUSTES MANUAIS DE PRECO, FAZER AQUI!\n                #if row[\"Ticker\"] == \"poraque fic fip\" and price &lt; 50000:\n                #    print(\"&gt;&gt;&gt;&gt; PRE\u00c7O DE OMEGA AJUSTADO MANUALMENTE &lt;&lt;&lt;&lt;\")\n                #    \n                #    price = 216526.697109\n\n                return  price\n\n        data[\"Price\"] = data.apply(lambda row: __get_price(row), axis=1)\n\n        return data\n\n\n    def get_caixas(self, merged_data):\n        cash_calc = cash_calculator.CashCalculator(self.input_bases_dir/self.input_base_file,today=self.today)\n        #pegando caixas dos fundos onshore\n        #print()\n        caixas = {}\n        for fund_name in self.fund_names[\"onshore\"]:\n            df = merged_data.loc[ merged_data[\"Fundo\"] == fund_name ]\n            # setar qtd de termos\n            total_termo =  self.query_mngr.get_term_debt(fund_name)\n\n            self.data_extractor.set_term_adj(fund_name, total_termo)\n\n            # setar qtd de futuro e ticker (CONSIDERA-SE QUE TERA APENAS 1 TIPO)\n            # TODO: aceitar mais de 1 tipo de futuro\n            ticker, qtd_fut = \"\", 0\n            for idx, row in df.loc[ ((df[\"Tipo\"] == \"fut_dol\") | (df[\"Tipo\"] == \"fut_dol_min\")) ].iterrows():\n                if row[\"Quantidade\"] != 0 :\n                    qtd_fut = row[\"Quantidade\"]\n                    ticker = row[\"Ticker\"]\n                    break\n            self.data_extractor.set_fut_data(fund_name, [ticker, qtd_fut])\n\n            trades = df.loc[df[\"Grupo\"] == \"trade rv\"]\n            #caixa = self.data_extractor.get_cx_data(fund_name, trades)\n            caixa = cash_calc.get_net_cash(fund_name, trades, data_extractor=self.data_extractor, query_mngr=self.query_mngr, source=\"onshore\")\n            caixas[fund_name.replace(\"_\", \" \")] = caixa[\"Caixa\"]\n            #if fund_name == \"global\":\n            #    caixas[fund_name] -= 4600100 * self.data_extractor.get_usdbrl()\n            #if fund_name == \"maratona\":\n            #    caixas[fund_name] = 77760.43\n\n            logger.info(fund_name, caixa)\n            logger.info(\"-- -- -- -- -- -- -- --\")\n\n        logger.info(\"## ## ## ## ## ## ## ## ##\")\n        # pegando caixas dos fundos offshore\n        for fund_name in self.fund_names[\"offshore\"]:\n            df = merged_data.loc[ merged_data[\"Fundo\"] == fund_name ]\n\n            trades = df.loc[df[\"Grupo\"] == \"trade rv\"]\n            #caixa = self.data_extractor.get_cx_data(fund_name, trades)\n            caixa = cash_calc.get_net_cash(fund_name, trades, data_extractor=self.data_extractor, query_mngr=self.query_mngr, source=\"offshore\")\n            caixas[fund_name] = caixa[\"Caixa\"]\n            logger.info(fund_name, caixa)\n            logger.info(\"-- -- -- -- -- -- -- --\")\n\n\n        # salvando caixas na base\n        def __set_cx(row, cxs):\n            fund_name = row[\"Fundo\"]\n            # Retornando valor do caixa onshore\n            if row[\"Asset\"] == \"caixa\":\n                return cxs[fund_name]\n            # Para testar o caixa offshore, olhamos o lipi como fund a\n            fund_name = fund_name.replace(\"lipizzaner\", \"fund a\")\n            # Retornando valor do caixa offshore para cada tipo de conta\n            if row[\"Ticker\"] in ['us asset account', 'us margin account', 'money market']:#row[\"Asset\"] == \"caixa us\":\n                account_type = row[\"Ticker\"]\n                return cxs[fund_name][account_type]\n\n            # Nao eh ativo de caixa, retorna o valor que ja estava\n            return row[\"Price\"]\n\n        merged_data[\"Price\"] = merged_data.apply(lambda row: __set_cx(row, caixas), axis=1)\n\n        return merged_data\n\n\n    def populate_location(self, data):\n\n        def __is_on_off(row):\n            # bdrs classificar como Onshore\n            if row[\"Tipo\"] == \"a\u00e7\u00f5es_bdr\":\n                return \"bz\"\n\n            exceptions = {\n                \"meli\"  : \"us\",\n                \"stone\" : \"us\",\n                \"xp inc\": \"us\",\n                \"vc bz\" : \"us\",\n                \"vc agro\" : \"us\",\n            }\n\n            # ETF de S&amp;P precisou sair de exceptions, pois VOO segue a regra padrao\n            if (row[\"Ticker_BBG\"] == \"ivvb11 bz equity\") or (\"ivvb11\" in row[\"Ticker\"]):\n                return \"bz\"\n\n            if row[\"Asset\"] in exceptions:\n                return exceptions[row[\"Asset\"]]\n            if row[\"Tipo\"] in exceptions:\n                return exceptions[row[\"Tipo\"]]\n            # nao caiu numa das excecoes, entao vai ser us\n            # apenas se nao for exposto ao real\n            return row[\"Currency Exposure\"]\n\n        data[\"Location\"] = data.apply(lambda row: __is_on_off(row), axis=1)\n\n        return data\n\n    def calculate_MktVal(self, data):\n        \"\"\"\n        Calcula o valor de mercado de cada ativo em cada fundo base\n        \"\"\"\n        def __get_MktVal(row):\n            #if row[\"Grupo\"] == \"fut_dol\":\n            #    return 0\n\n            if row[\"Tipo\"] == 'ndf usdbrl':\n                # Vai ser calculado pela variacao em relacao ao preco medio *  quantidade\n                avg_price = self.query_mngr.get_avg_price(row[\"Fundo\"], row[\"Asset\"]+\"_\"+row[\"Ticker\"], self.today)\n                if avg_price &lt; 0.1 and row[\"Fundo\"] == 'lipizzaner':\n                    # ativo esta pelo fund a\n                    avg_price = self.query_mngr.get_avg_price('fund a', row[\"Asset\"]+\"_\"+row[\"Ticker\"], self.today)\n\n                ndf_price = self.query_mngr.get_price(row[\"Ticker\"])\n\n                if row['Fundo'] in self.fund_names[\"onshore\"]:\n                    return row[\"Quantidade\"] * (ndf_price-avg_price) # Resultado em BRL\n                return row[\"Quantidade\"] * ((ndf_price/avg_price)-1) # Resultado em USD\n\n            if row[\"Fundo\"] in self.fund_names[\"onshore\"]:\n                # Nos onshore precisamos corrigir alguns precos pelo dolar\n                if row[\"Location\"] == \"us\" and row[\"Ticker\"] not in [\"spx seg hawker jun23\", \"SPX SEG HAWKER SEP23\".lower()]:\n                    return row[\"Quantidade\"] * row[\"Price\"] * self.USD_BRL\n\n            return row[\"Quantidade\"] * row[\"Price\"]\n\n        data[\"Mkt_Value\"] = data.apply(lambda row: __get_MktVal(row), axis=1)\n        return data\n\n\n    def segregate_funds(self, data):\n        #funds = {\n        #    f_name : pd.DataFrame({}, columns=data.columns) for f_name in self.fund_names[\"all\"]\n        #}\n\n        check_partial = [\"a\u00e7\u00f5es\", \"termo\", \"swap\", #\"caixa\", \"caixa_us\",\"margin\",\n                        \"spacs\", \"futuro\", \"fut_dol\", \"metricas\", \"fundo_vc\", \"bmf\"]\n        check_partial_eqt = check_partial + [\"vc\", \"fundos_fia\"]\n        check_partial_eqt_acoes = check_partial + [\"vc\"]\n\n        assets = self.query_mngr.get_table(\"assets\")[[\"Key\", \"Luxor_Classification\", \"Manager\"]]\n\n        data[\"Key\"] = data[\"Asset\"] + \"_\" + data[\"Ticker\"]\n        data = pd.merge(data, assets, on=\"Key\", how=\"left\")\n        data = data.drop([\"Key\"], axis=1)\n        funds = {\n            f_name : pd.DataFrame({}, columns=data.columns) for f_name in self.fund_names[\"all\"]\n        }\n\n        for index, row in data.iterrows():\n\n            #if row[\"Fundo\"] == \"lipizzaner\":\n            #    # copiar para o que representa o total\n            #    funds[\"lipi\"].loc[len(funds[\"lipi\"])] = row\n            #    funds[\"edel_lipi\"].loc[len(funds[\"edel_lipi\"])] = row\n            #    # verificar se precisa copiar para o parcial(acoes)\n            #    if row[\"Grupo\"] in check_partial:\n            #        funds[\"lipi_acoes\"].loc[len(funds[\"lipi_acoes\"])] = row\n            #        funds[\"edel_lipi_acoes\"].loc[len(funds[\"edel_lipi_acoes\"])] = row\n\n\n            if row[\"Fundo\"] == \"fund a\":\n                # copiar para o que representa o total\n                funds[\"fund_a\"].loc[len(funds[\"fund_a\"])] = row\n\n                # verificar se precisa copiar para o parcial(ex-vc)\n                if (row[\"Grupo\"] in check_partial) and (row[\"Luxor_Classification\"] in [\"stocks\"]):\n                    funds[\"fund_a_ex_vc\"].loc[len(funds[\"fund_a_ex_vc\"])] = row\n\n            elif row[\"Fundo\"] == \"fund b\":\n                # copiar para o que representa o total\n                funds[\"fund_b\"].loc[len(funds[\"fund_b\"])] = row\n                # verificar se precisa copiar para o parcial(ex-vc)\n                if (row[\"Grupo\"] in check_partial) and (row[\"Luxor_Classification\"] in [\"stocks\"]):\n                    funds[\"fund_b_ex_vc\"].loc[len(funds[\"fund_b_ex_vc\"])] = row\n\n                # Adicionando no fund_b + lipi,\n                # mas precisa multiplicar pelo dolar\n                row[\"Mkt_Value\"] = row[\"Mkt_Value\"] * self.USD_BRL\n                funds[\"edel_lipi\"].loc[len(funds[\"edel_lipi\"])] = row\n                if (row[\"Grupo\"] in check_partial) and (row[\"Luxor_Classification\"] in [\"stocks\"]):\n                    funds[\"edel_lipi_acoes\"].loc[len(funds[\"edel_lipi_acoes\"])] = row\n\n\n            elif row[\"Fundo\"] == \"hmx\":\n                # copiar para o que representa o total\n                funds[\"hmx\"].loc[len(funds[\"hmx\"])] = row\n                # verificar se precisa copiar para o parcial(ex-vc)\n                if (row[\"Grupo\"] in check_partial) and (row[\"Luxor_Classification\"] in [\"stocks\"]):\n                    funds[\"hmx_ex_vc\"].loc[len(funds[\"hmx_ex_vc\"])] = row\n\n                # Adicionando no maratona + HMX, mas precisa multiplicar pelo dolar\n                row[\"Mkt_Value\"] = row[\"Mkt_Value\"] * self.USD_BRL\n                funds[\"mrtn_hmx\"].loc[len(funds[\"mrtn_hmx\"])] = row\n\n            elif row[\"Fundo\"] == \"maratona\":\n                # copiar para o que representa o total\n                funds[\"maratona\"].loc[len(funds[\"maratona\"])] = row\n                funds[\"mrtn_hmx\"].loc[len(funds[\"mrtn_hmx\"])] = row\n                # verificar se precisa copiar para o parcial(ex-vc)\n                if (row[\"Grupo\"] in check_partial) and (row[\"Luxor_Classification\"] in [\"stocks\"]):\n                    funds[\"maratona_acoes\"].loc[len(funds[\"maratona_acoes\"])] = row\n\n\n            elif row[\"Fundo\"] == \"lipizzaner\":\n                # copiar para o que representa o total\n                funds[\"lipizzaner\"].loc[len(funds[\"lipizzaner\"])] = row\n                row_edel_lipi = row.copy()\n                row_edel_lipi[\"Mkt_Value\"] = row_edel_lipi[\"Mkt_Value\"] * self.query_mngr.get_eduardo_lipi_particip(self.today)\n                funds[\"edel_lipi\"].loc[len(funds[\"edel_lipi\"])] = row_edel_lipi\n                if (row[\"Grupo\"] in check_partial) : #and (row[\"Luxor_Classification\"] in [\"stocks\"]):\n                    funds[\"lipizzaner_acoes\"].loc[len(funds[\"lipizzaner_acoes\"])] = row\n                    funds[\"edel_lipi_acoes\"].loc[len(funds[\"edel_lipi_acoes\"])] = row_edel_lipi\n\n                if row[\"Grupo\"] in check_partial_eqt: \n                    # Inserimos no all equities total\n                    funds[\"all_eqt\"].loc[len(funds[\"all_eqt\"])] = row\n                # verificamos se eh fia antes de inserir no acoes\n                if (row[\"Grupo\"] in check_partial_eqt_acoes) and (row[\"Luxor_Classification\"] in [\"stocks\"]):\n                    funds[\"all_eqt_acoes\"].loc[len(funds[\"all_eqt_acoes\"])] = row\n\n            elif row[\"Fundo\"] == \"mangalarga fic fia\":\n                # copiar para o que representa o total\n                funds[\"mangalarga_fic_fia\"].loc[len(funds[\"mangalarga_fic_fia\"])] = row\n                #if row[\"Grupo\"] in check_partial:\n                #    funds[\"mangalarga_master_acoes\"].loc[len(funds[\"mangalarga_master_acoes\"])] = row\n                #if row[\"Grupo\"] in check_partial_eqt: \n                    # Inserimos no all equities total\n                #    funds[\"all_eqt\"].loc[len(funds[\"all_eqt\"])] = row\n                # verificamos se eh fia antes de inserir no acoes\n                #if row[\"Grupo\"] in check_partial_eqt_acoes:\n                #    funds[\"all_eqt_acoes\"].loc[len(funds[\"all_eqt_acoes\"])] = row\n\n        return funds\n\n\n    def get_pls(self, funds):\n        \"\"\"\n        Recebe como input um dicionario de dataframes de fundos\n        Soma a coluna Mkt_Value de cada um e retorna um dicion\u00e1rio\n        de pls, usando mesma chave\n        \"\"\"\n        pls = {}\n\n        for key, fund_df in funds.items():\n            pl = fund_df[\"Mkt_Value\"].sum()\n            pls[key] = pl\n\n        return pls\n\n\n    def calculate_percentages(self, funds, pls):\n        \"\"\"Calcula o percentual de cada ativo no fundo.\"\"\"\n\n        for fund_key, fund_df in funds.items():\n            # Repete para todos os fundos\n            percents = []\n            percents_abs = []\n\n            try:\n                for idx, row in fund_df.iterrows():\n                    pct = row[\"Mkt_Value\"]/pls[fund_key]\n                    percents.append(pct)\n                    percents_abs.append(abs(pct))\n\n            except ZeroDivisionError:\n                logger.error(\"Em 'calculate_percentage', divisao por pl 0\")\n            fund_df[\"%_pl\"] = percents\n            fund_df[\"%_pl_abs\"] = percents_abs\n\n        return funds\n\n\n    def get_non_groupable_vcs(self, funds, metric=0.01):\n        \"\"\"\n        Gera dicionario de vcs nao agrupaveis.\n        As chaves sao os VCs e os valores uma lista com os fundos\n        onde nao sera agrupado\n        Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'\n        \"\"\"\n        non_groupable = {\n            f_name:[] for f_name in self.fund_names[\"vc_funds_all\"]\n        }\n\n        for fund_name, fund_df in funds.items():\n            if fund_name not in self.fund_names[\"vc_funds\"]: continue\n\n            for idx, row in fund_df.iterrows():\n                if row[\"Grupo\"] == \"vc\" and row[\"Ticker\"] not in non_groupable[fund_name]:\n\n                    if row[\"%_pl_abs\"] &gt;= metric:\n                        non_groupable[fund_name].append(row[\"Ticker\"])\n\n        for f_name in self.fund_names[\"luxor_ptf\"]:\n            if f_name != \"fund_a\":\n                non_groupable[f_name] = non_groupable[\"fund_a\"][:]\n\n        # fund_b + lipi recebe o mapa do fund_b (excecao)\n        non_groupable[\"edel_lipi\"] = non_groupable[\"fund_b\"][:]\n        non_groupable[\"mrtn_hmx\"] = non_groupable[\"hmx\"][:]\n\n\n        return non_groupable\n\n\n    def get_non_groupable_iliquids(self, funds, metric=0.01):\n        \"\"\"\n        Gera dicionario de iliquids nao agrupaveis.\n        Os valores sao os VCs/PEs numa lista e as chaves os nomes dos fundos\n        onde nao sera agrupado\n        Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'\n        \"\"\"\n        non_groupable = {\n            f_name:[] for f_name in self.fund_names[\"iliquids_funds\"]\n        }\n\n        for fund_name, fund_df in funds.items():\n            if fund_name not in self.fund_names[\"iliquids_funds\"]: continue\n\n            for idx, row in fund_df.iterrows():\n                if (row[\"Grupo\"] == \"vc\" or row[\"Grupo\"] == \"private equity\") \\\n                        and row[\"Ticker\"] not in non_groupable[fund_name]:\n                    if row[\"%_pl_abs\"] &gt;= metric:\n                        non_groupable[fund_name].append(row[\"Ticker\"])\n\n\n        # fund_b + lipi recebe o mapa do fund_b (excecao)\n        non_groupable[\"edel_lipi\"] = non_groupable[\"fund_b\"][:]\n        non_groupable[\"mrtn_hmx\"] = non_groupable[\"hmx\"][:]\n\n        return non_groupable\n\n\n    def get_non_groupable_iliquids_managers(self, funds, metric=0.01):\n        \"\"\"\n        Gera dicionario de iliquids nao agrupaveis.\n        Os valores sao os tickers VCs/PEs numa lista e as chaves os nomes dos fundos\n        onde nao sera agrupado\n        Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'\n        \"\"\"\n        non_groupable = {\n            f_name:[] for f_name in self.fund_names[\"iliquids_funds\"]\n        }\n\n        for fund_name, fund_df in funds.items():\n            if fund_name not in self.fund_names[\"iliquids_funds\"]: continue\n\n            # na coluna Manager, vamos substituir os valores nan pelo valor da coluna Ticker\n            fund_df[\"Manager\"] = np.where((fund_df[\"Manager\"].isnull()) | (fund_df[\"Manager\"] == 'nan'),\n                                          fund_df[\"Ticker\"], fund_df[\"Manager\"])\n            grouped_by_manager = fund_df[[\"Manager\", \"%_pl_abs\"]].groupby(\"Manager\").sum().reset_index()\n            managers_not_to_group = grouped_by_manager.loc[grouped_by_manager[\"%_pl_abs\"] &gt;= metric, \"Manager\"]\n\n            tickers_not_to_group = fund_df.query(\"Manager in @managers_not_to_group and \\\n                                                 (Grupo == 'vc' or Grupo == 'private equity')\")[\"Ticker\"]\n            for ticker in tickers_not_to_group:\n                non_groupable[fund_name].append(ticker)\n\n        # fund_b + lipi recebe o mapa do fund_b (excecao)\n        #non_groupable[\"edel_lipi\"] = non_groupable[\"fund_b\"][:]\n        #non_groupable[\"mrtn_hmx\"] = non_groupable[\"hmx\"][:]\n\n        return non_groupable\n\n\n    def group_vcs(self, funds, non_groupable):\n        \"\"\"Altera a coluna 'Nomes' deixando diferente apenas o nome dos VCs\n            que nao serao agrupados e com mesmo nome os que serao.\"\"\"\n\n        for fund_name, fund_df in funds.items():\n            if fund_name not in self.fund_names[\"vc_funds_all\"]: continue\n            non_groupable_vcs = non_groupable[fund_name]\n\n            for idx, fund_row in fund_df.iterrows():\n                if fund_row[\"Grupo\"] == \"vc\":\n\n                    if fund_row[\"Ticker\"] in non_groupable_vcs:\n                        funds[fund_name].loc[idx, \"Nome\"] = self.query_mngr.get_asset_name(fund_row[\"Ticker\"].lower())\n                        funds[fund_name].loc[idx, \"Asset\"] = fund_row[\"Ticker\"]\n        return funds\n\n\n    def group_iliquids(self, funds, non_groupable):\n        \"\"\"Altera a coluna 'Nomes' deixando diferente apenas o nome dos Iliquids\n            que nao serao agrupados e com mesmo nome os que serao.\"\"\"\n\n        for fund_name, fund_df in funds.items():\n            if fund_name not in self.fund_names[\"iliquids_funds\"]: continue\n            non_groupable_vcs = non_groupable[fund_name]\n\n            for idx, fund_row in fund_df.iterrows():\n                if fund_row[\"Grupo\"] == \"vc\" or fund_row[\"Grupo\"] == \"private equity\" or fund_row[\"Asset\"] == 'tarpon':\n                    if fund_row[\"Ticker\"] in non_groupable_vcs:\n                        # Tanto o nome quanto asset passarao a ser o Manager\n                        # Quando o Manager for diferente da chave, vamos priorizar mostrar ele no nome\n                        if fund_row[\"Ticker\"] != fund_row[\"Manager\"]:\n                            funds[fund_name].loc[idx, \"Nome\"] = fund_row[\"Manager\"].title()\n                        else:\n                            # Vamos pegar o nome cadastrado na tabela ativos\n                            funds[fund_name].loc[idx, \"Nome\"] = self.query_mngr.get_asset_name(fund_row[\"Ticker\"].lower())\n                        #funds[fund_name].loc[idx, \"Asset\"] = fund_row[\"Ticker\"]\n                        funds[fund_name].loc[idx, \"Asset\"] = fund_row[\"Manager\"]\n\n                    else:\n                        # Vamos trocar o Asset e o Nome para iliquids bz ou iliquids us \n                        location = fund_row[\"Currency Exposure\"]\n                        funds[fund_name].loc[idx, \"Asset\"] = \"iliquids \" + location\n                        funds[fund_name].loc[idx, \"Nome\"] = \"Iliquids \" + location.upper()\n\n        return funds\n\n\n    def sum_rows_if(self, fund_df, grupo = [], tipo = [], ticker = [], asset = [],\n                        row_to_sum = \"%_pl\", modifier=\"\", name_1=\"Grupo\", name_2=\"Tipo\", \n                        name_3=\"Ticker\", name_4=\"Asset\"):\n        \"\"\" Soma os % dos ativos em fund_df.\n        No comportamento padr\u00e3o, ir\u00e1 excluir da soma aqueles que estiverem informados em\n        `grupo`, `tipo`, `ticker` ou `asset`.\n        Esse comportamento pode ser alterado pelo modificador\n        modificador: \n            `reverse` - Passa a somar somente os ativos definidos em grupo, tipo e ticker\n            `grupo &amp; ! tipo` - Somar apenas se estiver em grupo e nao estiver em tipo\n            `if_negative` - soma somente valores negativos presentes na lista `grupo`\n        \"\"\"\n        summation = 0\n        if modifier == \"\":\n            # por padrao, soma tudo menos o que estiver nas listas\n            # precisa satisfazer todas as listas no mesmo row\n            for idx, row in fund_df.iterrows():\n                if (row[name_1] not in grupo) and (row[name_2] not in tipo) and (row[name_3] not in ticker) and (row[name_4] not in asset):\n                    summation += row[row_to_sum]\n\n        elif modifier == \"reverse\":\n            # nessa variante, vai somar apenas se satisfizer as condicoes das listas\n            for idx, row in fund_df.iterrows():\n                if (len(grupo)== 0 or row[name_1] in grupo) and (len(tipo) == 0  or row[name_2] in tipo) and (len(ticker) == 0 or row[name_3] in ticker) and (len(asset) == 0 or row[name_4] in asset):\n                    summation += row[row_to_sum]\n\n        elif modifier == \"grupo &amp; !tipo\":\n            # nessa variante, vai somar apenas se estiver em grupo e nao estiver em tipo\n            for idx, row in fund_df.iterrows():\n                if (len(grupo) &gt; 0 and row[name_1] in grupo) and (len(tipo) &gt; 0  and row[name_2] not in tipo):\n                    summation += row[row_to_sum]\n        elif modifier == \"if_negative\":\n            # Soma todos os valores negativos\n            for idx, row in fund_df.iterrows():\n                if row[\"%_pl\"] &lt; 0:\n                    if name_1 == \"Grupo\":\n                        if row[name_1] in grupo:\n                            summation += row[row_to_sum]\n                    elif name_1 == \"Ticker\":\n                        if row[name_1] in ticker:\n                            summation += row[row_to_sum]\n        else:\n            logger.error(\"Erro: Modificador n\u00e3o identificado.\")\n\n\n        return summation\n\n\n    def value_at(self, fund_df, comp_col, comp_value, value_col, suppress_warnings=True, divide_by=1):\n\n        try:\n            comp_value = comp_value.lower()\n            val = fund_df.loc[ fund_df[comp_col] == comp_value, value_col].iloc[0]          \n            return val/divide_by\n\n        except IndexError:\n            if not suppress_warnings:\n                fundo = fund_df.at[0,\"Fundo\"]\n                logger.error(f\"N\u00e3o encontrado '{comp_value}' no {fundo}\")\n            return 0.0\n\n\n    def transform_and_sum(self, fund_df, grupo, tipo = \"\", col1=\"Quantidade\", col2=\"Price\", operation=\"*\", size=1):\n        proc_df = fund_df.loc[ fund_df[\"Grupo\"] == grupo ]\n        result = 0.0\n        for idx, row in proc_df.iterrows():\n            row = row.fillna(0)\n            if operation == \"*\":\n                result += row[col1] * row[col2] * size\n\n        return result\n\n\n    def get_ndf_exposure(self, fund_df):\n\n        ndfs = fund_df.query(\"Tipo == 'ndf usdbrl'\")[[\"Quantidade\", \"Price\"]]\n        if len(ndfs) == 0: return 0\n        ndfs[\"Exp\"] = ndfs[\"Quantidade\"] * ndfs[\"Price\"]\n        return -(ndfs[\"Exp\"].sum())\n\n\n    def compute_risk_metrics(self, funds):\n        # Inicializar base com colunas:\n        #    Fundo, Metrica, Valor\n        # As metricas normalmente consistem em somar rows especificos\n        # e dividir pelo pl.\n        # Possivel talvez usar uma funcao generica que permite definir\n        # alguns rows especificos e listar ao que ele deve se igualar\n\n        metrics_names = []\n        get_names = True\n        risk_df = pd.DataFrame({})\n\n        # Para drawdown global, vamos salvar alguns dados\n        short_tot_glob = 0\n        margin_tot_glob = 0\n\n        assets = self.query_mngr.get_table(\"assets\")\n\n        for fund_name, fund_df in funds.items():\n\n            if fund_name == \"mangalarga_fic_fia\":\n                # Para o manga fic fia vamos copiar as metricas do manga master                 \n                continue\n\n            fund_metrics = []\n\n            #Portfolio Long/PL\n            grupo = [\"short\"]\n            metric_val = self.sum_rows_if(fund_df, grupo)\n            if get_names: metrics_names.append(\"Portfolio Long/PL\")\n            fund_metrics.append(metric_val)\n\n            #Gross Exposure\n            grupo = [\"swap\", \"loan\", \"caixa\", \"caixa_us\", \"termo\", \"short\", \"taxas e custos\", \"ntnb\"]\n            metric_val = self.sum_rows_if(fund_df, grupo)\n            if get_names: metrics_names.append(\"Gross Exposure\")\n            fund_metrics.append(metric_val)\n\n            #Net Long\n            # grupos que nao deve somar:\n            grupo = [\"caixa\", \"caixa_us\", \"swap\", \"termo\", \"loan\", \"taxas e custos\", \"ntnb\"] # quando tiver ouro, tb nao pode impactar o net long.\n            metric_val = self.sum_rows_if(fund_df, grupo)\n            if get_names: metrics_names.append(\"Net Long\")\n            fund_metrics.append(metric_val)\n\n            #Gross Eqt. Exp.\n            grupo = [\"caixa\", \"caixa_us\", \"swap\", \"termo\", \"loan\", \"rf_privada\", \"fundos\", \"short\", \"taxas e custos\", \"ntnb\"]\n            metric_val = self.sum_rows_if(fund_df, grupo)\n            if get_names: metrics_names.append(\"Gross Equity Exp.\")\n            fund_metrics.append(metric_val)\n\n            #Net Equity Exp.\n            grupo = [\"caixa\", \"caixa_us\", \"swap\", \"termo\", \"loan\", \"rf_privada\", \"fundos\", \"taxas e custos\", \"ntnb\"]\n            metric_val = self.sum_rows_if(fund_df, grupo)\n            if get_names: metrics_names.append(\"Net Equity Exp.\")\n            fund_metrics.append(metric_val)\n\n            #Net Public Eqt. Exp.\n            grupo = [\"caixa\", \"caixa_us\", \"swap\", \"termo\", \"loan\", \"rf_privada\", \"fundos\", \"vc\", \"taxas e custos\", \"ntnb\"]\n            metric_val = self.sum_rows_if(fund_df, grupo)\n            if get_names: metrics_names.append(\"Net Public Eqt. Exp.\")\n            fund_metrics.append(metric_val)\n\n            #% public Eqt.\n            metric_val = self.sum_rows_if(fund_df, [\"fundos_fia\", \"fundo_vc\"], modifier=\"reverse\") # fundo_vc era referente ao 3G. Venture capital nao entra\n            metric_val += self.sum_rows_if(fund_df, [\"a\u00e7\u00f5es\"], [\"spac\", \"spac_w\"], modifier=\"grupo &amp; !tipo\")\n            if get_names: metrics_names.append(\"% Public Eqt.\")\n            fund_metrics.append(metric_val)\n\n            #% acoes.\n            metric_val = self.sum_rows_if(fund_df, [\"a\u00e7\u00f5es\"], modifier=\"reverse\") \n            if get_names: metrics_names.append(\"% A\u00e7\u00f5es\")\n            fund_metrics.append(metric_val)\n\n            #% fias.\n            metric_val = self.sum_rows_if(fund_df, [\"fundos_fia\"], modifier=\"reverse\") # fundo_vc era referente ao 3G. Venture capital nao entra\n            if get_names: metrics_names.append(\"% FIAs\")\n            fund_metrics.append(metric_val)\n\n            #% termos\n            grupo = [\"termo\"]\n            metric_val = self.sum_rows_if(fund_df, grupo, modifier=\"reverse\", row_to_sum=\"%_pl_abs\")\n            if get_names: metrics_names.append(\"% Termos\")\n            fund_metrics.append(metric_val)\n\n            #% VC\n            grupo = [\"vc\"]\n            metric_val = self.sum_rows_if(fund_df, grupo, modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% VCs\")\n            fund_metrics.append(metric_val)\n\n            #% Private Equity\n            grupo = [\"private equity\"]\n            metric_val = self.sum_rows_if(fund_df, grupo, modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% PE\")\n            fund_metrics.append(metric_val)\n\n\n             #% Hedge Funds\n            grupo = [\"fundos\"]\n            metric_val = self.sum_rows_if(fund_df, grupo, modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% Hedge Funds\")\n            fund_metrics.append(metric_val)\n\n            #% P&amp;L Swap + termos\n            grupo = [\"swap\", \"termo\", \"caixa\", \"caixa_us\", \"loan\", \"swap_eurusd\", \"fut_dol\"]\n            metric_val = self.sum_rows_if(fund_df, grupo=grupo, modifier=\"if_negative\")\n            if get_names: metrics_names.append(\"% Passivos\")\n            fund_metrics.append(metric_val)\n\n            #% P&amp;L Swap\n            grupo = [\"swap_eurusd\"]\n            metric_val = self.sum_rows_if(fund_df, grupo, modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% P&amp;L Swap EURUSD\")\n            fund_metrics.append(metric_val)\n\n            #% Renda fixa privada (fidcs)\n            grupo = [\"rf_privada\"]\n            metric_val = self.sum_rows_if(fund_df, grupo, modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% Fixed Income\")\n            fund_metrics.append(metric_val)\n\n            #% SPACs\n            tipo = [\"spac\", \"spac_w\"]\n            metric_val = self.sum_rows_if(fund_df, tipo=tipo, modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% SPACs\")\n            fund_metrics.append(metric_val)\n\n            #% BDRs\n            tipo = [\"a\u00e7\u00f5es_bdr\"]\n            metric_val = self.sum_rows_if(fund_df, tipo=tipo, modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% BDRs\")\n            fund_metrics.append(metric_val)\n\n            #Offshore\n            metric_val_offshore = self.sum_rows_if(fund_df, [\"us\"], name_1=\"Currency Exposure\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"Offshore\")\n            fund_metrics.append(metric_val_offshore)\n\n            #Onshore\n            metric_val_onshore = self.sum_rows_if(fund_df, [\"us\"], name_1=\"Currency Exposure\")\n            if get_names: metrics_names.append(\"Onshore\")\n            fund_metrics.append(metric_val_onshore)\n\n            #Hedge Dol\n\n            fut_dol = self.transform_and_sum(fund_df, grupo=\"fut_dol\", size=-50)\n            swap_exp = self.value_at(fund_df, \"Tipo\", \"swap_exp\", \"Quantidade\")\n            ndf_exp = self.get_ndf_exposure(fund_df)\n            if fund_name =='edel_lipi':\n                ndf_exp = ndf_exp * self.query_mngr.get_eduardo_lipi_particip(self.today)\n\n            #if fund_name in [\"fund_b\", \"hmx\", \"fund_a\", \"edel_lipi\"]:\n                # variar exposicao offshore \n            #    exps = {\"fund_b\" : 1600000, \"hmx\" : 1780000, \"fund_a\" : 0, \"edel_lipi\" : 1600000*self.USD_BRL}\n            #    swap_exp += exps[fund_name]\n            metric_val_hedge = (fut_dol + swap_exp + ndf_exp)/self.pls[fund_name]\n\n\n            if get_names: metrics_names.append(\"Hedge Dol\")\n            fund_metrics.append(metric_val_hedge)\n\n            #Exp USD\n            metric_val = metric_val_offshore - metric_val_hedge\n            if get_names: metrics_names.append(\"Exp. USD\")  \n            fund_metrics.append(metric_val)\n\n            #Sup. Aju. Dol\n            metric_val = self.value_at(fund_df, \"Asset\", \"Sup. Aju. DOL\", \"Quantidade\")\n            if get_names: metrics_names.append(\"Sup. Aju. DOL\") #TODO precisa do caixa segregado\n            fund_metrics.append(metric_val)\n\n            #Enquadramento: offshore + acoes e fias onshore\n            metric_val = self.sum_rows_if(fund_df, tipo = [\"a\u00e7\u00f5es\", \"a\u00e7\u00f5es_bdr\", \"a\u00e7\u00f5es_us\", \"caixa_us\",\n                                                           \"fundo_fia\", \"omega\", \"spac\", \"vc bz\", \"vc us\", \n                                                           \"term debt\", \"unidas\", \"equity_fund\", \"etf\"],\n                                                            modifier=\"reverse\")\n            if fund_name == \"lipizzaner\":\n                metric_val = ((self.pls[\"fund_a\"] * self.USD_BRL)/self.pls[\"lipizzaner\"]) + self.sum_rows_if(fund_df, tipo = [\"a\u00e7\u00f5es\", \"a\u00e7\u00f5es_bdr\", \"fundo_fia\", \"omega\",\"term debt\"], modifier=\"reverse\")\n\n            if get_names: metrics_names.append(\"Enquadramento\")\n            fund_metrics.append(metric_val)\n\n            #S KHC: somar %_pl a\u00e7\u00f5es khc vendido\n            metric_val_s_khc = self.sum_rows_if(fund_df, [\"a\u00e7\u00f5es\"], [\"kraft heinz\"], name_2=\"Asset\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"S KHC\")\n            fund_metrics.append(metric_val_s_khc)\n\n            #Caixa Bloqueado\n            metric_cx_bloq = abs(metric_val_s_khc) + self.sum_rows_if(fund_df, [\"short\"], row_to_sum = \"%_pl_abs\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"Caixa Bloqueado\")\n            fund_metrics.append(metric_cx_bloq)\n\n            #Caixa Livre\n            metric_val = -metric_cx_bloq + self.sum_rows_if(fund_df, [\"caixa\", \"caixa_us\"], row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"Caixa Livre\")\n            fund_metrics.append(metric_val)\n\n            #% loan\n            metric_val = self.sum_rows_if(fund_df, [\"loan\"], row_to_sum = \"%_pl_abs\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% Loan\")\n            fund_metrics.append(metric_val)\n\n            #Alavancagem Caixa\n            metric_val = self.sum_rows_if(fund_df, [\"caixa\", \"caixa_us\", \"loan\"], row_to_sum = \"%_pl\", modifier=\"reverse\")\n            metric_val = 0 if metric_val &gt; 0 else abs(metric_val)\n            if get_names: metrics_names.append(\"Alavancagem Caixa\")\n            fund_metrics.append(metric_val)\n\n            #JPM Money Market\n            metric_val_mar_co = self.value_at(fund_df, \"Asset\", \"JP Money Market\", \"Quantidade\")\n            if get_names: metrics_names.append(\"JP Money Market\")\n            fund_metrics.append(metric_val_mar_co)\n\n            #Margin Cost\n            metric_val_mar_co = self.value_at(fund_df, \"Asset\", \"Margin Cost (SOFR +85bps)\", \"Quantidade\")\n            if get_names: metrics_names.append(\"Margin Cost (SOFR +85bps)\")\n            fund_metrics.append(metric_val_mar_co)\n\n            #Retorno Short\n            metric_val_ret_sh = self.value_at(fund_df, \"Asset\", \"Retorno Short\", \"Quantidade\")\n            if get_names: metrics_names.append(\"Retorno Short\")\n            fund_metrics.append(metric_val_ret_sh)\n\n            #Carry Loan: return_short - margin_cost\n            paid_in_khc_dividends = 0.0414 * 0.7 - 0.0414\n            metric_val = metric_val_ret_sh - metric_val_mar_co + paid_in_khc_dividends\n            if get_names: metrics_names.append(\"Custo Short KHC\")\n            fund_metrics.append(metric_val)\n\n            #Drawdown - limite margem\n            short_tot = self.sum_rows_if(fund_df, [\"caixa_us\", \"loan\"], modifier=\"reverse\", row_to_sum=\"Mkt_Value\")\n\n            # % Capital Commited (USD)\n            checklist = self.fund_names[\"simulated\"] + self.fund_names[\"onshore\"] #+ [\"lipi\"] #, \"manga\"]\n            multiplier = 1 if fund_name not in checklist else self.USD_BRL\n\n            comm_usd = self.value_at(fund_df, \"Ticker\", \"usd commited capital\", \"Quantidade\", divide_by=self.pls[fund_name]) * multiplier\n            comm_brl = self.value_at(fund_df, \"Ticker\", \"brl commited capital\", \"Quantidade\", divide_by=self.pls[fund_name])\n\n            metric_val = comm_usd + comm_brl\n            if get_names: metrics_names.append(\"% Uncalled Capital\")\n            fund_metrics.append(metric_val)\n\n\n            # TODO APAGAR DEPOIS DE EFETIVADO #LOAN #EMPRESTIMO #EDUARDO\n            if fund_name == \"fund_b\":\n                short_tot -= 1200000    # Emprestimo do Edu Garantido pela Edelweiss (conta como garantia)\n                #short_tot -= 2600000    # 2.6mm esprestimo Zaratustra garantido pela Edelweiss\n\n\n            short_khc = self.value_at(fund_df, \"Ticker\", \"khc us equity\", \"Mkt_Value\")\n            short_tot += short_khc\n\n            margin_tot = self.sum_rows_if(fund_df, [\"a\u00e7\u00f5es\"], modifier=\"reverse\", row_to_sum=\"Mkt_Value\") - short_khc\n\n            metric_val = (-(short_tot)/0.5)/margin_tot - 1\n            if get_names: metrics_names.append(\"Max Drawdown - ML\")\n            fund_metrics.append(metric_val)\n\n\n            # Drawdown global - limite margem\n            other_fund = \"fund_b\" if fund_name == \"fund_a\" else \"fund_a\"\n\n            short_tot_glob = self.sum_rows_if(funds[other_fund], [\"caixa_us\", \"loan\"], modifier=\"reverse\", row_to_sum=\"Mkt_Value\")\n            if other_fund == \"fund_b\":\n                short_tot_glob -= 1200000   # Emprestimo do Edu Garantido pela Edelweiss (conta como garantia)\n                #short_tot_glob -= 2600000    # 2.6mm esprestimo Zaratustra garantido pela Edelweiss\n\n            other_short_khc = self.value_at(funds[other_fund], \"Ticker\", \"khc us equity\", \"Mkt_Value\")\n            short_tot_glob += other_short_khc + short_tot\n            margin_tot_glob = self.sum_rows_if(funds[other_fund], [\"a\u00e7\u00f5es\"], modifier=\"reverse\", row_to_sum=\"Mkt_Value\") - other_short_khc\n            margin_tot_glob += margin_tot\n            metric_val = (-(short_tot_glob)/0.5)/margin_tot_glob - 1\n            if get_names: metrics_names.append(\"Max Drawdown Global - ML\")\n            fund_metrics.append(metric_val)\n\n            # % Luxor\n            stocks_tickers = list(assets.query(\"Luxor_Classification.isin(['stocks'])\")[\"Ticker\"])\n            metric_val = self.sum_rows_if(fund_df,\n                ticker=stocks_tickers, row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% Luxor\")\n            fund_metrics.append(metric_val)\n\n            # % ETF S&amp;P\n            metric_val = self.sum_rows_if(fund_df, asset=['etf s&amp;p'], row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% ETF S&amp;P\")\n            fund_metrics.append(metric_val)\n\n            # % IP Atlas\n            metric_val = self.sum_rows_if(fund_df, asset=['ip atlas'], row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% IP\")\n            fund_metrics.append(metric_val)\n\n            # % SPX Hawker\n            metric_val = self.sum_rows_if(fund_df, asset=['spx hawker', 'spx eagle'], row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% SPX Hawker\")\n            fund_metrics.append(metric_val)\n\n            # % TCI\n            metric_val = self.sum_rows_if(fund_df, asset=['tci'], row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% TCI\")\n            fund_metrics.append(metric_val)\n\n            # % Cash (classification = Fixed Income e % PL &gt; 0)\n            cash_assets = list(assets.query(\"Luxor_Classification.isin(['fixed income'])\")[\"Ticker\"]) + ['ibit us equity']\n            metric_val = self.sum_rows_if(fund_df, ticker=cash_assets, row_to_sum = \"%_pl\", modifier=\"reverse\")\n            metric_val_debt = self.sum_rows_if(fund_df, ticker=cash_assets, row_to_sum = \"%_pl\", modifier=\"if_negative\", name_1=\"Ticker\")\n            metric_val = metric_val - metric_val_debt # Devolvendo o percentual que foi subtraido.\n            if get_names: metrics_names.append(\"% Cash\")\n            fund_metrics.append(metric_val)\n\n            # % Iliquids\n            metric_val = self.sum_rows_if(fund_df,\n                grupo=['vc', 'private equity'], row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% Iliquids\")\n            fund_metrics.append(metric_val)\n\n            # % BR Equity Funds\n            metric_val = self.sum_rows_if(fund_df, grupo=['fundos_fia'], row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% BR Equity Funds\")\n            fund_metrics.append(metric_val)\n\n            # % Equities\n            equities = list(assets.query(\"Luxor_Classification.isin(['equity funds', 'stocks', 'vc', 'private equity'])\")[\"Ticker\"])\n            metric_val = self.sum_rows_if(fund_df,\n                ticker=equities, row_to_sum = \"%_pl\", modifier=\"reverse\")\n            if get_names: metrics_names.append(\"% Equity\")\n            fund_metrics.append(metric_val)\n\n            # % Debt\n            debt_metric = self.sum_rows_if(fund_df,\n                grupo=['caixa', 'caixa_us', 'loan', 'termo', 'taxas e custos'], row_to_sum = \"%_pl\", modifier=\"if_negative\")\n\n            # % Non Equities\n            equities = list(assets.query(\"Luxor_Classification.isin(['equity funds', 'stocks', 'vc', 'private equity'])\")[\"Ticker\"])\n            metric_val = self.sum_rows_if(fund_df,\n                ticker=equities, row_to_sum = \"%_pl\",)\n            metric_val = metric_val - debt_metric\n            if get_names: metrics_names.append(\"% Non Equity\")\n            fund_metrics.append(metric_val)\n            # Adicionando % Debt depois, para aparecer por ultimo na ordem\n            if get_names: metrics_names.append(\"% Debt\")\n            fund_metrics.append(debt_metric)\n\n            # % Leverage com relacao as acoes\n            if fund_name in self.fund_names['partial']:\n                reference_portfolio_name = self.partial_to_complete_name_map[fund_name]\n                reference_portfolio = funds[reference_portfolio_name]\n            else:\n                reference_portfolio = fund_df\n\n            total_debt = self.sum_rows_if(reference_portfolio, \n                                            grupo=['caixa', 'caixa_us', 'loan', 'termo', 'taxas e custos'],\n                                            row_to_sum=\"Mkt_Value\", modifier='reverse')\n            total_equity = self.sum_rows_if(fund_df, grupo=['a\u00e7\u00f5es'], row_to_sum=\"Mkt_Value\", modifier='reverse') \n            metric_val = total_debt/total_equity\n            if metric_val &gt; 0:\n                metric_val = 0\n            metric_val = abs(metric_val)\n            #print(f\"Fund: {fund_name}, debt:{total_debt}, equity:{total_equity}, leverage:{metric_val}\")\n            if get_names: metrics_names.append(\"% Leverage vs stocks\")\n            fund_metrics.append(metric_val)\n\n\n            if get_names:\n                # preenche df com nomes das metricas e ordem de exibicao\n                risk_df[\"M\u00e9trica\"] = metrics_names\n                order = [i for i in range(len(metrics_names))]\n                risk_df[\"Ordem\"] = order\n                get_names = False\n\n            # adiciona metricas do fundo\n            risk_df[fund_name] = fund_metrics\n            if fund_name == \"lipizzaner\":\n                risk_df[\"mangalarga_fic_fia\"] = fund_metrics  \n\n        return risk_df\n\n\n    def compute_positions_variation(self):\n        \"\"\"\n            Calcula a variacao de posicoes de cada fundo num periodo determidado e gera um output\n        \"\"\"\n        fund_names = [\"Lipizzaner\", \"Fund A\", \"Fund B\", \"Mangalarga FIC FIA\"]\n\n        # Definindo data mais atual(hoje), numero de dias que vamos voltar e data anterior\n        recent_date = self.query_mngr.get_bday_offset(self.today,offset=0)\n\n        days_offset = dt.timedelta(days=7)\n        if dt.date.today() == dt.date(2025,2,10):\n            #Ajuste da data, para pegar variacao completa do periodo de aporte no IP Atlas\n            days_offset = dt.timedelta(days=11) \n\n        previous_date = self.query_mngr.get_bday_offset(self.today-days_offset,offset=0)\n        print(f\"delta_positions_date: {previous_date}\")\n\n        assets = self.query_mngr.get_table(\"assets\")\n\n        variations = []\n\n        for f_name in fund_names:\n\n            spx_hawker_keys = [\"spx hawker_spx hawker cl aset18\", \"spx hawker_spx seg hawker jan22\", \"spx hawker_spx hawker cl amar22\", \"spx hawker_spx seg hawker feb22\"]\n\n            diff = self.query_mngr.get_position_variation(f_name.lower(), recent_date, previous_date).query(\"Key not in @spx_hawker_keys\")\n\n            closed = list(diff.loc[diff[\"Closed\"], \"Key\"])\n            bought = list(diff.loc[diff[\"Variation\"] &gt; 0.01, \"Key\"])\n            sold = list(diff.loc[((diff[\"Variation\"] &lt; -0.01) &amp; (~diff[\"Closed\"])), \"Key\"])\n\n            f_variations = [f_name]\n            for k in closed:\n                if \"term debt\" not in k.lower():\n                    row =  assets[assets[\"Key\"] == k]\n                    name = row[\"Name\"].squeeze()\n                    modifier = \"Zeragem \"\n                    f_variations.append(modifier+name)\n\n            for k in bought:\n                if \"term debt\" not in k.lower():\n                    row =  assets[assets[\"Key\"] == k]\n                    name = row[\"Name\"].squeeze()\n                    modifier = \"B \"\n                    if row[\"Group\"].squeeze() == \"vc\":\n                        modifier = \"\"\n                    f_variations.append(modifier+name)\n\n            for k in sold:\n                if \"term debt\" not in k.lower():\n                    row =  assets[assets[\"Key\"] == k]\n                    name = row[\"Name\"].squeeze()\n                    modifier = \"S \"\n\n                    f_variations.append(modifier+name)\n\n            variations.append(f_variations.copy())\n\n        variations = pd.DataFrame(variations).transpose()\n        variations = variations.rename(columns=variations.iloc[0]).drop(variations.index[0]).fillna(\"-\")\n\n        return variations\n\n\n    def compute_metrics_variation(self):\n\n        fund_names = [\"Lipizzaner\", \"Fund A\", \"Fund B\", \"Mangalarga FIC FIA\", \"Maratona\"]\n        variations = None\n        # Definindo data mais atual(hoje), numero de dias que vamos voltar e data anterior\n        recent_date = self.query_mngr.get_bday_offset(self.today,offset=0)\n        days_offset = dt.timedelta(days=7)\n\n        days_offset = dt.timedelta(days=7)\n        if dt.date.today() == dt.date(2025,2,10):\n            #Ajuste da data, para pegar variacao completa do periodo de aporte no IP Atlas\n            days_offset = dt.timedelta(days=11) \n\n        previous_date = self.query_mngr.get_bday_offset(self.today-days_offset,offset=0)\n\n        print(f\"delta_metrics_date: {previous_date}\")\n\n        for f_name in fund_names:\n\n            metrics_variations = self.query_mngr.get_risk_metric_variation(f_name.lower(), recent_date, previous_date)\n            metrics_variations[\"Fund\"] = f_name.lower()\n            if variations is None:\n                variations = metrics_variations.copy()\n            else:\n                variations = pd.concat([variations, metrics_variations])\n\n        return variations\n\n\n    def process_assets_concat(self, funds):\n        \"\"\"Agrupa as linhas que representam o mesmo 'Asset'\n           acumulando os percentuais e editando o nome mostrado.\n\n           Aten\u00e7\u00e3o: m\u00e9todo modifica diretamente self.funds\n        \"\"\"\n        def __concat_asset(gpd_rows, fund_name):\n\n            if len(gpd_rows) &lt;= 1:\n                return gpd_rows\n\n            # Obtendo do gpd_rows, o valor do asset referente ao grupo\n\n            rows_to_delete = gpd_rows.index.tolist()\n\n            # separa um row para ser alterado\n            row = gpd_rows.iloc[0].copy()\n\n            qtd, mkt_val, pct_pl = 0,0,0\n            pct_grouped = \"\"\n\n            # somando valores que precisam ficar nos dados agrupados\n            for idx, row in gpd_rows.iterrows():\n                qtd += row[\"Quantidade\"]\n                mkt_val += row[\"Mkt_Value\"]\n                pct_pl += row[\"%_pl\"]\n                gpd_name = \" \".join(row[\"Ticker\"].split(\" \")[:3]) if row[\"Grupo\"] != \"vc\" and row[\"Grupo\"] != \"private equity\" and row[\"Tipo\"] != 'fidc trybe' else row[\"Ticker\"]\n\n                #gpd_name = row[\"Ticker\"].split(\" \")[0] if row[\"Grupo\"] != \"vc\" and row[\"Tipo\"] != 'fidc trybe' else row[\"Ticker\"]\n\n                pct_grouped += gpd_name.upper() +\" \\t\"+ \"{:.2f}\".format(row[\"%_pl\"]*100) + \"%\\n\"\n\n            if row[\"Grupo\"] == \"vc\" or row[\"Grupo\"] == 'private equity':\n                if row[\"Nome\"].lower() in ['iliquids bz', 'iliquids us']:\n                    asset = row[\"Nome\"].lower() \n                    row[\"Asset\"] = asset\n\n            row[\"Quantidade\"] = qtd\n            row[\"Mkt_Value\"] = mkt_val\n            row[\"%_pl\"] = pct_pl\n            if qtd != 0:\n                row[\"Price\"] = mkt_val/qtd\n            row[\"%_pl_abs\"] = abs(pct_pl)\n            row[\"%_agrupados\"] = pct_grouped.strip(\"\\n\")\n\n            # precisamos descobrir qual categoria de ativos estamos agrupando\n            # para saber como personalizar o nome mostrado \n            # podemos obter uma lista dos tipos presentes:\n            asset_types = gpd_rows[\"Tipo\"].tolist()\n\n            try: \n                # Descomentar caso voltemos a ter spacs \n                #if (\"spac\" in asset_types) or (\"spac_w\" in asset_types):\n                #    # mapear alteracao para spacs\n                #    extra_pct = gpd_rows.loc[ gpd_rows[\"Tipo\"] == \"spac_w\", \"%_pl\"].iloc[0]\n                #    extra_pct = \"{:.1f}\".format(extra_pct*100)\n                #    row[\"Nome\"] = row[\"Nome\"] + \"|Warrant \" + extra_pct + \"%\"\n\n                if \"omega\" in asset_types:\n                    # mapear alteracao para omega|poraque\n                    pct_omge = gpd_rows.loc[ gpd_rows[\"Tipo\"] == \"a\u00e7\u00f5es\", \"%_pl\"].iloc[0]\n                    pct_omge = \"{:.1f}\".format(pct_omge*100)\n                    row[\"Nome\"] += \"|OMGE3 \" + pct_omge + \"%\"\n\n                #elif \"unidas\" in asset_types:\n                #    # mapear alteracao para localiza|unidas\n                #    pct_lcam = gpd_rows.loc[ gpd_rows[\"Tipo\"] == \"unidas\", \"%_pl\"].iloc[0]\n                #    pct_lcam = \"{:.1f}\".format(pct_lcam*100)\n                #    row[\"Nome\"] += \"|LCAM3 \" + pct_lcam + \"%\"\n\n\n                if (\"a\u00e7\u00f5es_bdr\" in asset_types) and (fund_name in self.fund_names[\"trading\"]):\n                    # personalizar com qtd BDRs\n                    pct_bdr = gpd_rows.loc[ gpd_rows[\"Tipo\"] == \"a\u00e7\u00f5es_bdr\", \"%_pl\"].iloc[0]\n                    pct_bdr = \"{:.1f}\".format(pct_bdr*100)\n                    row[\"Nome\"] += \"|BDR \" + pct_bdr + \"%\"\n\n            except Exception:\n                logger.error(\"Falha ao tentar formatar nome dos ativos agrupados.\")\n\n                sys.exit()   \n\n            for idx in rows_to_delete:\n                self.funds[fund_name].drop(idx, inplace=True)\n\n\n            self.funds[fund_name].loc[rows_to_delete[0]] = row\n\n        # Repetimos o mesmo processo para todas as carteiras\n\n        for fund_name, fund_df in funds.items():\n\n\n            #fund_df[\"Asset\"] = fund_df[\"Asset\"].apply(lambda x: x.replace(\"fidc trybe ii\", \"fidc trybe\").replace(\"fidc trybe i\", \"fidc trybe\"))\n            #fund_df[\"Nome\"] = fund_df[\"Nome\"].apply(lambda x: x.replace(\"FIDC Trybe II\", \"FIDC Trybe\").replace(\"FIDC Trybe I\", \"FIDC Trybe\"))\n\n            # primeiro colocamos uma nova coluna para manter % dos rows agrupados\n            fund_df[\"%_agrupados\"] = None\n            fund_df.reset_index(drop=True).groupby(\"Asset\", as_index=False).apply(\n                        lambda gpd_rows: __concat_asset(gpd_rows, fund_name),\n                        include_groups=False\n                        )\n\n\n    #def split_assets_rule(self, funds):\n#\n    #    aportes_resgates = {\n    #        \"mangalarga master\" : {\"spx hawker\" : 3300000.00 * self.USD_BRL, \"verde\" : 7600000},\n    #        \"lipi\" : {\"verde\" : 1625000, \"capstone\" : 1075000}\n    #    }\n    #    \n    #    for fund_name in funds:\n#\n    #        data = self.funds[fund_name].copy()\n    #        data[\"%_pl_mov\"] = 0\n    #        data[\"%_pl_abs_mov\"] = 0\n    #        rows_to_append = []\n    #        for idx, row in data.iterrows():\n    #            split_row = False\n    #            mkt_val_var = 0\n    #            if row[\"Fundo\"] in aportes_resgates:\n    #                if row[\"Asset\"] in aportes_resgates[row[\"Fundo\"]]:\n    #                    split_row = True\n    #                    mkt_val_var = aportes_resgates[row[\"Fundo\"]][row[\"Asset\"]]\n    #                elif row[\"Ticker\"] in aportes_resgates[row[\"Fundo\"]]:\n    #                    split_row = True\n    #                    mkt_val_var = aportes_resgates[row[\"Fundo\"]][row[\"Ticker\"]]\n    #            if split_row:\n    #                \n    #                new_row = row.copy()\n    #                row[\"Mkt_Value\"] -= mkt_val_var\n    #                row[\"%_pl\"] -= mkt_val_var/self.pls[fund_name]\n    #                row[\"%_pl_abs\"] -= abs(mkt_val_var/self.pls[fund_name])\n    #                \n    #                new_row[\"Mkt_Value\"] = mkt_val_var\n    #                new_row[\"Cor\"] = self.colors[\"verde_escuro\"]\n    #                new_row[\"%_pl_mov\"] = mkt_val_var/self.pls[fund_name]\n    #                new_row[\"%_pl_abs_mov\"] = abs(mkt_val_var/self.pls[fund_name])\n    #                #new_row[\"Asset\"] = row[\"Asset\"]+\"_mov\"\n    #                rows_to_append.append(new_row)\n    #                    \n    #            \n    #        for row in rows_to_append:\n    #            data.loc[len(data)] = row\n#\n    #        self.funds[fund_name] = data\n#\n\n    def define_assets_order(self, funds):\n\n        for fund_name, fund_df in funds.items():\n            order = []\n\n            for idx, fund_row in fund_df.iterrows():\n                order_key = fund_row[\"Grupo\"]\n                if fund_row[\"Tipo\"] in [\"spac\", \"spac_w\"]:\n                    order_key = fund_row[\"Tipo\"] \n\n                mod = self.order_modifiers[order_key]\n                mod = mod - fund_row[\"Mkt_Value\"]\n                order.append(mod)\n\n            funds[fund_name][\"Ordem\"] = order\n\n        return self.funds\n\n\n    def __create_exec_info(self):\n        \"\"\"Coloca Data e Hora da execu\u00e7\u00e3o num DataFrame.\"\"\"\n        #date_time = dt.datetime.now()\n\n        #date_time = date_time.strftime(\"%d/%m/%Y %H:%M:%S\").split()\n        date = dt.datetime.today().strftime(\"%d/%m/%Y\")\n        time = self.query_mngr.get_px_update_time().strftime(\"%H:%M:%S\")\n\n\n        #Desconto unidas TODO: TEMPORARIO, REMOVER DEPOIS\n        #Portfolio Long/PL\n        #px_localiza = self.asset_prices.loc[\"rent3 bz equity\"].iloc[0]\n        #unidas_pos_deal = px_localiza*0.4468\n\n        #px_unidas = self.asset_prices.loc[\"lcam3 bz equity\"].iloc[0]\n        #unidas_ex_div = px_unidas - 0.835414\n        #desconto_unidas = unidas_pos_deal/unidas_ex_div -1\n        #print(f\"desconto lcam/rent: {desconto_unidas}\" )\n\n        #df = pd.DataFrame({\n        #    \"Data\" : [date_time[0]],\n        #    \"Hora\" : [date_time[1]],\n        #    \"Desc_LCAM3\" : 1,\n        #})\n        df = pd.DataFrame({\n            \"Data\" : [date],\n            \"Hora\" : [time],\n            \"Desc_LCAM3\" : 1,\n        })\n        return df\n\n    def __create_lipi_investors_table(self):\n\n        lipi_investors = pd.DataFrame({}, columns=[\"Investor\", \"%_invested\"])\n\n        manga_fic_invested = self.query_mngr.calculate_fund_particip(\"mangalarga fic fia\", \"lipizzaner\", date=self.today)\n        manga_ii_fic_invested = self.query_mngr.calculate_fund_particip(\"mangalarga ii fic fia\", \"lipizzaner\", date=self.today)\n        eduardo_invested = self.query_mngr.get_eduardo_lipi_particip(date=self.today) #1 - manga_fic_invested - manga_ii_fic_invested\n\n        lipi_investors.loc[len(lipi_investors)] = {\"Investor\" : \"% Manga FIC\", \"%_invested\" : manga_fic_invested}\n        lipi_investors.loc[len(lipi_investors)] = {\"Investor\" : \"% Manga II FIC\", \"%_invested\" : manga_ii_fic_invested}\n        lipi_investors.loc[len(lipi_investors)] = {\"Investor\" : '% Eduardo', \"%_invested\" : eduardo_invested}\n\n        return lipi_investors\n\n\n    def calculate_day_attr(self, funds):\n\n        funds_updated = {}\n        def __calculate_attr(fund_name, prev_ptf, cur_ptf_row):\n\n            if cur_ptf_row[\"Quantidade\"] == 0: return 0\n\n            if cur_ptf_row[\"Tipo\"] in [\"caixa\", \"caixa_us\"] and fund_name != \"lipizzaner_acoes\":\n                return 0\n\n            asset_key = cur_ptf_row[\"Key\"]\n            prev_asset_row = prev_ptf.query(\"Key == @asset_key\")\n            if len(prev_asset_row) != 1:\n                return 0\n\n            # obtendo peso do ativo na carteira anterior\n            prev_weight = prev_asset_row[\"%_pl\"].squeeze()\n            variation = 0\n            if cur_ptf_row[\"Tipo\"] not in [\"caixa\", \"caixa_us\"]:\n\n                # Determinando preco atual, a partir do valor de mercado e quantidades\n                cur_mkt_val = cur_ptf_row[\"Mkt_Value\"]\n                cur_amnt = cur_ptf_row[\"Quantidade\"]\n                cur_price = cur_mkt_val/cur_amnt\n\n                prev_mkt_val = prev_asset_row[\"Mkt_Value\"].squeeze()\n                prev_amnt = prev_asset_row[\"Quantidade\"].squeeze()\n                prev_price = prev_mkt_val/prev_amnt\n                if prev_price == 0: return 0\n\n                variation = (cur_price/prev_price) -1\n            elif cur_ptf_row[\"Tipo\"] == \"caixa\":\n                variation = cur_ptf_row[\"Selic\"]/prev_asset_row[\"Selic\"].squeeze()-1\n\n            elif cur_ptf_row[\"Tipo\"] == \"caixa_us\":\n                variation = cur_ptf_row[\"USDBRL\"]/prev_asset_row[\"USDBRL\"].squeeze()-1\n\n            return prev_weight * variation\n\n        #Obtendo portfolios do dia anterior\n        prev_ptfs_path = Path(\"carteiras_luxor_historico\")/\"bases_completas\"\n        filename = \"base_portfolios_\" + str(self.yesterday) + \".xlsx\"\n        prev_ptfs_file_path = prev_ptfs_path/filename\n        prev_ptfs = pd.read_excel(prev_ptfs_file_path, sheet_name=None)\n\n        #print(prev_ptfs_file_path)\n        # Iterando pelos portfolios atualizados de hoje\n        for fund_name, fund_df in funds.items():\n\n            fund_df[\"Key\"] = fund_df[\"Asset\"] + fund_df[\"Ticker\"]\n            #try:\n            prev_ptf = prev_ptfs[fund_name]\n\n            fund_df[\"USDBRL\"] = self.USD_BRL\n            fund_df[\"Date\"] = self.today\n            fund_df[\"Selic\"] = self.query_mngr.get_price(\"bzacselc index\")\n            fund_df[\"Key\"] = fund_df[\"Asset\"] + \"_\" + fund_df[\"Ticker\"]\n\n\n            fund_df[\"%_attr\"] = fund_df.apply(lambda row: __calculate_attr(fund_name, prev_ptf, row), axis=1)\n            #except:\n                #print(f\"Nao foi encotrado portfolio anterior para a carteira '{fund_name}'\")\n                #pass\n\n            funds_updated[fund_name] = fund_df.copy()\n\n        return funds_updated\n\n\n    def set_date_and_usdbrl(self, funds):\n\n        funds_updated = {}\n        for fund_name, fund_df in funds.items():\n            fund_df[\"Date\"] = self.today\n            fund_df['USDBRL'] = self.USD_BRL\n            funds_updated[fund_name] = fund_df.copy()\n\n\n        return funds_updated\n\n\n    def save_portfolio_full_database(self, funds):\n        filename = \"base_portfolios.xlsx\"\n        path = Path(\"bases_output\")/filename\n\n\n        writer = pd.ExcelWriter(path, engine='xlsxwriter')\n\n        for fund_name, fund_df in funds.items():\n            fund_df.to_excel(writer, index=False, sheet_name=fund_name)\n\n        writer.close()\n\n        if (dt.datetime.now().hour &gt;= 17)  or (self.today&lt;dt.date.today()) : #or (dt.datetime.now().hour &lt; 9):\n            # entao vamos salvar uma base extra para controle de exposicao\n            date = self.today\n            #if dt.datetime.now().hour &lt; 9:\n            #    # salvamos antes da abertura o fechamento de ontem -&gt; Nao, pois esta deixando inconsistencias nas metricas.\n            #    date -= dt.timedelta(days=1)\n\n            filename = \"base_portfolios\" + \"_\" + str(date)+\".xlsx\"\n            path_bases_completas = Path(\"carteiras_luxor_historico\")/\"bases_completas\"/filename\n            filename = \"Gerencial Luxor.xlsx\"\n            path_reconciliacao = Path().absolute().parents[3]/\"BackOffice\"/\"01 - Carteiras\"/\"Concilia\u00e7\u00e3o Di\u00e1ria Bancos\"/filename\n\n            for path in [path_bases_completas, path_reconciliacao]:\n                writer = pd.ExcelWriter(path, engine='xlsxwriter')\n                for fund_name, fund_df in funds.items():\n                    fund_df.to_excel(writer, index=False, sheet_name=fund_name)\n\n                committed_capital_data = pd.read_excel(self.input_bases_dir/self.input_base_file, sheet_name=\"Committed Capital\")\n                committed_capital_data.to_excel(writer, index=False, sheet_name=\"committed_capital\")\n\n                writer.close()\n\n\n    def save_portfolio_risk_metrics(self):\n\n        # Salvamos sempre ~~apenas com o mercado fechado~~\n        #if dt.datetime.now().hour &gt;= 17 or dt.datetime.now().hour &lt; 9:\n        df = self.risk_df.copy()\n\n        today = self.today\n        # if dt.datetime.now().hour &lt; 9: \n        #     # Entao estamos rodando a carteira de ontem!! -&gt; Nao. Vamos rodar a de ontem somente quando solicitado explicitamente!\n        #     # Pegamos entao o dia util anterior\n        #     today = today - dt.timedelta(days = max(1, (today.weekday() + 6) % 7 -3) )\n\n        df[\"Date\"] = today\n        filename = \"hist_risk_metrics.xlsx\"\n        filepath = Path(\"carteiras_luxor_historico\")/\"metricas_risco\"/filename\n\n        # Vamos ler o conteudo do arquivo primeiro\n        data = pd.read_excel(filepath, engine=\"openpyxl\").dropna(how=\"any\")\n        data[\"Date\"] = data[\"Date\"].dt.date\n\n        # Vamos descartar tudo que estiver no dia de hoje ou depois\n        data = data.loc[data[\"Date\"] &lt; today]\n\n        # Vamos juntar as bases. Em caso de n.a., repetimos o valor da metrica do dia anterior\n        data = pd.concat([data, df]).ffill()#fillna(method=\"ffill\")\n        print(data)\n        # Sobrescrevendo arquivo com dados atualizados\n        data.to_excel(filepath, index=False)\n\n\n    def save_portfolio(self, funds):\n\n        filename = \"base_carteira_online_v12.xlsx\"\n        if self.is_develop_branch:\n            # Alteramos arquivo de saida se for branch develop\n            filename = \"base_carteira_online_v12_develop.xlsx\"\n\n        paths = [\n            os.path.join(self.onedrive_path, filename),\n            os.path.join(os.getcwd(), \"bases_output\", filename)\n        ]\n        for path in paths:       \n            writer = pd.ExcelWriter(path, engine='xlsxwriter')\n\n            for fund_name, fund_df in funds.items():\n\n                f = fund_df.copy()\n                #f = f.drop(\"Luxor_Classification\", axis=1)\n                f.to_excel(writer, index=False, sheet_name=fund_name)\n\n            writer.close()\n\n\n    def run(self):\n\n        self.query_mngr.update()\n        self.build()\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.__create_exec_info","title":"<code>__create_exec_info()</code>","text":"<p>Coloca Data e Hora da execu\u00e7\u00e3o num DataFrame.</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def __create_exec_info(self):\n    \"\"\"Coloca Data e Hora da execu\u00e7\u00e3o num DataFrame.\"\"\"\n    #date_time = dt.datetime.now()\n\n    #date_time = date_time.strftime(\"%d/%m/%Y %H:%M:%S\").split()\n    date = dt.datetime.today().strftime(\"%d/%m/%Y\")\n    time = self.query_mngr.get_px_update_time().strftime(\"%H:%M:%S\")\n\n\n    #Desconto unidas TODO: TEMPORARIO, REMOVER DEPOIS\n    #Portfolio Long/PL\n    #px_localiza = self.asset_prices.loc[\"rent3 bz equity\"].iloc[0]\n    #unidas_pos_deal = px_localiza*0.4468\n\n    #px_unidas = self.asset_prices.loc[\"lcam3 bz equity\"].iloc[0]\n    #unidas_ex_div = px_unidas - 0.835414\n    #desconto_unidas = unidas_pos_deal/unidas_ex_div -1\n    #print(f\"desconto lcam/rent: {desconto_unidas}\" )\n\n    #df = pd.DataFrame({\n    #    \"Data\" : [date_time[0]],\n    #    \"Hora\" : [date_time[1]],\n    #    \"Desc_LCAM3\" : 1,\n    #})\n    df = pd.DataFrame({\n        \"Data\" : [date],\n        \"Hora\" : [time],\n        \"Desc_LCAM3\" : 1,\n    })\n    return df\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.__load_funds_base","title":"<code>__load_funds_base(filename)</code>","text":"<p>Carrega base de dados com as carteiras dos fundos. Concatena todas numa tabela s\u00f3. Args:     file_name (str): caminho para arquivo .xlsx</p> <p>Returns:</p> Type Description <p>Dataframe de pandas</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def __load_funds_base(self, filename):\n    \"\"\"\n        Carrega base de dados com as carteiras dos fundos.\n        Concatena todas numa tabela s\u00f3.\n        Args:\n            file_name (str): caminho para arquivo .xlsx\n\n        Returns:\n            Dataframe de pandas\n    \"\"\"\n    try:\n        data = pd.read_excel(self.input_bases_dir/filename, sheet_name=[\"Lipizzaner\", \"Fund A\", \"Fund B\", \"HMX\", \"Mangalarga FIC\", \"Maratona\"])        \n\n        # tratando dados:\n        for k, v in data.items():\n            data[k] = v[ v.columns[:4] ].dropna(how=\"any\")\n            data[k] = data[k].loc[ ((data[k][\"Quantidade\"] != 0) | (data[k][\"Ticker\"] == \"Caixa (a receber)\")), :]\n\n        # caixa a receber\n        f_data = data[\"Fund A\"]\n        idx_mod = f_data.loc[((f_data[\"Asset\"] == \"Caixa (a receber)\") &amp; (f_data[\"Ticker\"] == \"Caixa (a receber)\"))].index[0]\n\n        f_data.at[idx_mod, \"Quantidade\"] = 0/self.USD_BRL\n        data[\"Fund A\"] = f_data.copy()\n\n        f_data = data[\"Lipizzaner\"]\n        idx_mod = f_data.loc[((f_data[\"Asset\"] == \"Caixa (a receber)\") &amp; (f_data[\"Ticker\"] == \"Caixa (a receber)\"))].index[0]\n        f_data.at[idx_mod, \"Quantidade\"] = 0/self.USD_BRL # global vai multiplicar pelo mesmo dolar depois e netar\n        data[\"Lipizzaner\"] = f_data.copy()\n\n        # Seguiremos com os dados concatenados para facilitar por enquanto\n        data = pd.concat(data.values(), ignore_index=True)\n\n        # Antes de converter para lower case, copiar coluna Assets\n        assets_copy = data[\"Asset\"]\n        # converte todo o texto para lower case\n        data = data.apply(lambda row: row.astype(str).str.lower())\n\n        # remove espacos em branco do inicio e do fim de cada entrada\n        data = data.apply(lambda row: row.str.strip())\n\n        # coluna quantidade precisa ser reconvertida para numerico\n        try:\n            data[\"Quantidade\"] = pd.to_numeric(data[\"Quantidade\"]).round(8)\n        except:\n            logger.error(\"Voce errou, humano.\")\n            logger.error(\"Tem texto onde era pra ter quantidade.\")\n            logger.error(data[\"Quantidade\"])\n            data.to_excel(\"error.xlsx\")\n            sys.exit()\n\n        # Inserindo coluna com nomes personalizavel\n        data[\"Nome\"] = assets_copy\n        data[\"Nome\"] = data[\"Nome\"].apply(lambda x: x.replace(\" |\", \"|\").replace(\"| \", \"|\"))\n\n        cols = [\"Nome\"] + [col for col in data.columns if col != \"Nome\"]\n\n        data = data[cols]\n\n        return data\n\n    except FileNotFoundError:\n        logger.error(f\"Arquivo {filename} n\u00e3o encontrado em {self.input_bases_dir}\")\n        logger.error(\"Finalizando...\")\n        sys.exit()\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.__update_colors","title":"<code>__update_colors(data)</code>","text":"<p>Atualiza a cor com base em algum criterio. Por enquanto, apenas coloca como vermelho se o market value for negativo.</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def __update_colors(self, data):\n    \"\"\"Atualiza a cor com base em algum criterio.\n    Por enquanto, apenas coloca como vermelho se o\n    market value for negativo.\n    \"\"\"\n    for idx, row in data.iterrows():\n        if row[\"Mkt_Value\"] &lt; 0:\n            data.loc[idx, \"Cor\"] = self.colors[\"vermelho\"]\n        else:\n            data.loc[idx, \"Cor\"] = self.color_map[row[\"Grupo\"]]\n\n    return data\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.calculate_MktVal","title":"<code>calculate_MktVal(data)</code>","text":"<p>Calcula o valor de mercado de cada ativo em cada fundo base</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def calculate_MktVal(self, data):\n    \"\"\"\n    Calcula o valor de mercado de cada ativo em cada fundo base\n    \"\"\"\n    def __get_MktVal(row):\n        #if row[\"Grupo\"] == \"fut_dol\":\n        #    return 0\n\n        if row[\"Tipo\"] == 'ndf usdbrl':\n            # Vai ser calculado pela variacao em relacao ao preco medio *  quantidade\n            avg_price = self.query_mngr.get_avg_price(row[\"Fundo\"], row[\"Asset\"]+\"_\"+row[\"Ticker\"], self.today)\n            if avg_price &lt; 0.1 and row[\"Fundo\"] == 'lipizzaner':\n                # ativo esta pelo fund a\n                avg_price = self.query_mngr.get_avg_price('fund a', row[\"Asset\"]+\"_\"+row[\"Ticker\"], self.today)\n\n            ndf_price = self.query_mngr.get_price(row[\"Ticker\"])\n\n            if row['Fundo'] in self.fund_names[\"onshore\"]:\n                return row[\"Quantidade\"] * (ndf_price-avg_price) # Resultado em BRL\n            return row[\"Quantidade\"] * ((ndf_price/avg_price)-1) # Resultado em USD\n\n        if row[\"Fundo\"] in self.fund_names[\"onshore\"]:\n            # Nos onshore precisamos corrigir alguns precos pelo dolar\n            if row[\"Location\"] == \"us\" and row[\"Ticker\"] not in [\"spx seg hawker jun23\", \"SPX SEG HAWKER SEP23\".lower()]:\n                return row[\"Quantidade\"] * row[\"Price\"] * self.USD_BRL\n\n        return row[\"Quantidade\"] * row[\"Price\"]\n\n    data[\"Mkt_Value\"] = data.apply(lambda row: __get_MktVal(row), axis=1)\n    return data\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.calculate_percentages","title":"<code>calculate_percentages(funds, pls)</code>","text":"<p>Calcula o percentual de cada ativo no fundo.</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def calculate_percentages(self, funds, pls):\n    \"\"\"Calcula o percentual de cada ativo no fundo.\"\"\"\n\n    for fund_key, fund_df in funds.items():\n        # Repete para todos os fundos\n        percents = []\n        percents_abs = []\n\n        try:\n            for idx, row in fund_df.iterrows():\n                pct = row[\"Mkt_Value\"]/pls[fund_key]\n                percents.append(pct)\n                percents_abs.append(abs(pct))\n\n        except ZeroDivisionError:\n            logger.error(\"Em 'calculate_percentage', divisao por pl 0\")\n        fund_df[\"%_pl\"] = percents\n        fund_df[\"%_pl_abs\"] = percents_abs\n\n    return funds\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.compute_positions_variation","title":"<code>compute_positions_variation()</code>","text":"<p>Calcula a variacao de posicoes de cada fundo num periodo determidado e gera um output</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def compute_positions_variation(self):\n    \"\"\"\n        Calcula a variacao de posicoes de cada fundo num periodo determidado e gera um output\n    \"\"\"\n    fund_names = [\"Lipizzaner\", \"Fund A\", \"Fund B\", \"Mangalarga FIC FIA\"]\n\n    # Definindo data mais atual(hoje), numero de dias que vamos voltar e data anterior\n    recent_date = self.query_mngr.get_bday_offset(self.today,offset=0)\n\n    days_offset = dt.timedelta(days=7)\n    if dt.date.today() == dt.date(2025,2,10):\n        #Ajuste da data, para pegar variacao completa do periodo de aporte no IP Atlas\n        days_offset = dt.timedelta(days=11) \n\n    previous_date = self.query_mngr.get_bday_offset(self.today-days_offset,offset=0)\n    print(f\"delta_positions_date: {previous_date}\")\n\n    assets = self.query_mngr.get_table(\"assets\")\n\n    variations = []\n\n    for f_name in fund_names:\n\n        spx_hawker_keys = [\"spx hawker_spx hawker cl aset18\", \"spx hawker_spx seg hawker jan22\", \"spx hawker_spx hawker cl amar22\", \"spx hawker_spx seg hawker feb22\"]\n\n        diff = self.query_mngr.get_position_variation(f_name.lower(), recent_date, previous_date).query(\"Key not in @spx_hawker_keys\")\n\n        closed = list(diff.loc[diff[\"Closed\"], \"Key\"])\n        bought = list(diff.loc[diff[\"Variation\"] &gt; 0.01, \"Key\"])\n        sold = list(diff.loc[((diff[\"Variation\"] &lt; -0.01) &amp; (~diff[\"Closed\"])), \"Key\"])\n\n        f_variations = [f_name]\n        for k in closed:\n            if \"term debt\" not in k.lower():\n                row =  assets[assets[\"Key\"] == k]\n                name = row[\"Name\"].squeeze()\n                modifier = \"Zeragem \"\n                f_variations.append(modifier+name)\n\n        for k in bought:\n            if \"term debt\" not in k.lower():\n                row =  assets[assets[\"Key\"] == k]\n                name = row[\"Name\"].squeeze()\n                modifier = \"B \"\n                if row[\"Group\"].squeeze() == \"vc\":\n                    modifier = \"\"\n                f_variations.append(modifier+name)\n\n        for k in sold:\n            if \"term debt\" not in k.lower():\n                row =  assets[assets[\"Key\"] == k]\n                name = row[\"Name\"].squeeze()\n                modifier = \"S \"\n\n                f_variations.append(modifier+name)\n\n        variations.append(f_variations.copy())\n\n    variations = pd.DataFrame(variations).transpose()\n    variations = variations.rename(columns=variations.iloc[0]).drop(variations.index[0]).fillna(\"-\")\n\n    return variations\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.get_non_groupable_iliquids","title":"<code>get_non_groupable_iliquids(funds, metric=0.01)</code>","text":"<p>Gera dicionario de iliquids nao agrupaveis. Os valores sao os VCs/PEs numa lista e as chaves os nomes dos fundos onde nao sera agrupado Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def get_non_groupable_iliquids(self, funds, metric=0.01):\n    \"\"\"\n    Gera dicionario de iliquids nao agrupaveis.\n    Os valores sao os VCs/PEs numa lista e as chaves os nomes dos fundos\n    onde nao sera agrupado\n    Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'\n    \"\"\"\n    non_groupable = {\n        f_name:[] for f_name in self.fund_names[\"iliquids_funds\"]\n    }\n\n    for fund_name, fund_df in funds.items():\n        if fund_name not in self.fund_names[\"iliquids_funds\"]: continue\n\n        for idx, row in fund_df.iterrows():\n            if (row[\"Grupo\"] == \"vc\" or row[\"Grupo\"] == \"private equity\") \\\n                    and row[\"Ticker\"] not in non_groupable[fund_name]:\n                if row[\"%_pl_abs\"] &gt;= metric:\n                    non_groupable[fund_name].append(row[\"Ticker\"])\n\n\n    # fund_b + lipi recebe o mapa do fund_b (excecao)\n    non_groupable[\"edel_lipi\"] = non_groupable[\"fund_b\"][:]\n    non_groupable[\"mrtn_hmx\"] = non_groupable[\"hmx\"][:]\n\n    return non_groupable\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.get_non_groupable_iliquids_managers","title":"<code>get_non_groupable_iliquids_managers(funds, metric=0.01)</code>","text":"<p>Gera dicionario de iliquids nao agrupaveis. Os valores sao os tickers VCs/PEs numa lista e as chaves os nomes dos fundos onde nao sera agrupado Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def get_non_groupable_iliquids_managers(self, funds, metric=0.01):\n    \"\"\"\n    Gera dicionario de iliquids nao agrupaveis.\n    Os valores sao os tickers VCs/PEs numa lista e as chaves os nomes dos fundos\n    onde nao sera agrupado\n    Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'\n    \"\"\"\n    non_groupable = {\n        f_name:[] for f_name in self.fund_names[\"iliquids_funds\"]\n    }\n\n    for fund_name, fund_df in funds.items():\n        if fund_name not in self.fund_names[\"iliquids_funds\"]: continue\n\n        # na coluna Manager, vamos substituir os valores nan pelo valor da coluna Ticker\n        fund_df[\"Manager\"] = np.where((fund_df[\"Manager\"].isnull()) | (fund_df[\"Manager\"] == 'nan'),\n                                      fund_df[\"Ticker\"], fund_df[\"Manager\"])\n        grouped_by_manager = fund_df[[\"Manager\", \"%_pl_abs\"]].groupby(\"Manager\").sum().reset_index()\n        managers_not_to_group = grouped_by_manager.loc[grouped_by_manager[\"%_pl_abs\"] &gt;= metric, \"Manager\"]\n\n        tickers_not_to_group = fund_df.query(\"Manager in @managers_not_to_group and \\\n                                             (Grupo == 'vc' or Grupo == 'private equity')\")[\"Ticker\"]\n        for ticker in tickers_not_to_group:\n            non_groupable[fund_name].append(ticker)\n\n    # fund_b + lipi recebe o mapa do fund_b (excecao)\n    #non_groupable[\"edel_lipi\"] = non_groupable[\"fund_b\"][:]\n    #non_groupable[\"mrtn_hmx\"] = non_groupable[\"hmx\"][:]\n\n    return non_groupable\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.get_non_groupable_vcs","title":"<code>get_non_groupable_vcs(funds, metric=0.01)</code>","text":"<p>Gera dicionario de vcs nao agrupaveis. As chaves sao os VCs e os valores uma lista com os fundos onde nao sera agrupado Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def get_non_groupable_vcs(self, funds, metric=0.01):\n    \"\"\"\n    Gera dicionario de vcs nao agrupaveis.\n    As chaves sao os VCs e os valores uma lista com os fundos\n    onde nao sera agrupado\n    Ser\u00e1 agrup\u00e1vel se for menor que o % informado por 'metric'\n    \"\"\"\n    non_groupable = {\n        f_name:[] for f_name in self.fund_names[\"vc_funds_all\"]\n    }\n\n    for fund_name, fund_df in funds.items():\n        if fund_name not in self.fund_names[\"vc_funds\"]: continue\n\n        for idx, row in fund_df.iterrows():\n            if row[\"Grupo\"] == \"vc\" and row[\"Ticker\"] not in non_groupable[fund_name]:\n\n                if row[\"%_pl_abs\"] &gt;= metric:\n                    non_groupable[fund_name].append(row[\"Ticker\"])\n\n    for f_name in self.fund_names[\"luxor_ptf\"]:\n        if f_name != \"fund_a\":\n            non_groupable[f_name] = non_groupable[\"fund_a\"][:]\n\n    # fund_b + lipi recebe o mapa do fund_b (excecao)\n    non_groupable[\"edel_lipi\"] = non_groupable[\"fund_b\"][:]\n    non_groupable[\"mrtn_hmx\"] = non_groupable[\"hmx\"][:]\n\n\n    return non_groupable\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.get_pls","title":"<code>get_pls(funds)</code>","text":"<p>Recebe como input um dicionario de dataframes de fundos Soma a coluna Mkt_Value de cada um e retorna um dicion\u00e1rio de pls, usando mesma chave</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def get_pls(self, funds):\n    \"\"\"\n    Recebe como input um dicionario de dataframes de fundos\n    Soma a coluna Mkt_Value de cada um e retorna um dicion\u00e1rio\n    de pls, usando mesma chave\n    \"\"\"\n    pls = {}\n\n    for key, fund_df in funds.items():\n        pl = fund_df[\"Mkt_Value\"].sum()\n        pls[key] = pl\n\n    return pls\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.group_iliquids","title":"<code>group_iliquids(funds, non_groupable)</code>","text":"<p>Altera a coluna 'Nomes' deixando diferente apenas o nome dos Iliquids que nao serao agrupados e com mesmo nome os que serao.</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def group_iliquids(self, funds, non_groupable):\n    \"\"\"Altera a coluna 'Nomes' deixando diferente apenas o nome dos Iliquids\n        que nao serao agrupados e com mesmo nome os que serao.\"\"\"\n\n    for fund_name, fund_df in funds.items():\n        if fund_name not in self.fund_names[\"iliquids_funds\"]: continue\n        non_groupable_vcs = non_groupable[fund_name]\n\n        for idx, fund_row in fund_df.iterrows():\n            if fund_row[\"Grupo\"] == \"vc\" or fund_row[\"Grupo\"] == \"private equity\" or fund_row[\"Asset\"] == 'tarpon':\n                if fund_row[\"Ticker\"] in non_groupable_vcs:\n                    # Tanto o nome quanto asset passarao a ser o Manager\n                    # Quando o Manager for diferente da chave, vamos priorizar mostrar ele no nome\n                    if fund_row[\"Ticker\"] != fund_row[\"Manager\"]:\n                        funds[fund_name].loc[idx, \"Nome\"] = fund_row[\"Manager\"].title()\n                    else:\n                        # Vamos pegar o nome cadastrado na tabela ativos\n                        funds[fund_name].loc[idx, \"Nome\"] = self.query_mngr.get_asset_name(fund_row[\"Ticker\"].lower())\n                    #funds[fund_name].loc[idx, \"Asset\"] = fund_row[\"Ticker\"]\n                    funds[fund_name].loc[idx, \"Asset\"] = fund_row[\"Manager\"]\n\n                else:\n                    # Vamos trocar o Asset e o Nome para iliquids bz ou iliquids us \n                    location = fund_row[\"Currency Exposure\"]\n                    funds[fund_name].loc[idx, \"Asset\"] = \"iliquids \" + location\n                    funds[fund_name].loc[idx, \"Nome\"] = \"Iliquids \" + location.upper()\n\n    return funds\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.group_vcs","title":"<code>group_vcs(funds, non_groupable)</code>","text":"<p>Altera a coluna 'Nomes' deixando diferente apenas o nome dos VCs que nao serao agrupados e com mesmo nome os que serao.</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def group_vcs(self, funds, non_groupable):\n    \"\"\"Altera a coluna 'Nomes' deixando diferente apenas o nome dos VCs\n        que nao serao agrupados e com mesmo nome os que serao.\"\"\"\n\n    for fund_name, fund_df in funds.items():\n        if fund_name not in self.fund_names[\"vc_funds_all\"]: continue\n        non_groupable_vcs = non_groupable[fund_name]\n\n        for idx, fund_row in fund_df.iterrows():\n            if fund_row[\"Grupo\"] == \"vc\":\n\n                if fund_row[\"Ticker\"] in non_groupable_vcs:\n                    funds[fund_name].loc[idx, \"Nome\"] = self.query_mngr.get_asset_name(fund_row[\"Ticker\"].lower())\n                    funds[fund_name].loc[idx, \"Asset\"] = fund_row[\"Ticker\"]\n    return funds\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.merge_data","title":"<code>merge_data(assets_data, funds_data)</code>","text":"<p>Realiza jun\u00e7\u00e3o interna sobre as colunas 'Asset' e 'Ticker' Args:     assets_data (Pandas Dataframe): Base com dados dos ativos     funds_data (Pandas Dataframe): Base com dados dos fundos</p> <p>Returns:</p> Type Description <p>Pandas Dataframe: Base com os dados unificados</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def merge_data(self, assets_data, funds_data):\n    \"\"\"Realiza jun\u00e7\u00e3o interna sobre as colunas 'Asset' e 'Ticker'\n    Args:\n        assets_data (Pandas Dataframe): Base com dados dos ativos\n        funds_data (Pandas Dataframe): Base com dados dos fundos\n\n    Returns:\n        Pandas Dataframe: Base com os dados unificados\n    \"\"\"\n    data = pd.merge(funds_data, assets_data,\n                     how=\"inner\", on=[\"Asset\",\"Ticker\"])\n    return data\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.populate_prices_BBG","title":"<code>populate_prices_BBG(data)</code>","text":"<p>Popula a coluna 'Prices' com o preco atual de cada ativo.     Se nao for aplicavel, preenche com valor 1.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>Dataframe de pandas com dados da carteira e ativos   concatenados</p> required <p>Returns:     Dataframe de pandas com coluna de pre\u00e7os preenchida</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def populate_prices_BBG(self, data):\n    \"\"\"Popula a coluna 'Prices' com o preco atual de cada ativo.\n        Se nao for aplicavel, preenche com valor 1.\n\n    Args:\n        data: Dataframe de pandas com dados da carteira e ativos\n              concatenados\n    Returns:\n        Dataframe de pandas com coluna de pre\u00e7os preenchida\n    \"\"\"\n\n    def __get_price(row):\n        # Fun\u00e7\u00e3o auxiliar para preencher condicionalmente\n        swaps_zerados = True\n        # se nao tem ticker bbg, preco valera 1\n        if row[\"Ticker_BBG\"] == \"nan\": \n            # TODO calculos das metricas internamente\n            if row[\"Grupo\"] == \"metricas\":\n                return 0\n\n            if row[\"Grupo\"] == \"swap\":\n                if row[\"Fundo\"].lower() == \"maratona\":\n                    return 1\n                # Se for swap, retorna o p&amp;l\n                if row[\"Tipo\"] == \"swap_p&amp;l\":\n                    if \"liq\" in row[\"Ticker\"]:\n                        return 1\n                    if swaps_zerados : return 0\n                    #else:\n                        # \n                    return self.data_extractor.get_swap_pnl(row[\"Fundo\"].title())\n\n                elif row[\"Tipo\"] == \"swap_exp\":\n                    if swaps_zerados : return 0 # ALTERAR PARA PEGAR METRICA DE EXPOSICAO\n\n                    return self.data_extractor.get_swap_exp(row[\"Fundo\"].title())\n\n            #ip_atlas= \"Outrigger Fund Ltd - IP Atlas USD Equity Class - Subclass B\".lower()\n            #if row[\"Ticker\"] == ip_atlas:\n            #    return 1000\n\n            if row[\"Grupo\"] == 'termo':\n                # Vamos pegar o preco medio da execucao, que eh o preco do termo fixo\n                try:\n                    df = self.query_mngr.get_table(\"last_positions_by_bank\")\n                    ticker = row[\"Ticker\"].lower()\n\n                    price = df.query(\"Ticker == @ticker\").tail(1)[\"Avg_price\"].squeeze()\n                    return -price\n                except:\n                    logger.error(f\"Erro ao buscar preco do termo {ticker}\")\n                    pass\n\n            price = self.query_mngr.get_price(row[\"Ticker\"], px_date=self.today)\n            if price == 0 or price is None: #Nao encontrado\n                price = 1\n\n            #if row[\"Grupo\"] == \"termo\":\n            #    price = -price\n\n            return price\n\n        else:\n\n            price = self.query_mngr.get_price(row[\"Ticker_BBG\"])\n            #if row[\"Ticker_BBG\"] == 'lzrfy us equity':\n            #    price = self.query_mngr.get_price(\"rent3 bz equity\")\n            #    price = price/self.query_mngr.get_price(\"usdbrl curncy\")\n\n            # Preco do contrato de dolar vai ser a variacao de um dia pro outro\n            if row[\"Tipo\"] in [\"fut_dol\", \"fut_dol_min\"]:\n                px_today = self.query_mngr.get_price(row[\"Ticker_BBG\"])\n                prev_date = self.query_mngr.get_bday_offset(self.today, -1, location=\"bz\")\n                px_prev = self.query_mngr.get_price(row[\"Ticker_BBG\"], px_date=prev_date)\n                size = 50 if row[\"Tipo\"] == \"fut_dol\" else 10\n\n                price = (px_today - px_prev) * size\n\n            # Se for bdr, precisaremos ajustar pelo peso\n            if row[\"Tipo\"] == \"a\u00e7\u00f5es_bdr\":\n                bdr_ticker = row[\"Ticker\"]\n                peso = self.query_mngr.get_bdr_size(bdr_ticker)\n\n\n                #if bdr_ticker == \"amzo34 bz equity\":\n                #    print(\"-------&gt; PESO DE AMZO34 ALTERADO NA MAO!!!!!! &lt;-----------\")\n                #    peso = peso*20\n                price = price * self.USD_BRL / peso\n\n            # AJUSTES MANUAIS DE PRECO, FAZER AQUI!\n            #if row[\"Ticker\"] == \"poraque fic fip\" and price &lt; 50000:\n            #    print(\"&gt;&gt;&gt;&gt; PRE\u00c7O DE OMEGA AJUSTADO MANUALMENTE &lt;&lt;&lt;&lt;\")\n            #    \n            #    price = 216526.697109\n\n            return  price\n\n    data[\"Price\"] = data.apply(lambda row: __get_price(row), axis=1)\n\n    return data\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.process_assets_concat","title":"<code>process_assets_concat(funds)</code>","text":"<p>Agrupa as linhas que representam o mesmo 'Asset' acumulando os percentuais e editando o nome mostrado.</p> <p>Aten\u00e7\u00e3o: m\u00e9todo modifica diretamente self.funds</p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def process_assets_concat(self, funds):\n    \"\"\"Agrupa as linhas que representam o mesmo 'Asset'\n       acumulando os percentuais e editando o nome mostrado.\n\n       Aten\u00e7\u00e3o: m\u00e9todo modifica diretamente self.funds\n    \"\"\"\n    def __concat_asset(gpd_rows, fund_name):\n\n        if len(gpd_rows) &lt;= 1:\n            return gpd_rows\n\n        # Obtendo do gpd_rows, o valor do asset referente ao grupo\n\n        rows_to_delete = gpd_rows.index.tolist()\n\n        # separa um row para ser alterado\n        row = gpd_rows.iloc[0].copy()\n\n        qtd, mkt_val, pct_pl = 0,0,0\n        pct_grouped = \"\"\n\n        # somando valores que precisam ficar nos dados agrupados\n        for idx, row in gpd_rows.iterrows():\n            qtd += row[\"Quantidade\"]\n            mkt_val += row[\"Mkt_Value\"]\n            pct_pl += row[\"%_pl\"]\n            gpd_name = \" \".join(row[\"Ticker\"].split(\" \")[:3]) if row[\"Grupo\"] != \"vc\" and row[\"Grupo\"] != \"private equity\" and row[\"Tipo\"] != 'fidc trybe' else row[\"Ticker\"]\n\n            #gpd_name = row[\"Ticker\"].split(\" \")[0] if row[\"Grupo\"] != \"vc\" and row[\"Tipo\"] != 'fidc trybe' else row[\"Ticker\"]\n\n            pct_grouped += gpd_name.upper() +\" \\t\"+ \"{:.2f}\".format(row[\"%_pl\"]*100) + \"%\\n\"\n\n        if row[\"Grupo\"] == \"vc\" or row[\"Grupo\"] == 'private equity':\n            if row[\"Nome\"].lower() in ['iliquids bz', 'iliquids us']:\n                asset = row[\"Nome\"].lower() \n                row[\"Asset\"] = asset\n\n        row[\"Quantidade\"] = qtd\n        row[\"Mkt_Value\"] = mkt_val\n        row[\"%_pl\"] = pct_pl\n        if qtd != 0:\n            row[\"Price\"] = mkt_val/qtd\n        row[\"%_pl_abs\"] = abs(pct_pl)\n        row[\"%_agrupados\"] = pct_grouped.strip(\"\\n\")\n\n        # precisamos descobrir qual categoria de ativos estamos agrupando\n        # para saber como personalizar o nome mostrado \n        # podemos obter uma lista dos tipos presentes:\n        asset_types = gpd_rows[\"Tipo\"].tolist()\n\n        try: \n            # Descomentar caso voltemos a ter spacs \n            #if (\"spac\" in asset_types) or (\"spac_w\" in asset_types):\n            #    # mapear alteracao para spacs\n            #    extra_pct = gpd_rows.loc[ gpd_rows[\"Tipo\"] == \"spac_w\", \"%_pl\"].iloc[0]\n            #    extra_pct = \"{:.1f}\".format(extra_pct*100)\n            #    row[\"Nome\"] = row[\"Nome\"] + \"|Warrant \" + extra_pct + \"%\"\n\n            if \"omega\" in asset_types:\n                # mapear alteracao para omega|poraque\n                pct_omge = gpd_rows.loc[ gpd_rows[\"Tipo\"] == \"a\u00e7\u00f5es\", \"%_pl\"].iloc[0]\n                pct_omge = \"{:.1f}\".format(pct_omge*100)\n                row[\"Nome\"] += \"|OMGE3 \" + pct_omge + \"%\"\n\n            #elif \"unidas\" in asset_types:\n            #    # mapear alteracao para localiza|unidas\n            #    pct_lcam = gpd_rows.loc[ gpd_rows[\"Tipo\"] == \"unidas\", \"%_pl\"].iloc[0]\n            #    pct_lcam = \"{:.1f}\".format(pct_lcam*100)\n            #    row[\"Nome\"] += \"|LCAM3 \" + pct_lcam + \"%\"\n\n\n            if (\"a\u00e7\u00f5es_bdr\" in asset_types) and (fund_name in self.fund_names[\"trading\"]):\n                # personalizar com qtd BDRs\n                pct_bdr = gpd_rows.loc[ gpd_rows[\"Tipo\"] == \"a\u00e7\u00f5es_bdr\", \"%_pl\"].iloc[0]\n                pct_bdr = \"{:.1f}\".format(pct_bdr*100)\n                row[\"Nome\"] += \"|BDR \" + pct_bdr + \"%\"\n\n        except Exception:\n            logger.error(\"Falha ao tentar formatar nome dos ativos agrupados.\")\n\n            sys.exit()   \n\n        for idx in rows_to_delete:\n            self.funds[fund_name].drop(idx, inplace=True)\n\n\n        self.funds[fund_name].loc[rows_to_delete[0]] = row\n\n    # Repetimos o mesmo processo para todas as carteiras\n\n    for fund_name, fund_df in funds.items():\n\n\n        #fund_df[\"Asset\"] = fund_df[\"Asset\"].apply(lambda x: x.replace(\"fidc trybe ii\", \"fidc trybe\").replace(\"fidc trybe i\", \"fidc trybe\"))\n        #fund_df[\"Nome\"] = fund_df[\"Nome\"].apply(lambda x: x.replace(\"FIDC Trybe II\", \"FIDC Trybe\").replace(\"FIDC Trybe I\", \"FIDC Trybe\"))\n\n        # primeiro colocamos uma nova coluna para manter % dos rows agrupados\n        fund_df[\"%_agrupados\"] = None\n        fund_df.reset_index(drop=True).groupby(\"Asset\", as_index=False).apply(\n                    lambda gpd_rows: __concat_asset(gpd_rows, fund_name),\n                    include_groups=False\n                    )\n</code></pre>"},{"location":"modules/carteira_online/production/portfolio_builder/#carteira_online.production.portfolio_builder.PortfolioBuilder.sum_rows_if","title":"<code>sum_rows_if(fund_df, grupo=[], tipo=[], ticker=[], asset=[], row_to_sum='%_pl', modifier='', name_1='Grupo', name_2='Tipo', name_3='Ticker', name_4='Asset')</code>","text":"<p>Soma os % dos ativos em fund_df. No comportamento padr\u00e3o, ir\u00e1 excluir da soma aqueles que estiverem informados em <code>grupo</code>, <code>tipo</code>, <code>ticker</code> ou <code>asset</code>. Esse comportamento pode ser alterado pelo modificador modificador:      <code>reverse</code> - Passa a somar somente os ativos definidos em grupo, tipo e ticker     <code>grupo &amp; ! tipo</code> - Somar apenas se estiver em grupo e nao estiver em tipo     <code>if_negative</code> - soma somente valores negativos presentes na lista <code>grupo</code></p> Source code in <code>carteira_online\\production\\portfolio_builder.py</code> <pre><code>def sum_rows_if(self, fund_df, grupo = [], tipo = [], ticker = [], asset = [],\n                    row_to_sum = \"%_pl\", modifier=\"\", name_1=\"Grupo\", name_2=\"Tipo\", \n                    name_3=\"Ticker\", name_4=\"Asset\"):\n    \"\"\" Soma os % dos ativos em fund_df.\n    No comportamento padr\u00e3o, ir\u00e1 excluir da soma aqueles que estiverem informados em\n    `grupo`, `tipo`, `ticker` ou `asset`.\n    Esse comportamento pode ser alterado pelo modificador\n    modificador: \n        `reverse` - Passa a somar somente os ativos definidos em grupo, tipo e ticker\n        `grupo &amp; ! tipo` - Somar apenas se estiver em grupo e nao estiver em tipo\n        `if_negative` - soma somente valores negativos presentes na lista `grupo`\n    \"\"\"\n    summation = 0\n    if modifier == \"\":\n        # por padrao, soma tudo menos o que estiver nas listas\n        # precisa satisfazer todas as listas no mesmo row\n        for idx, row in fund_df.iterrows():\n            if (row[name_1] not in grupo) and (row[name_2] not in tipo) and (row[name_3] not in ticker) and (row[name_4] not in asset):\n                summation += row[row_to_sum]\n\n    elif modifier == \"reverse\":\n        # nessa variante, vai somar apenas se satisfizer as condicoes das listas\n        for idx, row in fund_df.iterrows():\n            if (len(grupo)== 0 or row[name_1] in grupo) and (len(tipo) == 0  or row[name_2] in tipo) and (len(ticker) == 0 or row[name_3] in ticker) and (len(asset) == 0 or row[name_4] in asset):\n                summation += row[row_to_sum]\n\n    elif modifier == \"grupo &amp; !tipo\":\n        # nessa variante, vai somar apenas se estiver em grupo e nao estiver em tipo\n        for idx, row in fund_df.iterrows():\n            if (len(grupo) &gt; 0 and row[name_1] in grupo) and (len(tipo) &gt; 0  and row[name_2] not in tipo):\n                summation += row[row_to_sum]\n    elif modifier == \"if_negative\":\n        # Soma todos os valores negativos\n        for idx, row in fund_df.iterrows():\n            if row[\"%_pl\"] &lt; 0:\n                if name_1 == \"Grupo\":\n                    if row[name_1] in grupo:\n                        summation += row[row_to_sum]\n                elif name_1 == \"Ticker\":\n                    if row[name_1] in ticker:\n                        summation += row[row_to_sum]\n    else:\n        logger.error(\"Erro: Modificador n\u00e3o identificado.\")\n\n\n    return summation\n</code></pre>"},{"location":"modules/carteira_online/production/return_calculator/","title":"<code>return_calculator.py</code>","text":"<p>Calcula a cota gerencial di\u00e1ria, al\u00e9m dos retornos 1D, MTD e YTD.</p> <p>Script de atualizacao da base historica de retornos.</p> <ul> <li>A cada mes e ano devemos alterar o preco da cota em 'self.last_quotas'</li> <li>As alteracoes de # cotas onshore eh lida automaticamente das carteiras do BTG</li> <li>As alteracoes de # cotas offshore precisam ser alteradas na m\u00e3o (em hist_returns.xlsx)</li> </ul> <p>Fluxo: 1. Carragamento das bases de hoje e do dia anterior e carregamento da base historica 2. Base historica lida \u00e9 segregada por fundo em um dicionario 3. Atualiza\u00e7\u00e3o dos fundos onshore:     3.1 Leitura das carteiras do BTG mais atual para resgatar qtd de cotas     3.2 Insere linha nova com dados do btg ou altera a #_cotas, recalculando cota e propagando alteracao 4.  5.  6.</p>"},{"location":"modules/carteira_online/production/return_calculator/#carteira_online.production.return_calculator.ReturnCalculator","title":"<code>ReturnCalculator</code>","text":"Source code in <code>carteira_online\\production\\return_calculator.py</code> <pre><code>class ReturnCalculator:\n\n    def __init__(self, today=dt.date.today()):\n\n        self.query_mngr = LuxorQuery(is_develop_mode=False)\n\n        self.today = today\n        self.yesterday = self.query_mngr.get_bday_offset(self.today, offset=-1, location='all')\n        self.last_month_end = dt.date(self.today.year, self.today.month, 1) - dt.timedelta(days=1)\n        self.last_year_end = dt.date(self.today.year, 1, 1) - dt.timedelta(days=1)\n\n        self.is_holiday_bz = self.query_mngr.is_holiday(self.today, \"bz\")\n        self.is_holiday_us = self.query_mngr.is_holiday(self.today, \"us\")\n\n\n        #print(f\"today: {self.today}\\nyesterday: {self.yesterday}\")\n        logger.info(f\"today: {self.today}\\nyesterday: {self.yesterday}\")\n\n        # Atualizando cotas de inicio de mes e de inicio de ano\n        self.previous_quotas = {\n           # \"manga\"  : [2.17550708005788, 2.72917283],  #31/12/2021  | 31/12/2021 \n            \"lipizzaner\"   : [self.query_mngr.get_price(\"lipizzaner\", px_date=self.last_month_end),\n                        self.query_mngr.get_price(\"lipizzaner\", px_date=self.last_year_end)],\n\n            #\"mangalarga master\" : [self.query_mngr.get_price(\"mangalarga master\", px_date=self.last_month_end),\n            #            self.query_mngr.get_price(\"mangalarga master\", px_date=self.last_year_end)], \n\n            \"fund a\" : [self.query_mngr.get_price(\"fund a\", px_date=self.last_month_end),\n                        self.query_mngr.get_price(\"fund a\", px_date=self.last_year_end)],\n\n            \"fund b\" : [self.query_mngr.get_price(\"fund b\", px_date=self.last_month_end),\n                        self.query_mngr.get_price(\"fund b\", px_date=self.last_year_end)],\n\n            \"hmx\"    : [self.query_mngr.get_price(\"hmx\", px_date=self.last_month_end),\n                        self.query_mngr.get_price(\"hmx\", px_date=self.last_year_end)],\n            \"mangalarga fic fia\" : [self.query_mngr.get_price(\"mangalarga fic fia\", px_date=self.last_month_end),\n                                 self.query_mngr.get_price(\"mangalarga fic fia\", px_date=self.last_year_end)],\n\n            \"maratona\" : [self.query_mngr.get_price(\"maratona\", px_date=self.last_month_end),\n                                 self.query_mngr.get_price(\"maratona\", px_date=self.last_year_end)],\n        }\n\n\n        #add_data_to_ptfs.add_data_to_ptfs(self.query_mngr, n_days=2)\n        #self.luxor_equities_quotas = equities_return_updater.build_equities_hist_return(self.query_mngr, self.today, Path(\"bases_historicas\"))\n\n        # definindo base de portfolios do dia de hoje\n        self.ptf_path = os.path.join(\"bases_output\", \"base_portfolios.xlsx\")\n        # definindo base de portfolios do dia anterior mais recente\n        self.prev_pft_path = self.get_prev_pft_path(directory=os.path.join(\"carteiras_luxor_historico\", \"bases_completas\") )\n\n        # definindo nome do arquivo de saida \n        self.returns_path = os.path.join(\"bases_historicas\", \"historical_return.xlsx\")\n\n        # padronizado para cd .. root folder -&gt; os.pardir\n        self.onedrive_path = os.pardir\n\n        # Inicializamos extrator de dados(f\u00e7s bbg)\n        self.data_extractor = adex.AssetDataExtractor(query_mngr=self.query_mngr)\n\n        # Criando alguns agrupamentos de nomes para facilitar edicoes futuras\n        self.fund_names = [ \"lipizzaner\", \"fund a\", \"fund b\", \"hmx\", \"lipizzaner usd\", \"fund b brl\", \"mangalarga fic fia\", \"maratona\", \"mangalarga fic fia usd\", \"maratona usd\"]\n        self.onshore = [ \"lipizzaner\", \"mangalarga fic fia\", \"maratona\"]\n        self.offshore = [\"fund a\", \"fund b\", \"hmx\"]\n\n        self.benchmarks_names = {\"BBG000L8K6M8 INDEX\" : \"Ibovespa\", \"INX INDEX\" : \"S&amp;P\",\n                                 \"usdbrl curncy\" : \"USDBRL\", \"qqq us equity\" : \"QQQ\",\n                                 \"EZU US equity\" : \"EZU\", \"ACWX US Equity\" : \"MSCI AX\",\n                                 \"ACWI US Equity\" : \"MSCI AC\", \"SMI INDEX\" : \"SMI (CHF)\", \n                                 #\"usdeur curncy\":\"USDEUR\", \"usdchf curncy\":\"USDCHF\",\n                                 \"spx fund spc - hawker portfolio class a shares august 2023 series\": \"SPX Hawker\",\n                                 \"outrigger fund ltd - ip atlas usd equity class - subclass b\" : \"IP Atlas\",\n                                 \"the childrens investment fund class h apr 2025\" : \"TCI\",\n                                 \"bzacselc index\" : \"CDI\",# \"80 s&amp;p 20 jpmutcc\" : \"80% S&amp;P + 20% MM US\",\n                                 \"benchmark luxor sp500 85/15 cash\" : \"Benchmark Luxor\", \"benchmark luxor sp500 85/15 cash brl\" : \"Benchmark Luxor \", \n                                 \"luxor target return\" : \"Luxor Target Return\", \"luxor target return brl\" : \"Luxor Target Return \",\n                                 #\"lipizzaner_acoes\" : \"Luxor Public Equities\", \"lipizzaner_acoes_usd\" : \"Luxor Public Equities \",\n                                 #\"luxor public equities\" : \"Luxor Public Equities\", \"luxor public equities brl\" : \"Luxor Public Equities \",\n                                 #\"luxor liquid equities\" : \"Luxor Liquid Equities\", \"luxor liquid equities brl\" : \"Luxor Liquid Equities \",\n                                 \"luxor equities\" : \"Luxor Equities\", \"luxor equities brl\" : \"Luxor Equities \",\n                                 \"luxor non equities\" : \"Luxor Non Equities\", \"luxor non equities brl\" : \"Luxor Non Equities \",\n                                 \"ax1 comdty\":\"Caf\u00e9 \", \"ls1 comdty usd\" : \"Arroba \", \"sjc1 comdty\" : \"Soja \",\n                                 \"ax1 comdty brl\":\"Caf\u00e9\", \"ls1 comdty\" : \"Arroba\", \"sjc1 comdty brl\" : \"Soja\",\n                                 \"luxor stocks\" : \"Luxor Stocks\", \"luxor stocks brl\" : \"Luxor Stocks \"}#, \"SPXT INDEX\" : \"S&amp;P\"}\n\n\n        # cota no final do ultimo [mes, ano] \n\n        # Definir variacoes manuais no PL. --&gt; PL!! \n        # Ira ajustar a cota para calculo do retorno mtd e ytd, mas sem manter no historico.\n        # Remover ajuste quando atualizado na Britech\n        # -&gt; Valor negativo quando teve aporte sem ajuste de # e positvo quando teve resgate sem ajuste de #\n        self.fund_pl_variation_adjust = {\n            \"lipizzaner\" : 0, #11267102,#-9670000,\n            #\"mangalarga master\" : -39727000,#\n            \"fund a\" : 0,#-2_330_000-162_000,#-368000,#\n            \"fund b\" : 0,#113_500-36_000, #40_000, #50_000,\n            \"hmx\" : 0,\n            \"mangalarga fic fia\" : 0,\n            \"maratona\" : 0,#950000,\n        }\n\n        #carregando bases de dados na memoria\n        self.perf_attr_bases = {}\n        # carrega base de hoje\n        self.ptf_bases = self.load_excel(self.ptf_path)\n        # carrega base de ontem (dia anterior mais recente)\n        self.prev_ptf_bases = self.load_excel(self.prev_pft_path)\n\n        #print(\"Calculando performance attribution diaria...\")\n        logger.info(\"Calculando performance attribution diaria...\")\n        self.calculate_perf_attr()\n\n        #print(\"Atualizando base historica...\")\n        logger.info(\"Atualizando base historica...\")\n        #base de retornos ordenada por data\n        self.hist_returns = self.load_excel(self.returns_path, sheet_name=\"retornos\")\n        self.hist_returns[\"Data\"] = self.hist_returns[\"Data\"].dt.date\n        # TODO - Remover essa linha depois de ajeitar o nome dos fundos\n        self.hist_returns = self.segregate_returns(self.hist_returns)\n\n        self.cur_fund_data = self.calculate_quota()\n\n\n    def get_prev_pft_path(self, directory):\n        \"\"\"Buscamos no diretorio a base de portfolios mais recente disponivel (sem ser o de hoje)\"\"\"\n\n        # Extraindo data do nome de cada arquivo e encontrando a maior menor do que hoje\n        files = os.listdir(directory)\n        #today = dt.datetime.today()\n        today = dt.datetime(self.today.year, self.today.month, self.today.day)\n        dates = [ dt.datetime.strptime(file.split(\"_\")[-1].replace(\".xlsx\",\"\"), \"%Y-%m-%d\") for file in files]\n        dates = list(filter(lambda base_date : base_date &lt; today, dates))\n        max_date = max(dates)\n\n        if today.weekday() != 0:\n            if (today - max_date).days &gt; 1:\n                logger.warning(\"ATEN\u00c7\u00c3O: Base de portfolios anterior esta desatualizada!\")\n        else:\n            if (today - max_date).days &gt; 3:\n                logger.warning(\"ATEN\u00c7\u00c3O: Base de portfolios anterior esta desatualizada!\")\n\n        file = \"base_portfolios_\" + max_date.strftime(\"%Y-%m-%d\") + \".xlsx\"\n        return os.path.join(directory, file)\n\n\n    def segregate_returns(self, returns_db):\n        dbs = {}\n        for fund in self.fund_names:\n            dbs[fund] = returns_db.loc[ returns_db[\"Fundo\"] == fund ]\n        dbs[\"Benchmarks\"]  = returns_db.loc[ returns_db[\"Fundo\"].isin(self.benchmarks_names) ]\n        return dbs\n\n\n    def load_excel(self, file_path, sheet_name=None):\n\n        try:\n            data = pd.read_excel(file_path, sheet_name=sheet_name)\n            return data\n\n        except:\n            #print(f\"N\u00e3o foi poss\u00edvel abrir a arquivo {file_path}.\")\n            logger.error(f\"N\u00e3o foi poss\u00edvel abrir a arquivo {file_path}.\")\n            sys.exit()\n\n\n    def calculate_quota(self):\n\n        funds_cur_quota_data = {}\n\n        for fund in self.fund_names:\n            if fund == \"lipizzaner usd\" or fund == \"mangalarga fic fia usd\" or fund == \"maratona usd\" or fund == \"fund b brl\": continue\n            hist_data = self.query_mngr.get_table(fund.replace(\" \",\"_\")+\"_quotas\")\n\n            prev_data_entry = hist_data.loc[hist_data[\"Date\"].dt.date &lt;= self.today].tail(1)\n            prev_quota_amnt = prev_data_entry[\"#\"].squeeze()\n\n            aum_adjust = self.fund_pl_variation_adjust[fund]\n\n            if aum_adjust != 0:\n                logger.warning(f\"ATEN\u00c7\u00c3O: PL do {fund} ajustado em {aum_adjust}.\")\n                #print(f\"ATEN\u00c7\u00c3O: PL do {fund} ajustado em {aum_adjust}.\")\n\n            cur_aum = self.calculate_pls(fund) + aum_adjust\n            cur_quota_value = cur_aum/prev_quota_amnt\n\n            funds_cur_quota_data[fund] = {\"amount\" : prev_quota_amnt, \"quota\" : cur_quota_value}\n            #print(fund,  funds_cur_quota_data)\n\n        return funds_cur_quota_data\n\n\n    def calculate_pls(self, fund_name):\n\n        # correcao para mensal e anual\n        base = self.ptf_bases[fund_name.replace(\" \", \"_\")]\n\n\n\n        # Para o manga fic fia, vamos pegar o pl em tempo real do manga master e somar com o caixa presente no fic fia\n        if fund_name == \"mangalarga fic fia\":\n            lipizzaner_pl = (self.calculate_pls(\"lipizzaner\") + self.fund_pl_variation_adjust[\"lipizzaner\"] ) * self.query_mngr.calculate_fund_particip(\"mangalarga fic fia\", \"lipizzaner\", self.today)\n            #print(f\"manga fic: {base.loc[base[\"Asset\"] != \"lipizzaner\", \"Mkt_Value\"].sum()}\")\n            return lipizzaner_pl + base.loc[base[\"Asset\"] != \"lipizzaner\", \"Mkt_Value\"].sum()\n\n        return base[\"Mkt_Value\"].sum()\n\n\n    def calculate_returns(self, date_offset=0):\n        \"\"\"\n            Calcula uma nova linha de retorno( retorno do dia ) com base no output\n            gerado das carteiras. Para calcular data retroativa basta informar\n            date_offset( &gt; 0) e usar base de ativos da data correspondente.\n        \"\"\"\n\n        # para o retorno diario, atualizamos performance attribution\n        #self.calculate_perf_attr()\n        ref_date = self.today - dt.timedelta(days=date_offset)\n        ref_date = dt.datetime.fromordinal(ref_date.toordinal()).date()\n\n        for fund in self.fund_names:\n\n            hist_returns = self.hist_returns[fund][ self.hist_returns[fund][\"Data\"] &lt; self.today]\n\n            is_lipizzaner_usd = fund == \"lipizzaner usd\"\n            is_manga_usd = fund == \"mangalarga fic fia usd\"\n            is_maratona_usd = fund == \"maratona usd\"\n            is_fund_b_brl = fund == 'fund b brl'\n            if is_lipizzaner_usd:\n                #fund = \"mangalarga fic fia\" # vai calcular global em real\n                fund = \"lipizzaner\" # vai calcular lipizzaner em real\n            elif is_fund_b_brl:\n                fund='fund b'    \n            elif is_manga_usd:\n                fund = \"mangalarga fic fia\"\n            elif is_maratona_usd:\n                fund = \"maratona\"\n\n\n            calculated_quota = self.cur_fund_data[fund][\"quota\"]\n            calculated_amnt = self.cur_fund_data[fund][\"amount\"]\n            calculated_aum = calculated_amnt * calculated_quota\n\n            # retorno no dia\n            ret_d = self.perf_attr_bases[fund][\"Perf_attr\"].sum()\n            last_quota = self.query_mngr.get_quota_adjusted_by_amortization(fund, self.cur_fund_data[fund][\"quota\"], self.today)\n\n            previous_quota = self.query_mngr.get_quota_adjusted_by_amortization(fund, self.previous_quotas[fund][0], self.last_month_end)\n            ret_m = last_quota / previous_quota-1\n\n            previous_quota = self.query_mngr.get_quota_adjusted_by_amortization(fund, self.previous_quotas[fund][1], self.last_year_end)\n            ret_y = last_quota / previous_quota-1\n\n\n            # Se for primeiro dia util do mes, retorno mtd ser\u00e1 igual ao attribution di\u00e1rio\n            location = \"bz\" if fund in self.onshore else \"us\"\n\n            if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').month != self.today.month:\n                ret_m = ret_d\n\n            # Se for primeiro dia util do ano, retorno ytd sera igual ao attribution diario tamb\u00e9m\n            if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').year != self.today.year:\n                ret_y = ret_d\n\n            # Removemos os ajustes manuais antes de salvar a cota. Importante para nao reaplicarmos o ajuste.\n            #calculated_amnt -= self.fund_pl_variation_adjust[fund]\n\n            if is_lipizzaner_usd or is_manga_usd or is_maratona_usd: #Vamos corrigir o retorno pelo d\u00f3lar para obter o global em usd\n                usd_1d = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n                usd_mtd = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_month_end)\n                usd_ytd = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_year_end)\n                ret_d = (1+ret_d)/(1+usd_1d)-1\n                ret_m = (1+ret_m)/(1+usd_mtd)-1\n                ret_y = (1+ret_y)/(1+usd_ytd)-1\n                fund = fund + \" usd\"\n\n            if is_fund_b_brl: #Vamos corrigir o retorno pelo d\u00f3lar para obter o fund b em BRL\n                usd_1d = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n                usd_mtd = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_month_end)\n                usd_ytd = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_year_end)\n                ret_d = (1+ret_d)*(1+usd_1d)-1\n                ret_m = (1+ret_m)*(1+usd_mtd)-1\n                ret_y = (1+ret_y)*(1+usd_ytd)-1\n                fund = fund + \" brl\"\n\n            #print(fund, calculated_amnt)\n            # inserindo row\n            new_row = [self.today, fund, fund, calculated_aum, calculated_amnt, calculated_quota, ret_d, ret_m, ret_y, np.nan, 0]\n\n            hist_returns.loc[len(hist_returns)] = new_row\n            self.hist_returns[fund] = hist_returns.copy()\n\n\n        self.calculate_benchmarks()\n\n        #self.check_returns()\n\n        self.save_hist_return()\n\n\n\n\n    def calculate_benchmarks(self, date_offset=1) :\n\n        # --&gt; CARTEIRA CALCULADA DO EDUARDO. ESTAMOS CALCULANDO, MAS NAO ESTAMOS MOSTRANDO.\n        def _get_visa_golden_pct_change(recent_date, previous_date):\n\n            def get_pl(date):\n\n                if date &lt;=dt.date(2022,7,31):\n                    return 1002791.70 #Euros -- aplicacao inicial 1002635.20 -&gt; custo dos ativos\n\n                pl = 153.42 # Saldo em conta-corrente\n\n                for tk in assets.keys():\n                    px = self.query_mngr.get_price(tk, date)\n                    curncy = assets[tk][\"curncy\"]\n                    adj_to_usd = 1 if assets[tk][\"curncy\"] == \"usd\" else self.query_mngr.get_price(\"usd\"+curncy+ \" curncy\", px_date=date)\n                    adj_to_eur = self.query_mngr.get_price(\"usdeur curncy\", px_date=date)\n\n                    px = (px/adj_to_usd)*adj_to_eur\n                    if assets[tk][\"is_bond\"]: px = px/100\n\n                    notional = px * assets[tk][\"size\"] * assets[tk][\"amout\"]\n\n                    pl += notional\n\n                return pl\n\n\n            assets = {  \"bp073764 corp\"   : {\"amout\" : 228,      \"size\" : 1000, \"curncy\" : \"usd\" , \"is_bond\" : True },\n                        \"jk482849 corp\"   : {\"amout\" : 200,      \"size\" : 1000, \"curncy\" : \"eur\" , \"is_bond\" : True },\n                        \"bv934023 corp\"   : {\"amout\" : 210,      \"size\" : 1000, \"curncy\" : \"usd\" , \"is_bond\" : True },\n                        \"bh495967 corp\"   : {\"amout\" : 20000000, \"size\" : 0.01, \"curncy\" : \"eur\" , \"is_bond\" : True },\n                        \"cssmi sw equity\" : {\"amout\" : 1636,     \"size\" : 1,    \"curncy\" : \"chf\" , \"is_bond\" : False},\n                    }\n\n\n            pl_variation = get_pl(recent_date)/get_pl(previous_date)\n\n            return pl_variation - 1\n\n        # Iniciando estrutura do dataframe\n        bench_df = pd.DataFrame({\"Data\" : [], \"Nome Fundo\" : [], \"Fundo\" : [], \"PL\" : [], \"#_cota\" : [], \"Cota\" : [], \"Var_1D\" : [], \"Var_mes\" : [], \"Var_ano\" : [], \"Status\" : [], \"Order\" : []})\n\n        brl_tickers = {\"benchmark luxor sp500 85/15 cash brl\", \"luxor target return brl\",\n                            #\"luxor public equities brl\", \n                            \"ax1 comdty brl\",\n                            #\"luxor liquid equities brl\", \n                            \"luxor non equities brl\", \"luxor stocks brl\",\n                            \"sjc1 comdty brl\", \"ls1 comdty usd\", \"luxor equities brl\"}\n\n        for tk, name in self.benchmarks_names.items():\n            if tk not in brl_tickers:\n                # Criando novas entradas para cada benchmark. Obtemos apenas a variacao percentual para cada um. \n                tk_1d = tk\n                if tk.lower() in ['outrigger fund ltd - ip atlas usd equity class - subclass b']:#, \"the childrens investment fund class h apr 2025\"]:\n                    tk_1d = 'inx index' #Forcar mostrar o sp500 do 1d pro ipAtlas, evitando inconsistencias\n                new_row = [self.today, name, name, 0, 0, 0,\n                            self.query_mngr.get_pct_change(tk_1d.lower(), recent_date=self.today, previous_date=self.yesterday),#self.query_mngr.get_bday_offset(self.today, -1)),\n                            self.query_mngr.get_pct_change(tk.lower(), recent_date=self.today, previous_date=self.last_month_end),\n                            self.query_mngr.get_pct_change(tk.lower(), recent_date=self.today, previous_date=self.last_year_end),\"ok\", 1\n                        ]\n\n                # Serie da selic vai ate d-1. vamos pegar a variacao diaria do dia anterior, e aplicar no dia, mes e ano\n                if tk == \"bzacselc index\":\n                    today = self.yesterday\n                    yesterday = self.query_mngr.get_bday_offset(today, offset=-1)\n                    daily = self.query_mngr.get_pct_change(tk.lower(), recent_date=today, previous_date=yesterday)\n                    new_row[6] = daily\n                    new_row[7] = (1+new_row[7])*(1+daily)-1\n                    new_row[8] = (1+new_row[8])*(1+daily)-1\n\n                if tk == 'luxor non equities':\n                    new_row[6] = 0\n\n                # Igualando mes ao diario, no primeiro dia do mes\n                if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').month != self.today.month:\n                    new_row[7] = new_row[6]\n                # Igualando ano ao diario, no primeiro dia do ano\n                if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').year != self.today.year:\n                    new_row[8] = new_row[6]\n\n                bench_df.loc[len(bench_df)] = new_row   \n\n            elif tk in brl_tickers - {\"ls1 comdty usd\"}:\n                # Precisa ser convertido para reais (ls1 eh um caso a parte)\n                tk = tk.replace(\" brl\", \"\")\n                # Criando novas entradas para cada janela. Obtemos apenas a variacao percentual para cada um. \n                new_row = [self.today, name, name, 0, 0, 0,                                                            #self.query_mngr.get_bday_offset(self.today, -1)\n                            (1+self.query_mngr.get_pct_change(tk, recent_date=self.today, previous_date= self.yesterday)) * (1 +self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday))-1,\n                            (1+self.query_mngr.get_pct_change(tk, recent_date=self.today, previous_date=self.last_month_end)) * (1+self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_month_end)) -1,\n                            (1+self.query_mngr.get_pct_change(tk, recent_date=self.today, previous_date=self.last_year_end)) * (1+self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_year_end))-1 ,\n                            \"ok\", 1\n                        ]\n\n                if tk == 'luxor non equities brl':\n                    new_row[6] = 0\n\n                # Igualando mes ao diario, no primeiro dia do mes\n                if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').month != self.today.month:\n                    new_row[7] = new_row[6]\n                # Igualando ano ao diario, no primeiro dia do ano\n                if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').year != self.today.year:\n                    new_row[8] = new_row[6]\n\n                bench_df.loc[len(bench_df)] = new_row\n\n            elif tk in {\"ls1 comdty usd\"}:\n                # Precisa ser convertido para dolares\n                tk = tk.replace(\" usd\", \"\")\n                # Criando novas entradas para cada janela. Obtemos apenas a variacao percentual para cada um. \n                new_row = [self.today, name, name, 0, 0, 0,                                                            #self.query_mngr.get_bday_offset(self.today, -1)\n                            (1+self.query_mngr.get_pct_change(tk, recent_date=self.today, previous_date= self.yesterday)) / (1 +self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday))-1,\n                            (1+self.query_mngr.get_pct_change(tk, recent_date=self.today, previous_date=self.last_month_end)) / (1+self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_month_end)) -1,\n                            (1+self.query_mngr.get_pct_change(tk, recent_date=self.today, previous_date=self.last_year_end)) / (1+self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_year_end))-1 ,\n                            \"ok\", 1\n                        ]\n\n                # Igualando mes ao diario, no primeiro dia do mes\n                if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').month != self.today.month:\n                    new_row[7] = new_row[6]\n                # Igualando ano ao diario, no primeiro dia do ano\n                if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').year != self.today.year:\n                    new_row[8] = new_row[6]\n\n                bench_df.loc[len(bench_df)] = new_row\n\n            #elif tk in [\"luxor public equities\", \"luxor public equities usd\"]:\n            #    \n            #    lipi_acoes_td = self.luxor_equities_quotas.query(\"Date &lt;= @self.today\").tail(1)[\"Quota\"].squeeze()\n            #    lipi_acoes_yterday = self.luxor_equities_quotas.query(\"Date &lt;= @self.yesterday\").tail(1)[\"Quota\"].squeeze()\n            #    lipi_acoes_month = self.luxor_equities_quotas.query(\"Date &lt;= @self.last_month_end\").tail(1)[\"Quota\"].squeeze()\n            #    lipi_acoes_year = self.luxor_equities_quotas.query(\"Date &lt;= @self.last_year_end\").tail(1)[\"Quota\"].squeeze()\n#\n            #    lipi_acoes_daily = lipi_acoes_td/lipi_acoes_yterday-1\n            #    lipi_acoes_mtd = lipi_acoes_td/lipi_acoes_month-1\n            #    lipi_acoes_ytd = lipi_acoes_td/lipi_acoes_year-1\n#\n            #    if tk == \"luxor public equities usd\":\n            #        lipi_acoes_daily = (1+lipi_acoes_daily) / (1 +self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday))-1\n            #        lipi_acoes_mtd = (1+lipi_acoes_mtd) / (1 +self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_month_end))-1\n            #        lipi_acoes_ytd = (1+lipi_acoes_ytd) / (1 +self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_year_end))-1\n            #    \n            #    new_row = [self.today, name, name, 0,0,0,lipi_acoes_daily, lipi_acoes_mtd, lipi_acoes_ytd, \"ok\", 1]\n            #    bench_df.loc[len(bench_df)] = new_row\n\n\n        # Inserindo retorno visa_golden DESATIVADO\n        bench_df.loc[len(bench_df)] = [self.today, \"Visa Golden\", \"Visa Golden\", 0,0,0, 0,0,0, \"ok\", 1]\n        #bench_df.loc[len(bench_df)] = [self.today, \"Visa Golden\", \"Visa Golden\", 0,0,0,\n        #                               _get_visa_golden_pct_change(self.today, self.query_mngr.get_bday_offset(self.today, -1)),\n        #                               _get_visa_golden_pct_change(self.today, self.last_month_end),\n        #                                _get_visa_golden_pct_change(self.today, self.last_year_end), \"ok\", 1\n        #                                ]\n\n\n        # Assim conseguimos substituir dados do dia facilmente\n        self.hist_returns[\"Benchmarks\"] = bench_df #pd.concat([db_frst_half, bench_df, db_last_half])\n\n\n    # s\u00f3 pra usar como base\n    def save_hist_return(self):\n\n        # concatenando bases numa coisa so\n        hist_returns = pd.concat(list(self.hist_returns.values()), ignore_index=True)\n        # TODO - Remover essa linha depois de ajeitar o nome dos fundos\n\n        names = {\"fund a\": \"Fund A\", \"fund b\" : \"Fund B\", \"hmx\" : \"HMX\", \"lipizzaner\" : \"Lipizzaner\",\n                 \"lipizzaner usd\" : \"Lipizzaner\", \"fund b brl\" : \"Fund B\", \"mangalarga fic fia usd\":\"Mangalarga FIC\",\n                 \"mangalarga fic fia\":\"Mangalarga FIC\", \"maratona\" : \"Maratona\", \"maratona usd\" : \"Maratona\"} \n        hist_returns[\"Nome Fundo\"] = hist_returns[\"Fundo\"].apply(lambda x: x if x not in names else names[x.lower()])\n\n        # Regra da ordenacao\n        # fundos luxor e benchmarks R$ -&gt; 1 ate 69\n        # fundos luxor e benchmarks US$ -&gt; 70 ate 170\n        # commodities R$ -&gt; 1000 ate 1999\n        # commodities US$ -&gt; &gt;2000\n\n        elmnt_map_usd = {\n            'lipizzaner usd': 70, 'Benchmark Luxor': 75, 'Luxor Target Return': 80,\n            'Luxor Equities': 86, 'Luxor Non Equities': 88,\n            'Luxor Stocks': 90, 'mangalarga fic fia usd': 92, 'maratona usd': 94,\n            'fund a' : 100, 'fund b' : 110, 'hmx' : 120, 'S&amp;P': 130, 'QQQ': 140,\n            'EZU': 150, 'MSCI AC': 160, 'MSCI AX': 170, 'SMI (CHF)': 180,\n            #'USDEUR': 190, 'USDCHF': 200,\n            \"IP Atlas\" : 125, \"TCI\" : 127, \"SPX Hawker\" : 130,\n            'Caf\u00e9': 1000, 'Soja': 1010, 'Arroba': 1020,\n            'Visa Golden': 280\n            }\n\n        elmnt_map_brl = {\n            'lipizzaner': 1, 'Benchmark Luxor ': 5, 'Luxor Target Return ': 10,\n            'Luxor Equities ': 16, 'Luxor Non Equities ': 18, 'Luxor Stocks ': 20,\n            'mangalarga fic fia': 22, 'fund b brl': 23, 'maratona': 24, 'Ibovespa': 26,\n            'USDBRL': 28, 'CDI': 30,\n            'Caf\u00e9 ': 2000, 'Soja ': 2010, 'Arroba ': 2020,\n            }\n\n        self.elmt_map = {**elmnt_map_usd, **elmnt_map_brl}        \n\n        hist_returns[\"Order\"] = hist_returns[\"Fundo\"].apply(lambda x: self.elmt_map[x])\n\n        #salvando\n        try:\n            paths = [\n                self.returns_path,\n                os.path.join(self.onedrive_path, \"historical_return.xlsx\") #TODO EM PRODUCAO AJUSTAR OUTPUT\n            ]\n            for path in paths:   \n\n                writer = pd.ExcelWriter(path, engine='xlsxwriter')\n                hist_returns.to_excel(writer, index=False, sheet_name=\"retornos\")\n\n                # Salvamos tb a performance attribution do diario\n                data = pd.concat(list(self.perf_attr_bases.values()), ignore_index=True)\n                data.to_excel(writer, index=False, sheet_name=\"Perf_Attr\")\n\n                writer.close()\n\n        except:\n            logger.error(\"N\u00e3o foi poss\u00edvel salvar a base de retornos.\")\n            logger.error(f\"Certifique-se de que '{self.returns_path}' est\u00e1 fechado.\")\n            #print(\"N\u00e3o foi poss\u00edvel salvar a base de retornos.\")\n            #print(f\"Certifique-se de que '{self.returns_path}' est\u00e1 fechado.\")\n            sys.exit()\n\n\n    def calculate_perf_attr(self):\n        \"\"\"\n            Calcula a atribuicao de performance diaria de cada fundo fazendo % variacao * % exposicao\n        \"\"\"\n\n        # Calcular atribuicao de performance para cada fundo\n        for fund_name in self.fund_names:\n            if fund_name == \"lipizzaner usd\" or fund_name ==\"mangalarga fic fia usd\" or fund_name ==\"maratona usd\" or fund_name == \"fund b brl\": continue\n\n            is_manga_fic = False\n            if fund_name == \"mangalarga fic fia\": # vamos computar o attribution do manga master\n                is_manga_fic = True\n                fund_name = \"lipizzaner\"\n\n            f_key = fund_name.replace(\" \", \"_\")\n            base = self.prev_ptf_bases[f_key]\n\n            # Formatando dataframes com colunas que vamos precisar\n            perf_attr_base = base[ [\"Nome\", \"Fundo\", \"Asset\", \"Ticker\", \"Quantidade\", \"Ticker_BBG\", \"Tipo\", \"Grupo\", \"Location\", \"%_pl\"]].copy(deep=False)\n\n            perf_attr_base[\"Fundo\"] = fund_name\n\n            usdbrl_pct_chg = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n\n            def __get_pct_var(row):\n                '''Obtem 1 percentual de variacao diaria baseado no tipo de ativo.'''\n\n                # JAMAIS USAR row[\"Fundo\"] AQUI DENTRO\n                if row[\"Grupo\"] in [\"caixa_us\", \"vc\", \"loan\", \"bond\", \"vc agro\"]:\n                    #TODO: consigo adicionar o vc no attribution diario, queremos fazer isso?\n                    if fund_name in self.offshore:\n                        return 0\n                    else:\n                        return usdbrl_pct_chg\n\n                # Economizamos processamento, nao precisamos calcular para ativos que nao estamos posicionados\n                if row[\"Quantidade\"] == 0:\n                    return 0\n\n                # Tentamos primeiro obter variacao usando ticker bbg\n                pct_change = self.query_mngr.get_pct_change(row[\"Ticker_BBG\"], recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n                #if row[\"Ticker_BBG\"] == 'lzrfy us equity': #tratando retorno da adr de localiza\n                #    pct_change = self.query_mngr.get_pct_change('rent3 bz equity', recent_date=self.today, previous_date=self.yesterday)\n                #    pct_change = (1 + pct_change)/(1 + usdbrl_pct_chg)-1 # Converte para usd\n                # Se nao for possivel, entao tentamos obter usando a coluna ticker\n                if pct_change == 0:\n                    pct_change = self.query_mngr.get_pct_change(row[\"Ticker\"], recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n                # Caso especifico onde o diario se da pela variacao de omega\n                if row[\"Ticker\"] == \"poraque fic fip\":\n                    pct_change = self.query_mngr.get_pct_change(\"mega3 bz equity\", recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n\n                # Se nao conseguimos obter a variacao, sera setada para 0\n                if pct_change is None:\n                    return 0\n\n                # para fundos onshore com ativos offshore, convertemos a variacao obtida pelo dolar\n                if (row[\"Location\"] == \"us\" and fund_name in self.onshore) or (row[\"Tipo\"] == \"a\u00e7\u00f5es_bdr\"):\n                    if row[\"Asset\"] != \"spx hawker\": #Hawker ja esta com a cota corrigida pelo dolar!\n                        # corrigir pelo dolar\n                        pct_change = pct_change + usdbrl_pct_chg + pct_change * usdbrl_pct_chg\n\n                return pct_change\n\n            def __calculate_perf_attr(row):\n                if row[\"Tipo\"] != \"swap_exp\":\n                    return row[\"%_var\"] * row[\"%_pl\"]\n                else:\n                    #sobe qndo o dolar cai e cai qndo o dolar sobe\n                    return -row[\"Quantidade\"] * usdbrl_pct_chg/self.calculate_pls(fund_name)\n\n            # Obter variacao percentual por ativo\n            perf_attr_base[\"%_var\"] = perf_attr_base.apply(lambda row:  __get_pct_var(row), axis=1)\n            # Calcular attribution diario\n            perf_attr_base[\"Perf_attr\"] = perf_attr_base.apply(lambda row: __calculate_perf_attr(row), axis=1)\n            # Remover colunas desnecessarias\n            perf_attr_base = perf_attr_base.drop(\"Quantidade\", axis=1)\n            # Salvando resultado\n            if is_manga_fic: # Foi calculado a partir do Lipizzaner\n                # Precisamos ajustar pelo caixa presente no fundo\n                perf_attr_base[\"Fundo\"] = \"mangalarga fic fia\"\n                # obtendo portfolio do manga\n                manga_ptf = self.ptf_bases[\"mangalarga_fic_fia\"]\n                # obtendo quando do portfolio eh caixa\n                cash_proportion = (manga_ptf.loc[manga_ptf[\"Asset\"] != \"lipizzaner\", \"Mkt_Value\"].sum()) / self.calculate_pls(\"mangalarga fic fia\")\n                # Vamos ajustar o attribution do manga pelo inverso da proporcao de caixa\n                perf_attr_base[\"Perf_attr\"] = perf_attr_base[\"Perf_attr\"].apply(lambda x: x * (1-cash_proportion))\n\n                self.perf_attr_bases[\"mangalarga fic fia\"] = perf_attr_base\n            else:\n                self.perf_attr_bases[fund_name] = perf_attr_base\n\n\n    def check_returns(self):\n\n        mtd_check = {}\n\n        ytd_check = {}\n        warning = {\"mtd\" : [], \"ytd\" : []}\n\n        for fund, rets in self.hist_returns.items():\n            if fund == \"Benchmarks\": continue\n\n            previous = rets.tail(2).iloc[0]\n            current = rets.tail(2).iloc[1]\n            m_check = previous[\"Var_mes\"] + current[\"Var_1D\"] - current[\"Var_mes\"]\n            mtd_check[fund] = round(m_check*100, 3)\n            if abs(m_check) &gt; 0.002:\n                warning[\"mtd\"].append(fund)\n\n            y_check = previous[\"Var_ano\"] + current[\"Var_1D\"] - current[\"Var_ano\"]\n\n            ytd_check[fund] = round(y_check*100, 3)\n            if abs(y_check) &gt; 0.002:\n                warning[\"ytd\"].append(fund)\n\n        try:\n            with open(\"logs/returns_check.txt\", \"w\") as f:\n\n                f.write(\"Check retorno mensal:\\n\")\n                if warning[\"mtd\"]:\n                    w = \"POSS\u00cdVEL ERRO EM: \" + \", \".join(warning[\"mtd\"])+\"\\n\"\n                    #print(w)\n                    logger.warning(w)\n                    f.write(w)\n\n                for f_name, diff in mtd_check.items():\n                    f.write(f\"{f_name}:\\t{diff}%\\n\")\n                f.write(\"\\n\")\n\n                f.write(\"Check retorno anual:\\n\")\n                if warning[\"ytd\"]:\n                    w = \"POSS\u00cdVEL ERRO EM: \" + \", \".join(warning[\"ytd\"])+\"\\n\"\n                    #print(w)\n                    logger.warning(w)\n                    f.write(w)\n                for f_name, diff in ytd_check.items():\n                    f.write(f\"{f_name}:\\t{diff}%\\n\")\n        except:\n            # arquivo aberto ou algum outro erro de escrita\n            logger.error(\"Falha ao salvar log...\")\n            #print(\"Falha ao salvar log...\")\n            logger.error(traceback.format_exc())\n            #traceback.print_exc()\n            #print(\"Check mensal:\")\n            logger.info(f\"Check mensal: {mtd_check}\")\n            #print(mtd_check)\n            #print(\"Check anual:\")\n            logger.info(f\"Check anual: {ytd_check}\")\n</code></pre>"},{"location":"modules/carteira_online/production/return_calculator/#carteira_online.production.return_calculator.ReturnCalculator.calculate_perf_attr","title":"<code>calculate_perf_attr()</code>","text":"<p>Calcula a atribuicao de performance diaria de cada fundo fazendo % variacao * % exposicao</p> Source code in <code>carteira_online\\production\\return_calculator.py</code> <pre><code>def calculate_perf_attr(self):\n    \"\"\"\n        Calcula a atribuicao de performance diaria de cada fundo fazendo % variacao * % exposicao\n    \"\"\"\n\n    # Calcular atribuicao de performance para cada fundo\n    for fund_name in self.fund_names:\n        if fund_name == \"lipizzaner usd\" or fund_name ==\"mangalarga fic fia usd\" or fund_name ==\"maratona usd\" or fund_name == \"fund b brl\": continue\n\n        is_manga_fic = False\n        if fund_name == \"mangalarga fic fia\": # vamos computar o attribution do manga master\n            is_manga_fic = True\n            fund_name = \"lipizzaner\"\n\n        f_key = fund_name.replace(\" \", \"_\")\n        base = self.prev_ptf_bases[f_key]\n\n        # Formatando dataframes com colunas que vamos precisar\n        perf_attr_base = base[ [\"Nome\", \"Fundo\", \"Asset\", \"Ticker\", \"Quantidade\", \"Ticker_BBG\", \"Tipo\", \"Grupo\", \"Location\", \"%_pl\"]].copy(deep=False)\n\n        perf_attr_base[\"Fundo\"] = fund_name\n\n        usdbrl_pct_chg = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n\n        def __get_pct_var(row):\n            '''Obtem 1 percentual de variacao diaria baseado no tipo de ativo.'''\n\n            # JAMAIS USAR row[\"Fundo\"] AQUI DENTRO\n            if row[\"Grupo\"] in [\"caixa_us\", \"vc\", \"loan\", \"bond\", \"vc agro\"]:\n                #TODO: consigo adicionar o vc no attribution diario, queremos fazer isso?\n                if fund_name in self.offshore:\n                    return 0\n                else:\n                    return usdbrl_pct_chg\n\n            # Economizamos processamento, nao precisamos calcular para ativos que nao estamos posicionados\n            if row[\"Quantidade\"] == 0:\n                return 0\n\n            # Tentamos primeiro obter variacao usando ticker bbg\n            pct_change = self.query_mngr.get_pct_change(row[\"Ticker_BBG\"], recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n            #if row[\"Ticker_BBG\"] == 'lzrfy us equity': #tratando retorno da adr de localiza\n            #    pct_change = self.query_mngr.get_pct_change('rent3 bz equity', recent_date=self.today, previous_date=self.yesterday)\n            #    pct_change = (1 + pct_change)/(1 + usdbrl_pct_chg)-1 # Converte para usd\n            # Se nao for possivel, entao tentamos obter usando a coluna ticker\n            if pct_change == 0:\n                pct_change = self.query_mngr.get_pct_change(row[\"Ticker\"], recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n            # Caso especifico onde o diario se da pela variacao de omega\n            if row[\"Ticker\"] == \"poraque fic fip\":\n                pct_change = self.query_mngr.get_pct_change(\"mega3 bz equity\", recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n\n            # Se nao conseguimos obter a variacao, sera setada para 0\n            if pct_change is None:\n                return 0\n\n            # para fundos onshore com ativos offshore, convertemos a variacao obtida pelo dolar\n            if (row[\"Location\"] == \"us\" and fund_name in self.onshore) or (row[\"Tipo\"] == \"a\u00e7\u00f5es_bdr\"):\n                if row[\"Asset\"] != \"spx hawker\": #Hawker ja esta com a cota corrigida pelo dolar!\n                    # corrigir pelo dolar\n                    pct_change = pct_change + usdbrl_pct_chg + pct_change * usdbrl_pct_chg\n\n            return pct_change\n\n        def __calculate_perf_attr(row):\n            if row[\"Tipo\"] != \"swap_exp\":\n                return row[\"%_var\"] * row[\"%_pl\"]\n            else:\n                #sobe qndo o dolar cai e cai qndo o dolar sobe\n                return -row[\"Quantidade\"] * usdbrl_pct_chg/self.calculate_pls(fund_name)\n\n        # Obter variacao percentual por ativo\n        perf_attr_base[\"%_var\"] = perf_attr_base.apply(lambda row:  __get_pct_var(row), axis=1)\n        # Calcular attribution diario\n        perf_attr_base[\"Perf_attr\"] = perf_attr_base.apply(lambda row: __calculate_perf_attr(row), axis=1)\n        # Remover colunas desnecessarias\n        perf_attr_base = perf_attr_base.drop(\"Quantidade\", axis=1)\n        # Salvando resultado\n        if is_manga_fic: # Foi calculado a partir do Lipizzaner\n            # Precisamos ajustar pelo caixa presente no fundo\n            perf_attr_base[\"Fundo\"] = \"mangalarga fic fia\"\n            # obtendo portfolio do manga\n            manga_ptf = self.ptf_bases[\"mangalarga_fic_fia\"]\n            # obtendo quando do portfolio eh caixa\n            cash_proportion = (manga_ptf.loc[manga_ptf[\"Asset\"] != \"lipizzaner\", \"Mkt_Value\"].sum()) / self.calculate_pls(\"mangalarga fic fia\")\n            # Vamos ajustar o attribution do manga pelo inverso da proporcao de caixa\n            perf_attr_base[\"Perf_attr\"] = perf_attr_base[\"Perf_attr\"].apply(lambda x: x * (1-cash_proportion))\n\n            self.perf_attr_bases[\"mangalarga fic fia\"] = perf_attr_base\n        else:\n            self.perf_attr_bases[fund_name] = perf_attr_base\n</code></pre>"},{"location":"modules/carteira_online/production/return_calculator/#carteira_online.production.return_calculator.ReturnCalculator.calculate_returns","title":"<code>calculate_returns(date_offset=0)</code>","text":"<p>Calcula uma nova linha de retorno( retorno do dia ) com base no output gerado das carteiras. Para calcular data retroativa basta informar date_offset( &gt; 0) e usar base de ativos da data correspondente.</p> Source code in <code>carteira_online\\production\\return_calculator.py</code> <pre><code>def calculate_returns(self, date_offset=0):\n    \"\"\"\n        Calcula uma nova linha de retorno( retorno do dia ) com base no output\n        gerado das carteiras. Para calcular data retroativa basta informar\n        date_offset( &gt; 0) e usar base de ativos da data correspondente.\n    \"\"\"\n\n    # para o retorno diario, atualizamos performance attribution\n    #self.calculate_perf_attr()\n    ref_date = self.today - dt.timedelta(days=date_offset)\n    ref_date = dt.datetime.fromordinal(ref_date.toordinal()).date()\n\n    for fund in self.fund_names:\n\n        hist_returns = self.hist_returns[fund][ self.hist_returns[fund][\"Data\"] &lt; self.today]\n\n        is_lipizzaner_usd = fund == \"lipizzaner usd\"\n        is_manga_usd = fund == \"mangalarga fic fia usd\"\n        is_maratona_usd = fund == \"maratona usd\"\n        is_fund_b_brl = fund == 'fund b brl'\n        if is_lipizzaner_usd:\n            #fund = \"mangalarga fic fia\" # vai calcular global em real\n            fund = \"lipizzaner\" # vai calcular lipizzaner em real\n        elif is_fund_b_brl:\n            fund='fund b'    \n        elif is_manga_usd:\n            fund = \"mangalarga fic fia\"\n        elif is_maratona_usd:\n            fund = \"maratona\"\n\n\n        calculated_quota = self.cur_fund_data[fund][\"quota\"]\n        calculated_amnt = self.cur_fund_data[fund][\"amount\"]\n        calculated_aum = calculated_amnt * calculated_quota\n\n        # retorno no dia\n        ret_d = self.perf_attr_bases[fund][\"Perf_attr\"].sum()\n        last_quota = self.query_mngr.get_quota_adjusted_by_amortization(fund, self.cur_fund_data[fund][\"quota\"], self.today)\n\n        previous_quota = self.query_mngr.get_quota_adjusted_by_amortization(fund, self.previous_quotas[fund][0], self.last_month_end)\n        ret_m = last_quota / previous_quota-1\n\n        previous_quota = self.query_mngr.get_quota_adjusted_by_amortization(fund, self.previous_quotas[fund][1], self.last_year_end)\n        ret_y = last_quota / previous_quota-1\n\n\n        # Se for primeiro dia util do mes, retorno mtd ser\u00e1 igual ao attribution di\u00e1rio\n        location = \"bz\" if fund in self.onshore else \"us\"\n\n        if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').month != self.today.month:\n            ret_m = ret_d\n\n        # Se for primeiro dia util do ano, retorno ytd sera igual ao attribution diario tamb\u00e9m\n        if self.query_mngr.get_bday_offset(self.today,offset=-1, location='all').year != self.today.year:\n            ret_y = ret_d\n\n        # Removemos os ajustes manuais antes de salvar a cota. Importante para nao reaplicarmos o ajuste.\n        #calculated_amnt -= self.fund_pl_variation_adjust[fund]\n\n        if is_lipizzaner_usd or is_manga_usd or is_maratona_usd: #Vamos corrigir o retorno pelo d\u00f3lar para obter o global em usd\n            usd_1d = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n            usd_mtd = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_month_end)\n            usd_ytd = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_year_end)\n            ret_d = (1+ret_d)/(1+usd_1d)-1\n            ret_m = (1+ret_m)/(1+usd_mtd)-1\n            ret_y = (1+ret_y)/(1+usd_ytd)-1\n            fund = fund + \" usd\"\n\n        if is_fund_b_brl: #Vamos corrigir o retorno pelo d\u00f3lar para obter o fund b em BRL\n            usd_1d = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.yesterday)#self.query_mngr.get_bday_offset(self.today, -1))\n            usd_mtd = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_month_end)\n            usd_ytd = self.query_mngr.get_pct_change(\"usdbrl curncy\", recent_date=self.today, previous_date=self.last_year_end)\n            ret_d = (1+ret_d)*(1+usd_1d)-1\n            ret_m = (1+ret_m)*(1+usd_mtd)-1\n            ret_y = (1+ret_y)*(1+usd_ytd)-1\n            fund = fund + \" brl\"\n\n        #print(fund, calculated_amnt)\n        # inserindo row\n        new_row = [self.today, fund, fund, calculated_aum, calculated_amnt, calculated_quota, ret_d, ret_m, ret_y, np.nan, 0]\n\n        hist_returns.loc[len(hist_returns)] = new_row\n        self.hist_returns[fund] = hist_returns.copy()\n\n\n    self.calculate_benchmarks()\n\n    #self.check_returns()\n\n    self.save_hist_return()\n</code></pre>"},{"location":"modules/carteira_online/production/return_calculator/#carteira_online.production.return_calculator.ReturnCalculator.get_prev_pft_path","title":"<code>get_prev_pft_path(directory)</code>","text":"<p>Buscamos no diretorio a base de portfolios mais recente disponivel (sem ser o de hoje)</p> Source code in <code>carteira_online\\production\\return_calculator.py</code> <pre><code>def get_prev_pft_path(self, directory):\n    \"\"\"Buscamos no diretorio a base de portfolios mais recente disponivel (sem ser o de hoje)\"\"\"\n\n    # Extraindo data do nome de cada arquivo e encontrando a maior menor do que hoje\n    files = os.listdir(directory)\n    #today = dt.datetime.today()\n    today = dt.datetime(self.today.year, self.today.month, self.today.day)\n    dates = [ dt.datetime.strptime(file.split(\"_\")[-1].replace(\".xlsx\",\"\"), \"%Y-%m-%d\") for file in files]\n    dates = list(filter(lambda base_date : base_date &lt; today, dates))\n    max_date = max(dates)\n\n    if today.weekday() != 0:\n        if (today - max_date).days &gt; 1:\n            logger.warning(\"ATEN\u00c7\u00c3O: Base de portfolios anterior esta desatualizada!\")\n    else:\n        if (today - max_date).days &gt; 3:\n            logger.warning(\"ATEN\u00c7\u00c3O: Base de portfolios anterior esta desatualizada!\")\n\n    file = \"base_portfolios_\" + max_date.strftime(\"%Y-%m-%d\") + \".xlsx\"\n    return os.path.join(directory, file)\n</code></pre>"},{"location":"modules/source_bases/scripts/custom_funds_quotas/","title":"custom_funds_quotas_updater.md","text":""},{"location":"modules/source_bases/scripts/custom_funds_quotas/#title-custom_funds_quotas_updaterpy","title":"title: custom_funds_quotas_updater.py","text":""},{"location":"modules/source_bases/scripts/custom_funds_quotas/#custom_funds_quotas_updaterpy","title":"<code>custom_funds_quotas_updater.py</code>","text":"<p>Gera s\u00e9ries de cotas para portfolios customizados da Luxor.</p>"},{"location":"modules/source_bases/scripts/daily_pnl_updater/","title":"daily_pnl_updater.md","text":""},{"location":"modules/source_bases/scripts/daily_pnl_updater/#title-daily_pnl_updaterpy","title":"title: daily_pnl_updater.py","text":""},{"location":"modules/source_bases/scripts/daily_pnl_updater/#daily_pnl_updaterpy","title":"<code>daily_pnl_updater.py</code>","text":"<p>Calcula PnL di\u00e1rio para portfolios customizados e fundos Luxor.</p>"},{"location":"modules/source_bases/scripts/daily_pnl_updater/#source_bases.scripts.daily_pnl_updater.create_daily_pnl_db","title":"<code>create_daily_pnl_db(lq, portfolios, today, bdays=30)</code>","text":"<p>Cria base di\u00e1ria de PnL para diversos portfolios (definidos na funcao).</p> <p>Parameters:</p> Name Type Description Default <code>lq</code> <code>LuxorQuery</code> <p>objeto LuxorQuery para realizar queries no banco de dados.</p> required <code>portfolios</code> <code>dict</code> <p>dicionario com os portfolios e sua configuracao.</p> required <code>today</code> <code>date</code> <p>data fim, referencia para calculo da data inicial</p> required <code>bdays</code> <code>int</code> <p>numero de dias uteis anteriores a ref_date.                 Ser\u00e1 usado para achar a data inicial. Defaults to 30.</p> <code>30</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Base di\u00e1ria de PnL.</p> Source code in <code>source_bases\\scripts\\daily_pnl_updater.py</code> <pre><code>def create_daily_pnl_db(lq: LuxorQuery, portfolios: dict, today: dt.date,\n                         bdays: int = 30)-&gt;pd.DataFrame:\n    \"\"\"Cria base di\u00e1ria de PnL para diversos portfolios (definidos na funcao).\n\n    Args:\n        lq (LuxorQuery): objeto LuxorQuery para realizar queries no banco de dados.\n        portfolios (dict): dicionario com os portfolios e sua configuracao.\n        today (dt.date): data fim, referencia para calculo da data inicial\n        bdays (int, optional): numero de dias uteis anteriores a ref_date.\n                            Ser\u00e1 usado para achar a data inicial. Defaults to 30.\n\n    Returns:\n        pd.DataFrame: Base di\u00e1ria de PnL.\n    \"\"\"\n    daily_pnls = []\n    # Criando base de pnl diario\n    for df_name, params in portfolios.items():\n        for currency in ['brl', 'usd']:\n            df = lq.calculate_daily_pnl(\n                params['fund_ref'], ref_date=today,\n                classification_filters=params['classification_filter'],\n                currency=currency, bdays=bdays, holiday_location=\"all\", asset_filters=params['asset_filter'],\n                group_filters=params['group_filter'], location_filters=params['location_filter'],\n                include_cash=params['include_cash'], include_cash_debt=params['include_cash_debt'],\n                currency_exposure_filters=params['currency_exposure'],\n                annual_adm_fee=params['fees'], usdbrl_ticker=\"usdbrl cmpl curncy\").rename(columns={\"Today\":\"Date\"})\n            df[\"Currency\"] = currency\n            df[\"Series_Alias\"] = df_name\n\n            #daily_fee = (1 + params['fees'])**(1/365) - 1\n            daily_pnls.append(df.copy())\n\n    daily_pnls = pd.concat(daily_pnls)\n\n    return daily_pnls\n</code></pre>"},{"location":"modules/source_bases/scripts/daily_pnl_updater/#source_bases.scripts.daily_pnl_updater.create_portfolios_compositions","title":"<code>create_portfolios_compositions(lq)</code>","text":"<p>Cria dicionario com as composicoes dos portfolios. Args:     lq (LuxorQuery): objeto LuxorQuery para realizar queries no banco de dados. Returns:     dict:</p> Source code in <code>source_bases\\scripts\\daily_pnl_updater.py</code> <pre><code>def create_portfolios_compositions(lq: LuxorQuery) -&gt; dict:\n    \"\"\" Cria dicionario com as composicoes dos portfolios.\n    Args:\n        lq (LuxorQuery): objeto LuxorQuery para realizar queries no banco de dados.\n    Returns:\n        dict:\n    \"\"\"\n\n    all_groups = set(lq.get_table('assets')['Group'])\n\n    luxor_stocks = {'stocks'}\n    equity_funds = {'equity funds'}\n    hedge_funds = {'hedge funds'}\n    venture_capital = {'vc'}\n    private_equity = {'private equity'}\n    fixed_income = {'fixed income'}\n\n    public_equities = equity_funds.union(luxor_stocks)\n    pe_and_vc = venture_capital.union(private_equity)\n\n    luxor_equities = public_equities.union(pe_and_vc)\n    luxor_non_equities = hedge_funds.union(fixed_income).union({'fx'})\n    luxor_public_equities = {'a\u00e7\u00f5es'}\n\n    portfolio_us = {\"us\"}\n    portfolio_br = {\"bz\"}\n\n    debt = {'debt'}\n\n    # Definindo paramentros dos portfolios que terao o PnL calculado\n    portfolios = {\n        'luxor equities': \n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : luxor_equities,\n            'group_filter' : [],\n            'asset_filter' : [],\n            'location_filter' : [],\n            'include_cash' : False,\n            'include_cash_debt': False,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : [],\n            },\n        'luxor non equities':\n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : luxor_non_equities,\n            'group_filter' : [],\n            'asset_filter' : [],\n            'location_filter' : [],\n            'include_cash' : True,\n            'include_cash_debt': False,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : [],\n            },\n        #TODO liquidos e iliquidos, a partir de liquidez do ativo\n        #'luxor liquid equities':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'group_filter' : luxor_liquid_equities,\n        #    'include_cash' : False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    },\n        #'luxor iliquids':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'group_filter' : luxor_iliquids,\n        #    'include_cash' : False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    },\n        #'public equities':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : public_equities,\n        #    'group_filter' : [],\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : [],\n        #    },\n        #'luxor public equities':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'group_filter' : luxor_public_equities,\n        #    'classification_filter' : [],\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : [],\n        #    },\n        #'pe and vc':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : pe_and_vc,\n        #    'group_filter' : [],\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : [],\n        #    },\n        'luxor stocks':\n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : luxor_stocks,\n            'group_filter' : [],\n            'asset_filter' : [],\n            'location_filter' : [],\n            'include_cash' : False,\n            'include_cash_debt': False,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : [],\n            },\n        #'venture capital':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : venture_capital,\n        #    'group_filter' : [],\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : [],\n        #    },\n        #'private equity':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : private_equity,\n        #    'group_filter' : [],\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : [],\n        #    },\n        #'equity funds':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : equity_funds,\n        #    'group_filter' : [],\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : [],\n        #    },\n        #'fixed income':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : fixed_income,\n        #    'group_filter' : [],\n        #    'location_filter' : [],\n        #    'include_cash' : True,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : [],\n        #    },\n        #'hedge funds':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : hedge_funds,\n        #    'group_filter' : [],\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : [],\n        #    },\n        'luxor tci':\n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : [],\n            'group_filter' : [],\n            'location_filter' : [],\n            'asset_filter' : ['tci'],\n            'include_cash' : False,\n            'include_cash_debt': False,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : [],\n            },\n\n        'luxor spx hawker':\n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : [],\n            'group_filter' : [],\n            'location_filter' : [], \n            'asset_filter' : ['spx hawker'],\n            'include_cash' : False,\n            'include_cash_debt': False,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : [],\n            },\n\n        'luxor ip atlas':\n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : [],\n            'group_filter' : [],\n            'location_filter' : [],\n            'asset_filter' : ['ip atlas'],\n            'include_cash' : False,\n            'include_cash_debt': False,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : [],\n            },\n\n        'luxor sp500':\n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : [],\n            'group_filter' : [],\n            'location_filter' : [], \n            'asset_filter' : ['etf s&amp;p'],\n            'include_cash' : False,\n            'include_cash_debt': False,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : [],\n            },\n\n        'lipizzaner ex fees':\n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : [],\n            'group_filter' : all_groups - {'luxor'},\n            'location_filter' : [],\n            'asset_filter' : [],\n            'include_cash' : True,\n            'include_cash_debt': True,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : [],\n            },\n        'lipizzaner':\n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : [],\n            'group_filter' : all_groups - {'luxor'},\n            'location_filter' : [],\n            'asset_filter' : [],\n            'include_cash' : True,\n            'include_cash_debt': True,\n            'include_costs' : True,\n            'fees' : 0.0238, # Incluindo todos os fees da luxor, inclusive os do Manga.\n            'currency_exposure' : [],\n            },\n        'maratona':\n            {'fund_ref' : 'maratona',\n            'classification_filter' : [],\n            'group_filter' : [],\n            'location_filter' : [],\n            'asset_filter' : [],\n            'include_cash' : True,\n            'include_cash_debt': True,\n            'include_costs' : True,\n            'fees' : 0.0010,\n            'currency_exposure' : [],\n            },\n        'fund b':\n            {'fund_ref' : 'fund b',\n            'classification_filter' : [],\n            'group_filter' : [],\n            'location_filter' : [],\n            'asset_filter' : [],\n            'include_cash' : True,\n            'include_cash_debt': True,\n            'include_costs' : True,\n            'fees' : 0.01,\n            'currency_exposure' : [],\n            },\n        'fund a':\n            {'fund_ref' : 'fund a',\n            'classification_filter' : [],\n            'group_filter' : [],\n            'location_filter' : [],\n            'asset_filter' : [],\n            'include_cash' : True,\n            'include_cash_debt': True,\n            'include_costs' : True,\n            'fees':0.02084,\n            'currency_exposure' : [],\n            },\n        'hmx':\n            {'fund_ref' : 'hmx',\n            'classification_filter' : [],\n            'group_filter' : [],\n            'location_filter' : [],\n            'asset_filter' : [],\n            'include_cash' : True,\n            'include_cash_debt': True,\n            'include_costs' : True,\n            'fees':0,\n            'currency_exposure' : [],\n            },\n        #'luxor portfolio us':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : [],\n        #    'group_filter' : all_groups - {'luxor'},\n        #    'location_filter' : portfolio_us,\n        #    'include_cash' : True,\n        #    'include_cash_debt': True,\n        #    'include_costs' : False,\n        #    'fees':0,\n        #    'currency_exposure' : [],\n        #    },\n        #'luxor portfolio br':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : [],\n        #    'group_filter' : all_groups - {'luxor'},\n        #    'location_filter' : portfolio_br,\n        #    'include_cash' : True,\n        #    'include_cash_debt': True,\n        #    'include_costs' : False,\n        #    'fees':0,\n        #    'currency_exposure' : [],\n        #    },\n        #'luxor stocks us':\n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : luxor_stocks,\n        #    'group_filter' : all_groups - {'luxor'},\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : ['us'],\n        #    },\n        #'luxor equities us': \n        #    {'fund_ref' : 'lipizzaner',\n        #    'classification_filter' : luxor_equities,\n        #    'group_filter' : all_groups - {'luxor'},\n        #    'location_filter' : [],\n        #    'include_cash' : False,\n        #    'include_cash_debt': False,\n        #    'include_costs' : False,\n        #    'fees' : 0,\n        #    'currency_exposure' : ['us'],\n        #    },\n        'debt': \n            {'fund_ref' : 'lipizzaner',\n            'classification_filter' : debt,\n            'group_filter' : all_groups - {'luxor'},\n            'location_filter' : [],\n            'asset_filter' : [],\n            'include_cash' : False,\n            'include_cash_debt': True,\n            'include_costs' : False,\n            'fees' : 0,\n            'currency_exposure' : ['us'],\n            },\n    }\n\n    return portfolios\n</code></pre>"},{"location":"modules/source_bases/scripts/hist_concentration_updater/","title":"<code>hist_concentration_updater.py</code>","text":"<p>Atualiza hist_portfolios_concentration.parquet (fun\u00e7\u00e3o descontinuada).</p>"},{"location":"modules/source_bases/scripts/historical_quotas_updater/","title":"historical_quotas_updater.md","text":""},{"location":"modules/source_bases/scripts/historical_quotas_updater/#title-historical_quotas_updaterpy","title":"title: historical_quotas_updater.py","text":""},{"location":"modules/source_bases/scripts/historical_quotas_updater/#historical_quotas_updaterpy","title":"<code>historical_quotas_updater.py</code>","text":"<p>Atualiza all_funds_quotas.parquet a partir de historical_returns.xlsx.</p>"},{"location":"modules/source_bases/scripts/non_bbg_data_updater/","title":"non_bbg_data_updater.md","text":""},{"location":"modules/source_bases/scripts/non_bbg_data_updater/#title-non_bbg_data_updaterpy","title":"title: non_bbg_data_updater.py","text":""},{"location":"modules/source_bases/scripts/non_bbg_data_updater/#non_bbg_data_updaterpy","title":"<code>non_bbg_data_updater.py</code>","text":"<p>Cria hist_non_bbg_px_last.parquet com s\u00e9ries personalizadas (IPCA+7, Hawker\u2026).</p>"},{"location":"modules/source_bases/scripts/non_bbg_data_updater/#source_bases.scripts.non_bbg_data_updater.create_fixed_index","title":"<code>create_fixed_index(non_bbg_base, annual_return, index_ticker)</code>","text":"<p>Cria um indicador com valor fixo de rendimento colocando direto na     tabela de ativos nao bbg.</p> <p>Parameters:</p> Name Type Description Default <code>non_bbg_base</code> <code>DataFrame</code> <p>base historica de ativos nao bbg.</p> required <code>annual_return</code> <code>float</code> <p>valor fixo de retorno.</p> required <code>index_ticker</code> <code>str</code> <p>ticker que sera usado para busca de precos.</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>source_bases\\scripts\\non_bbg_data_updater.py</code> <pre><code>def create_fixed_index(non_bbg_base, annual_return, index_ticker):\n    \"\"\" Cria um indicador com valor fixo de rendimento colocando direto na\n        tabela de ativos nao bbg.\n\n    Args:\n        non_bbg_base (pd.DataFrame): base historica de ativos nao bbg.\n        annual_return (float): valor fixo de retorno.\n        index_ticker (str): ticker que sera usado para busca de precos.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n\n    daily_return_with_factor = (1 + annual_return)**(1/365)\n\n    # Criando uma coluna com o retorno diario fatorado em todos os dias\n    non_bbg_base[index_ticker] = daily_return_with_factor\n    # Acumulando os retornos diarios (feito pelo metodo cumprod)\n    non_bbg_base[index_ticker] = non_bbg_base[index_ticker].cumprod()\n\n    return non_bbg_base\n</code></pre>"},{"location":"modules/source_bases/scripts/non_bbg_data_updater/#source_bases.scripts.non_bbg_data_updater.create_loan_usdeur_index","title":"<code>create_loan_usdeur_index(query_mngr, non_bbg_base, annual_return, loan_date, index_ticker)</code>","text":"<p>Cria um indicador com valor fixo de rendimento com variacao do cambio usdeur colocando direto na tabela de ativos nao bbg.</p> <p>Parameters:</p> Name Type Description Default <code>non_bbg_base</code> <code>DataFrame</code> <p>base historica de ativos nao bbg.</p> required <code>annual_return</code> <code>float</code> <p>valor fixo de retorno.</p> required <code>index_ticker</code> <code>str</code> <p>ticker que sera usado para busca de precos.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: non_bbg_base</p> Source code in <code>source_bases\\scripts\\non_bbg_data_updater.py</code> <pre><code>def create_loan_usdeur_index(query_mngr, non_bbg_base, annual_return, loan_date, index_ticker):\n    \"\"\" Cria um indicador com valor fixo de rendimento com variacao do cambio usdeur\n        colocando direto na tabela de ativos nao bbg.\n\n        Args:\n            non_bbg_base (pd.DataFrame): base historica de ativos nao bbg.\n            annual_return (float): valor fixo de retorno.\n            index_ticker (str): ticker que sera usado para busca de precos.\n\n        Returns:\n            pd.DataFrame: non_bbg_base\n        \"\"\"\n    daily_return_with_factor = (1 + annual_return)**(1/365)\n\n    new_index = query_mngr.get_prices([\"usdeur curncy\"], previous_date=loan_date)\\\n                            .rename(columns={\"Last_Price\" : \"usdeur\"})\\\n                            .set_index(\"Date\")\n    new_index[\"fixed\"] = daily_return_with_factor\n\n    new_index[\"fixed\"] = new_index[[\"fixed\"]].cumprod()\n    new_index[index_ticker] = new_index[\"fixed\"]/new_index[\"usdeur\"]\n    new_index = new_index[[index_ticker]]\n\n    non_bbg_base = non_bbg_base.set_index(\"Date\")\n    non_bbg_base[index_ticker] = new_index\n    non_bbg_base = non_bbg_base.reset_index()\n    return non_bbg_base\n</code></pre>"},{"location":"modules/source_bases/scripts/non_bbg_data_updater/#source_bases.scripts.non_bbg_data_updater.create_variable_index","title":"<code>create_variable_index(non_bbg_base, new_index_ticker, query_mngr, index_ticker_variable, fixed_return)</code>","text":"<p>Metodo padronizado para criacao de um index que possui uma parte variavel e uma fixa.     A parte variavel precisa estar ho historico de precos da luxor_db. Args:     non_bbg_base (pd.DataFrame): base historica de ativos nao bbg.     new_index_ticker (str): ticker do novo index.     query_mngr (LuxorQuery): referencia ao modulo de consulta a luxor_db.     index_ticker_variable (str): ticker do index variavel.      fixed_return (float): retorno fixo na base anual.</p> Source code in <code>source_bases\\scripts\\non_bbg_data_updater.py</code> <pre><code>def create_variable_index(non_bbg_base, new_index_ticker, query_mngr, index_ticker_variable, fixed_return):\n    \"\"\" Metodo padronizado para criacao de um index que possui uma parte variavel e uma fixa.\n        A parte variavel precisa estar ho historico de precos da luxor_db.\n    Args:\n        non_bbg_base (pd.DataFrame): base historica de ativos nao bbg.\n        new_index_ticker (str): ticker do novo index.\n        query_mngr (LuxorQuery): referencia ao modulo de consulta a luxor_db.\n        index_ticker_variable (str): ticker do index variavel. \n        fixed_return (float): retorno fixo na base anual.\n    \"\"\"\n    # Obtendo variacao da parcela variavel.\n    # Caso nao haja variacao na data de hoje, vamos usar a variacapo do dia seguinte\n    new_index = query_mngr.get_prices(\"sofrindx index\", period=\"60m\")\n    new_index[\"Date\"] = new_index[\"Date\"].dt.date\n    new_index[\"Variation_1d\"] = new_index[\"Last_Price\"].pct_change().fillna(0)\n    # Colocando a variacao na data mais recente, como a mais repetida dos ultimos 7 dias\n    last_variation = new_index.query(\"Variation_1d != 0\")[\"Variation_1d\"].tail(7).mode().iloc[0]\n    if max(new_index[\"Date\"]) &lt; dt.date.today():\n        last_row = new_index.tail(1).copy()\n        last_row[\"Date\"] = dt.date.today()\n        last_row[\"Variation_1d\"] = last_variation\n        new_index.loc[len(new_index)] = last_row.to_dict(orient=\"records\")[0]\n\n    new_index[\"Fidex_1d\"] = (1+fixed_return)**(1/365)-1\n    new_index[\"Result_1d\"] = new_index[\"Variation_1d\"] + new_index[\"Fidex_1d\"] + 1\n    new_index[\"Last_Price\"] = new_index[\"Result_1d\"].cumprod()\n    new_index = new_index.merge(non_bbg_base[[\"Date\"]], on=\"Date\", how=\"right\").fillna(1)\n    non_bbg_base[new_index_ticker] = new_index[\"Last_Price\"].copy()\n\n    return non_bbg_base\n</code></pre>"},{"location":"modules/source_bases/scripts/non_bbg_data_updater/#source_bases.scripts.non_bbg_data_updater.set_df_index","title":"<code>set_df_index(data, index_col, persist_cols=None)</code>","text":"<p>Defines a new index to a dataframe and removes index_col</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dataframe</code> required <code>index_col</code> <code>string</code> <p>name of the column to use as index</p> required <code>persist_cols</code> <code>list</code> <p>list of column names to keep in the dataframe</p> <code>None</code> Source code in <code>source_bases\\scripts\\non_bbg_data_updater.py</code> <pre><code>def set_df_index(data, index_col, persist_cols=None):\n    \"\"\"Defines a new index to a dataframe and removes index_col\n\n    Args:\n        data (dataframe): \n        index_col (string): name of the column to use as index\n        persist_cols (list): list of column names to keep in the dataframe\n    \"\"\"\n\n    data.index = data[index_col]\n    if persist_cols is None:\n        cols_to_keep = list(data.columns)\n        cols_to_keep.remove(index_col)\n        data = data[cols_to_keep]\n    else:\n        data = data[persist_cols]\n    return data\n</code></pre>"},{"location":"modules/source_bases/scripts/risk_metrics_updater/","title":"risk_metrics_updater.md","text":""},{"location":"modules/source_bases/scripts/risk_metrics_updater/#title-risk_metrics_updaterpy","title":"title: risk_metrics_updater.py","text":""},{"location":"modules/source_bases/scripts/risk_metrics_updater/#risk_metrics_updaterpy","title":"<code>risk_metrics_updater.py</code>","text":"<p>Atualiza incrementalmente historico_risk_metrics.xlsx \u2192 Parquet.</p>"},{"location":"modules/source_bases/scripts/swaps_updater/","title":"swaps_updater.md","text":""},{"location":"modules/source_bases/scripts/swaps_updater/#title-swaps_updaterpy","title":"title: swaps_updater.py","text":""},{"location":"modules/source_bases/scripts/swaps_updater/#swaps_updaterpy-legacy","title":"<code>swaps_updater.py</code> (LEGACY)","text":"<p>Exportava hist\u00f3rico de swap; n\u00e3o usado ap\u00f3s 2023.</p>"},{"location":"modules/source_bases/scripts/swaps_updater/#source_bases.scripts.swaps_updater.run","title":"<code>run(dst_file_path, source_swaps_path, swap_src_filename='historical_data.xlsx', is_develop_mode=False)</code>","text":"<p>Faz copia da base historica de swap mantida pela carteira online.     Nesse processo, fazemos padding para a carteira historica do global</p> <p>Parameters:</p> Name Type Description Default <code>dst_file_path</code> <code>_type_</code> required <code>source_swaps_path</code> <code>_type_</code> <p>description</p> required <code>swap_src_filename</code> <code>str</code> <p>description. Defaults to \"historical_data.xlsx\".</p> <code>'historical_data.xlsx'</code> Source code in <code>source_bases\\scripts\\swaps_updater.py</code> <pre><code>def run(dst_file_path, source_swaps_path, swap_src_filename=\"historical_data.xlsx\", is_develop_mode=False):\n    \"\"\" Faz copia da base historica de swap mantida pela carteira online.\n        Nesse processo, fazemos padding para a carteira historica do global\n\n    Args:\n        dst_file_path (_type_): \n        source_swaps_path (_type_): _description_\n        swap_src_filename (str, optional): _description_. Defaults to \"historical_data.xlsx\".\n    \"\"\"\n    logger.info(\"Obtendo base atualizada de swaps...\")\n\n    swap_data = pd.read_excel(os.path.join(source_swaps_path, swap_src_filename), engine=\"openpyxl\")\n\n    swap_data.columns = [\"Date\", \"Fund\", \"Exposure\", \"USDBRL Chg\", \"Result\", \"Day\", \"Due\", \"Status\"]\n\n    swap_data[\"Date\"] = swap_data[\"Date\"].dt.date\n\n    #base_sample = swap_data.loc[((swap_data[\"Date\"] &lt; dt.date(2022, 1, 26)) &amp; (swap_data[\"Fund\"] == \"Mangalarga\"))].copy()\n    #base_sample[\"Fund\"] = \"Global\"\n    #base_sample[\"Exposure\"] = 0\n    #base_sample[\"Result\"] = 0\n    #base_sample[\"Day\"] = 0\n    #base_sample[\"Due\"] = \"\"\n    #base_sample[\"Status\"] = \"ok\"\n\n    #swap_data = pd.concat([swap_data, base_sample])\n\n    swap_data.to_excel(dst_file_path, index=False, sheet_name=\"swap\")\n</code></pre>"},{"location":"modules/source_bases/scripts/us_cash_updater/","title":"us_cash_updater.md","text":""},{"location":"modules/source_bases/scripts/us_cash_updater/#title-us_cash_updaterpy","title":"title: us_cash_updater.py","text":""},{"location":"modules/source_bases/scripts/us_cash_updater/#us_cash_updaterpy","title":"<code>us_cash_updater.py</code>","text":"<p>Gera hist_us_cash.parquet com saldos de caixa offshore.</p>"},{"location":"planilhas/planilhas/","title":"Planilhas Detalhadas","text":"<p>Esta p\u00e1gina descreve em detalhes as principais planilhas utilizadas pelo LuxorASAP, indicando sua localiza\u00e7\u00e3o, fun\u00e7\u00e3o e scripts que interagem com cada uma delas.</p>","tags":["spreadsheets"]},{"location":"planilhas/planilhas/#operation_desk_v4xlsm","title":"Operation_Desk_V4.xlsm","text":"<ul> <li> <p>Localiza\u00e7\u00e3o <code>carteira_online/production/bases_input/Operation_Desk_V4.xlsm</code></p> </li> <li> <p>Descri\u00e7\u00e3o   Planilha de c\u00e1lculo de ajustes de posi\u00e7\u00e3o e CRUD de ativos da carteira online. Possui uma aba dedicada para cada fundo, listando seus ativos e quantidades.</p> </li> <li> <p>Intera\u00e7\u00f5es Principais </p> </li> <li><code>portfolio_builder.py</code>: l\u00ea as abas para montar o portf\u00f3lio inicial.  </li> <li>Serve de base para gera\u00e7\u00e3o de <code>input_data.xlsx</code>.  </li> <li>Influencia o <code>cash_calculator.py</code> atrav\u00e9s das posi\u00e7\u00f5es calculadas.</li> </ul>","tags":["spreadsheets"]},{"location":"planilhas/planilhas/#input_dataxlsx","title":"input_data.xlsx","text":"<ul> <li> <p>Localiza\u00e7\u00e3o <code>carteira_online/production/bases_input/input_data.xlsx</code></p> </li> <li> <p>Descri\u00e7\u00e3o   C\u00f3pia da <code>Operation_Desk_V4.xlsm</code> gerada ao clicar no bot\u00e3o \"salvar input\". Cont\u00e9m abas para cada fundo com as quantidades e uma aba <code>ativos</code> com o cadastro de ativos.</p> </li> <li> <p>Intera\u00e7\u00f5es Principais </p> </li> <li><code>portfolio_builder.py</code>: usa para gerar <code>base_portfolios.xlsx</code> e atualizar <code>base_carteira_online_v12.xlsx</code>.  </li> <li>Alimenta o relat\u00f3rio Power BI <code>carteira_online.pbix</code>.</li> </ul>","tags":["spreadsheets"]},{"location":"planilhas/planilhas/#base_portfoliosxlsx","title":"base_portfolios.xlsx","text":"<ul> <li> <p>Localiza\u00e7\u00e3o <code>carteira_online/production/bases_output/base_portfolios.xlsx</code></p> </li> <li> <p>Descri\u00e7\u00e3o   Base gerencial sem agrega\u00e7\u00f5es, com posi\u00e7\u00f5es e quantidades por fundo, gerada ao final do preg\u00e3o pelo <code>portfolio_builder.py</code>.</p> </li> <li> <p>Intera\u00e7\u00f5es Principais </p> </li> <li><code>portfolio_builder.py</code>: cria e salva nesta pasta.  </li> <li>Copiada para <code>carteiras_luxor_historico/bases_completas/</code> diariamente.  </li> <li><code>risk_metrics_updater.py</code> e <code>hist_concentration_updater.py</code>: consomem para c\u00e1lculo de m\u00e9tricas.</li> </ul>","tags":["spreadsheets"]},{"location":"planilhas/planilhas/#base_carteira_online_v12xlsx","title":"base_carteira_online_v12.xlsx","text":"<ul> <li> <p>Localiza\u00e7\u00e3o <code>carteira_online/base_carteira_online_v12.xlsx</code></p> </li> <li> <p>Descri\u00e7\u00e3o   Output final do <code>portfolio_builder.py</code>, com tabelas prontas para o relat\u00f3rio Power BI <code>carteira_online.pbix</code>.</p> </li> <li> <p>Intera\u00e7\u00f5es Principais </p> </li> <li><code>portfolio_builder.py</code>: gera esta planilha.  </li> <li>Power BI (<code>carteira_online.pbix</code>): consome diretamente.</li> </ul>","tags":["spreadsheets"]},{"location":"planilhas/planilhas/#historical_returnxlsx-raw","title":"historical_return.xlsx (raw)","text":"<ul> <li> <p>Localiza\u00e7\u00e3o <code>carteira_online/production/bases_historicas/historical_return.xlsx</code></p> </li> <li> <p>Descri\u00e7\u00e3o   Base de cotas gerenciais calculadas pelo <code>return_calculator.py</code>. Precisa de ajustes manuais nas quantidades para refletir aplica\u00e7\u00f5es e resgates dos fundos.</p> </li> <li> <p>Intera\u00e7\u00f5es Principais </p> </li> <li><code>return_calculator.py</code>: gera esta planilha.  </li> <li><code>historical_quotas_updater.py</code>: consome para atualizar s\u00e9ries de cotas em <code>source_bases</code>.  </li> <li>Relat\u00f3rio Power BI <code>relatorio_retornos.pbix</code>.</li> </ul>","tags":["spreadsheets"]},{"location":"planilhas/planilhas/#historical_returnxlsx-output-final","title":"historical_return.xlsx (output final)","text":"<ul> <li> <p>Localiza\u00e7\u00e3o <code>carteira_online/historical_return.xlsx</code></p> </li> <li> <p>Descri\u00e7\u00e3o   Vers\u00e3o final das tabelas de retornos gerenciais (1D, MTD, YTD) calculadas pelo <code>return_calculator.py</code> e formatadas para o relat\u00f3rio Power BI <code>relatorio_retornos.pbix</code>.</p> </li> <li> <p>Intera\u00e7\u00f5es Principais </p> </li> <li><code>return_calculator.py</code>: gera esta c\u00f3pia final.  </li> <li>Power BI (<code>relatorio_retornos.pbix</code>): consome diretamente.</li> </ul>","tags":["spreadsheets"]}]}